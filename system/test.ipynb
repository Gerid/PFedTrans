{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import logging\n",
    "\n",
    "from flcore.servers.serveravg import FedAvg\n",
    "from flcore.servers.serverpFedMe import pFedMe\n",
    "from flcore.servers.serverperavg import PerAvg\n",
    "from flcore.servers.serverprox import FedProx\n",
    "from flcore.servers.serverfomo import FedFomo\n",
    "from flcore.servers.serveramp import FedAMP\n",
    "from flcore.servers.servermtl import FedMTL\n",
    "from flcore.servers.serverlocal import Local\n",
    "from flcore.servers.serverper import FedPer\n",
    "from flcore.servers.serverapfl import APFL\n",
    "from flcore.servers.serverditto import Ditto\n",
    "from flcore.servers.serverrep import FedRep\n",
    "from flcore.servers.serverphp import FedPHP\n",
    "from flcore.servers.serverbn import FedBN\n",
    "from flcore.servers.serverrod import FedROD\n",
    "from flcore.servers.serverproto import FedProto\n",
    "from flcore.servers.serverdyn import FedDyn\n",
    "from flcore.servers.servermoon import MOON\n",
    "from flcore.servers.serverbabu import FedBABU\n",
    "from flcore.servers.serverapple import APPLE\n",
    "from flcore.servers.serverfedtrans import FedTrans \n",
    "\n",
    "from flcore.trainmodel.models import *\n",
    "\n",
    "from flcore.trainmodel.bilstm import BiLSTM_TextClassification\n",
    "# from flcore.trainmodel.resnet import resnet18 as resnet\n",
    "from flcore.trainmodel.alexnet import alexnet\n",
    "from flcore.trainmodel.mobilenet_v2 import mobilenet_v2\n",
    "from utils.result_utils import average_data\n",
    "from utils.mem_utils import MemReporter\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# hyper-params for Text tasks\n",
    "vocab_size = 98635\n",
    "max_len=200\n",
    "hidden_dim=32\n",
    "\n",
    "def run(args):\n",
    "\n",
    "    time_list = []\n",
    "    reporter = MemReporter()\n",
    "    model_str = args.model\n",
    "\n",
    "    for i in range(args.prev, args.times):\n",
    "        print(f\"\\n============= Running time: {i}th =============\")\n",
    "        print(\"Creating server and clients ...\")\n",
    "        start = time.time()\n",
    "\n",
    "        # Generate args.model\n",
    "        if model_str == \"mlr\":\n",
    "            if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = Mclr_Logistic(1*28*28, num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "                args.model = Mclr_Logistic(3*32*32, num_classes=args.num_classes).to(args.device)\n",
    "            else:\n",
    "                args.model = Mclr_Logistic(60, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"cnn\":\n",
    "            if args.dataset[:5] == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = FedAvgCNN(in_features=1, num_classes=args.num_classes, dim=1024).to(args.device)\n",
    "            elif args.dataset == \"omniglot\":\n",
    "                args.model = FedAvgCNN(in_features=1, num_classes=args.num_classes, dim=33856).to(args.device)\n",
    "            elif args.dataset[:5] == \"Cifar\":\n",
    "                args.model = FedAvgCNN(in_features=3, num_classes=args.num_classes, dim=1600).to(args.device)\n",
    "                # args.model = CifarNet(num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Digit5\":\n",
    "                args.model = Digit5CNN().to(args.device)\n",
    "            else:\n",
    "                args.model = FedAvgCNN(in_features=3, num_classes=args.num_classes, dim=10816).to(args.device)\n",
    "\n",
    "        elif model_str == \"dnn\": # non-convex\n",
    "            if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = DNN(1*28*28, 100, num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "                args.model = DNN(3*32*32, 100, num_classes=args.num_classes).to(args.device)\n",
    "            else:\n",
    "                args.model = DNN(60, 20, num_classes=args.num_classes).to(args.device)\n",
    "        \n",
    "        elif model_str == \"resnet\":\n",
    "            args.model = torchvision.models.resnet18(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = torchvision.models.resnet18(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = resnet18(num_classes=args.num_classes, has_bn=True, bn_block_num=4).to(args.device)\n",
    "\n",
    "        elif model_str == \"alexnet\":\n",
    "            args.model = alexnet(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = alexnet(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "        elif model_str == \"googlenet\":\n",
    "            args.model = torchvision.models.googlenet(pretrained=False, aux_logits=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = torchvision.models.googlenet(pretrained=True, aux_logits=False).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"mobilenet_v2\":\n",
    "            args.model = mobilenet_v2(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = mobilenet_v2(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "        elif model_str == \"lstm\":\n",
    "            args.model = LSTMNet(hidden_dim=hidden_dim, vocab_size=vocab_size, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"bilstm\":\n",
    "            args.model = BiLSTM_TextClassification(input_size=vocab_size, hidden_size=hidden_dim, output_size=args.num_classes, \n",
    "                        num_layers=1, embedding_dropout=0, lstm_dropout=0, attention_dropout=0, \n",
    "                        embedding_length=hidden_dim).to(args.device)\n",
    "\n",
    "        elif model_str == \"fastText\":\n",
    "            args.model = fastText(hidden_dim=hidden_dim, vocab_size=vocab_size, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"TextCNN\":\n",
    "            args.model = TextCNN(hidden_dim=hidden_dim, max_len=max_len, vocab_size=vocab_size, \n",
    "                            num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"Transformer\":\n",
    "            args.model = TransformerModel(ntoken=vocab_size, d_model=hidden_dim, nhead=2, d_hid=hidden_dim, nlayers=2, \n",
    "                            num_classes=args.num_classes).to(args.device)\n",
    "        \n",
    "        elif model_str == \"AmazonMLP\":\n",
    "            args.model = AmazonMLP().to(args.device)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        print(args.model)\n",
    "\n",
    "        # select algorithm\n",
    "        if args.algorithm == \"FedAvg\":\n",
    "            server = FedAvg(args, i)\n",
    "\n",
    "        elif args.algorithm == \"Local\":\n",
    "            server = Local(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedMTL\":\n",
    "            server = FedMTL(args, i)\n",
    "\n",
    "        elif args.algorithm == \"PerAvg\":\n",
    "            server = PerAvg(args, i)\n",
    "\n",
    "        elif args.algorithm == \"pFedMe\":\n",
    "            server = pFedMe(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedProx\":\n",
    "            server = FedProx(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedFomo\":\n",
    "            server = FedFomo(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedAMP\":\n",
    "            server = FedAMP(args, i)\n",
    "\n",
    "        elif args.algorithm == \"APFL\":\n",
    "            server = APFL(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedPer\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedPer(args, i)\n",
    "\n",
    "        elif args.algorithm == \"Ditto\":\n",
    "            server = Ditto(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedRep\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedRep(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedPHP\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedPHP(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedBN\":\n",
    "            server = FedBN(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedROD\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedROD(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedProto\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedProto(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedDyn\":\n",
    "            server = FedDyn(args, i)\n",
    "\n",
    "        elif args.algorithm == \"MOON\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = MOON(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedBABU\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedBABU(args, i)\n",
    "\n",
    "        elif args.algorithm == \"APPLE\":\n",
    "            server = APPLE(args, i)\n",
    "            \n",
    "        elif args.algorithm == \"FedTrans\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedTrans(args, i)\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    return server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_start = time.time()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # general\n",
    "    parser.add_argument('-go', \"--goal\", type=str, default=\"test\", \n",
    "                        help=\"The goal for this experiment\")\n",
    "    parser.add_argument('-dev', \"--device\", type=str, default=\"cuda\",\n",
    "                        choices=[\"cpu\", \"cuda\"])\n",
    "    parser.add_argument('-did', \"--device_id\", type=str, default=\"0\")\n",
    "    parser.add_argument('-data', \"--dataset\", type=str, default=\"mnist\")\n",
    "    parser.add_argument('-nb', \"--num_classes\", type=int, default=10)\n",
    "    parser.add_argument('-m', \"--model\", type=str, default=\"cnn\")\n",
    "    parser.add_argument('-p', \"--head\", type=str, default=\"cnn\")\n",
    "    parser.add_argument('-lbs', \"--batch_size\", type=int, default=10)\n",
    "    parser.add_argument('-lr', \"--local_learning_rate\", type=float, default=0.005,\n",
    "                        help=\"Local learning rate\")\n",
    "    parser.add_argument('-gr', \"--global_rounds\", type=int, default=1000)\n",
    "    parser.add_argument('-ls', \"--local_steps\", type=int, default=1)\n",
    "    parser.add_argument('-algo', \"--algorithm\", type=str, default=\"FedAvg\")\n",
    "    parser.add_argument('-jr', \"--join_ratio\", type=float, default=1.0,\n",
    "                        help=\"Ratio of clients per round\")\n",
    "    parser.add_argument('-rjr', \"--random_join_ratio\", type=bool, default=False,\n",
    "                        help=\"Random ratio of clients per round\")\n",
    "    parser.add_argument('-nc', \"--num_clients\", type=int, default=2,\n",
    "                        help=\"Total number of clients\")\n",
    "    parser.add_argument('-pv', \"--prev\", type=int, default=0,\n",
    "                        help=\"Previous Running times\")\n",
    "    parser.add_argument('-t', \"--times\", type=int, default=1,\n",
    "                        help=\"Running times\")\n",
    "    parser.add_argument('-eg', \"--eval_gap\", type=int, default=1,\n",
    "                        help=\"Rounds gap for evaluation\")\n",
    "    parser.add_argument('-dp', \"--privacy\", type=bool, default=False,\n",
    "                        help=\"differential privacy\")\n",
    "    parser.add_argument('-dps', \"--dp_sigma\", type=float, default=0.0)\n",
    "    parser.add_argument('-sfn', \"--save_folder_name\", type=str, default='models')\n",
    "    # practical\n",
    "    parser.add_argument('-cdr', \"--client_drop_rate\", type=float, default=0.0,\n",
    "                        help=\"Rate for clients that train but drop out\")\n",
    "    parser.add_argument('-tsr', \"--train_slow_rate\", type=float, default=0.0,\n",
    "                        help=\"The rate for slow clients when training locally\")\n",
    "    parser.add_argument('-ssr', \"--send_slow_rate\", type=float, default=0.0,\n",
    "                        help=\"The rate for slow clients when sending global model\")\n",
    "    parser.add_argument('-ts', \"--time_select\", type=bool, default=False,\n",
    "                        help=\"Whether to group and select clients at each round according to time cost\")\n",
    "    parser.add_argument('-tth', \"--time_threthold\", type=float, default=10000,\n",
    "                        help=\"The threthold for droping slow clients\")\n",
    "    # pFedMe / PerAvg / FedProx / FedAMP / FedPHP\n",
    "    parser.add_argument('-bt', \"--beta\", type=float, default=0.0,\n",
    "                        help=\"Average moving parameter for pFedMe, Second learning rate of Per-FedAvg, \\\n",
    "                        or L1 regularization weight of FedTransfer\")\n",
    "    parser.add_argument('-lam', \"--lamda\", type=float, default=1.0,\n",
    "                        help=\"Regularization weight for pFedMe and FedAMP\")\n",
    "    parser.add_argument('-mu', \"--mu\", type=float, default=0,\n",
    "                        help=\"Proximal rate for FedProx\")\n",
    "    parser.add_argument('-K', \"--K\", type=int, default=5,\n",
    "                        help=\"Number of personalized training steps for pFedMe\")\n",
    "    parser.add_argument('-lrp', \"--p_learning_rate\", type=float, default=0.01,\n",
    "                        help=\"personalized learning rate to caculate theta aproximately using K steps\")\n",
    "    # FedFomo\n",
    "    parser.add_argument('-M', \"--M\", type=int, default=5,\n",
    "                        help=\"Server only sends M client models to one client at each round\")\n",
    "    # FedMTL\n",
    "    parser.add_argument('-itk', \"--itk\", type=int, default=4000,\n",
    "                        help=\"The iterations for solving quadratic subproblems\")\n",
    "    # FedAMP\n",
    "    parser.add_argument('-alk', \"--alphaK\", type=float, default=1.0, \n",
    "                        help=\"lambda/sqrt(GLOABL-ITRATION) according to the paper\")\n",
    "    parser.add_argument('-sg', \"--sigma\", type=float, default=1.0)\n",
    "    # APFL\n",
    "    parser.add_argument('-al', \"--alpha\", type=float, default=1.0)\n",
    "    # Ditto / FedRep\n",
    "    parser.add_argument('-pls', \"--plocal_steps\", type=int, default=1)\n",
    "    # MOON\n",
    "    parser.add_argument('-ta', \"--tau\", type=float, default=1.0)\n",
    "    # FedBABU\n",
    "    parser.add_argument('-fts', \"--fine_tuning_steps\", type=int, default=1)\n",
    "    # APPLE\n",
    "    parser.add_argument('-dlr', \"--dr_learning_rate\", type=float, default=0.0)\n",
    "    parser.add_argument('-L', \"--L\", type=float, default=1.0)\n",
    "    #FedTrans\n",
    "    parser.add_argument('-ere', \"--every_recluster_eps\", type=int, default=5)\n",
    "    parser.add_argument('-ed', \"--emb_dim\", type=int, default=128)\n",
    "    parser.add_argument('-alr', \"--attn_learning_rate\", type=float, default=0.005)\n",
    "    parser.add_argument('-ncl', \"--num_cluster\", type=int, default=10)\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(args=[\"-data\",\"mnist\", \"-m\", \"cnn\", -algo FedTrans -gr 2500 -did 0 -go cnn -nc 2\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device_id\n",
    "\n",
    "    if args.device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"\\ncuda is not avaiable.\\n\")\n",
    "        args.device = \"cpu\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"Algorithm: {}\".format(args.algorithm))\n",
    "    print(\"Local batch size: {}\".format(args.batch_size))\n",
    "    print(\"Local steps: {}\".format(args.local_steps))\n",
    "    print(\"Local learing rate: {}\".format(args.local_learning_rate))\n",
    "    print(\"Total number of clients: {}\".format(args.num_clients))\n",
    "    print(\"Clients join in each round: {}\".format(args.join_ratio))\n",
    "    print(\"Client drop rate: {}\".format(args.client_drop_rate))\n",
    "    print(\"Time select: {}\".format(args.time_select))\n",
    "    print(\"Time threthold: {}\".format(args.time_threthold))\n",
    "    print(\"Global rounds: {}\".format(args.global_rounds))\n",
    "    print(\"Running times: {}\".format(args.times))\n",
    "    print(\"Dataset: {}\".format(args.dataset))\n",
    "    print(\"Local model: {}\".format(args.model))\n",
    "    print(\"Using device: {}\".format(args.device))\n",
    "\n",
    "    if args.device == \"cuda\":\n",
    "        print(\"Cuda device id: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    # if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "    #     generate_mnist('../dataset/mnist/', args.num_clients, 10, args.niid)\n",
    "    # elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "    #     generate_cifar10('../dataset/Cifar10/', args.num_clients, 10, args.niid)\n",
    "    # else:\n",
    "    #     generate_synthetic('../dataset/synthetic/', args.num_clients, 10, args.niid)\n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA],\n",
    "    #     profile_memory=True, \n",
    "    #     on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    #     ) as prof:\n",
    "    # with torch.autograd.profiler.profile(profile_memory=True) as prof:\n",
    "    server = run(args)\n",
    "\n",
    "    \n",
    "    # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
    "    # print(f\"\\nTotal time cost: {round(time.time()-total_start, 2)}s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "res = torch.load(\"cel.pt\")\n",
    "emb_list, weights = res\n",
    "print(emb_list, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.cat(emb_list, dim=0).squeeze(1)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flcore.servers.serverfedtrans import Attn_Model\n",
    "device = \"cuda:0\"\n",
    "x.to(device)\n",
    "attn_model = Attn_Model().to(device)\n",
    "weights = attn_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(\"res.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_e, iter_w = res['inter_clusters_res']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iter_e[1].size())\n",
    "\n",
    "x = torch.cat(iter_e, dim=0).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Attn_Model_C(nn.Module):\n",
    "    def __init__(self, emb_dim=128, attn_dim=128, num_heads=8):\n",
    "        super(Attn_Model_C, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.query = nn.Linear(emb_dim, attn_dim)\n",
    "        self.key = nn.Linear(emb_dim, attn_dim)\n",
    "        #self.inter_LN = nn.LayerNorm(attn_dim)\n",
    "\n",
    "        # 1-layer attention for simple verify\n",
    "\n",
    "    def forward(self, x, models=None, prev_models=None):\n",
    "        #x = self.inter_LN(x) \n",
    "        q = self.query(x)\n",
    "\n",
    "        k = self.key(x)\n",
    "        print(\"q:{}\\n{}\\nk:{}\".format(q,\"-\"*5,k))\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) \n",
    "        #scores = torch.matmul(q, k.transpose(-2, -1)) / (self.attn_dim ** 0.2)\n",
    "        print(scores)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attn_model_1 = Attn_Model_C().to(device)\n",
    "w = attn_model(x.to(device))\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in res['intra_clusters_res']:\n",
    "    if c is not None:\n",
    "        c_e = c[0]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    x = torch.cat(c_e, dim=0).squeeze(1)\n",
    "    w = attn_model_1(x.to(device))\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(res['intra_clusters_res'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"gm_avg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "ps = []\n",
    "param = [p.view(-1) for p in model.parameters()]\n",
    "ps = torch.concat(param, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试attn参数backward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model1 = torch.nn.Linear(10, 20)\n",
    "model2 = torch.nn.Linear(10, 20)\n",
    "emb_layer = torch.nn.Linear(220, 128)\n",
    "for p_1, p_2 in zip(model1.parameters(), model2.parameters()):\n",
    "    p_1.data += p_2.data\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "model1 = torch.nn.Linear(10, 20)\n",
    "model2 = torch.nn.Linear(10, 20)\n",
    "sd1 = model1.state_dict()\n",
    "sd2 = model2.state_dict()\n",
    "sd3 = OrderedDict()\n",
    "for name, param in model1.named_parameters():\n",
    "    sd3[name] = param.data.clone()\n",
    "print(\"sd1:{}\\nsd3:{}\".format(sd1,sd3))\n",
    "#for name, param in model1.named_parameters():\n",
    "#    sd2[name] = param.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn_Model(nn.Module):\n",
    "    def __init__(self, emb_dim=128, attn_dim=128, num_heads=8):\n",
    "        super(Attn_Model, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.query = nn.Linear(emb_dim, attn_dim)\n",
    "        self.key = nn.Linear(emb_dim, attn_dim)\n",
    "        #self.inter_LN = nn.LayerNorm(attn_dim)\n",
    "\n",
    "        # 1-layer attention for simple verify\n",
    "\n",
    "    def forward(self, x, models=None, prev_models=None):\n",
    "        #x = self.inter_LN(x) \n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) \n",
    "\n",
    "        #scaled coef removed since we want to diff weight matrix entries\n",
    "        #scores = torch.matmul(q, k.transpose(-2, -1)) / (self.attn_dim ** 0.5)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "##\n",
    "def w_add_params(global_model, m_weights, models_params, require_grad=False):\n",
    "    params = [] \n",
    "    res = copy.deepcopy(global_model)\n",
    "    for param in res.parameters():\n",
    "        param.data.zero_()\n",
    "            \n",
    "    for w, model_params in zip(m_weights, models_params):\n",
    "        for res_param, model_param in zip(res.parameters(), model_params):\n",
    "            if require_grad:\n",
    "                res_param.grad += model_param.grad.clone().detach() * w\n",
    "            res_param.data += model_param.data.clone().detach() * w\n",
    "    return res\n",
    "        \n",
    "        \n",
    "\n",
    "def emb(phead,emb_layer):\n",
    "    params = []\n",
    "    for p in phead.parameters():\n",
    "        params.append(p.flatten())\n",
    "    params = torch.cat(params)\n",
    "    return emb_layer(params)\n",
    "\n",
    "\n",
    "def weight_flatten(model):\n",
    "    params = []\n",
    "    for u in model.parameters():\n",
    "        params.append(u.view(-1))\n",
    "    params = torch.cat(params)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建模型以及计算weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [nn.Linear(10,20) for i in range(10)]\n",
    "flatten_models = [weight_flatten(model) for model in models]\n",
    "emb_layer = nn.Linear(len(flatten_models[0]), 128)\n",
    "models_emb = [emb(model, emb_layer) for model in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([e.reshape(1,-1) for e in models_emb], dim=0)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = Attn_Model()\n",
    "\n",
    "weights = attn_model(x)\n",
    "print(weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 暂时关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ms = []\n",
    "for i in range(weights.size()[0]):\n",
    "    w = [weights[i][j] for j in range(weights[i].size()[0])]\n",
    "    sd = copy.deepcopy(models[i].state_dict())\n",
    "    new_m = w_add_parameters(models[0], w, [model for model in models])\n",
    "    new_ms.append(new_m)\n",
    "print(len(new_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = nn.utils.parameters_to_vector(models[0].parameters())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 暂时关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = new_ms\n",
    "inner_optimizers = [torch.optim.SGD(models[i].parameters(),\\\n",
    "                                lr=0.005) for i in range(10)]\n",
    "inner_states = [copy.deepcopy(m.state_dict()) for m in models]\n",
    "\n",
    "for i in range(10):\n",
    "    #one-step local training\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = models[i](inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizers[i].step()\n",
    "    #print(\"cur_model:{}\".format(nn.utils.parameters_to_vector(models[0].parameters())))\n",
    "final_states = [m.state_dict() for m in models]\n",
    "delta_thetas = [OrderedDict({k: inner_states[i][k] - final_states[i][k] for k in models[i].state_dict().keys()}) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型参数修改函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "model = models[0]\n",
    "# 我想修改的模型state\n",
    "sd = model.state_dict()\n",
    "w = weights[0]\n",
    "# 用一个带grad的weight去aggregate模型参数\n",
    "# 把参数放回state，保存带梯度的参数到另外一个list，用于梯度计算\n",
    "print(sd)\n",
    "\n",
    "def w_add_parameters(sd, w, models):\n",
    "    sg = OrderedDict()\n",
    "    for w_i, model in zip(w, models):\n",
    "        for key in sd.keys():\n",
    "            if key not in sg.keys():\n",
    "                sg[key] = w_i * model.state_dict()[key]\n",
    "            else:\n",
    "                sg[key] = sg[key] + w_i * model.state_dict()[key]\n",
    "            print(sg)\n",
    "            sd[key] = sd[key] + sg[key].data\n",
    "    return sg, sd\n",
    "sg, sd = w_add_parameters(sd, w, models)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = models[0]\n",
    "## 我想修改的模型state\n",
    "#sd = model.state_dict()\n",
    "#w = weights[0]\n",
    "## 用一个带grad的weight去aggregate模型参数\n",
    "## 把参数放回state，保存带梯度的参数到另外一个list，用于梯度计算\n",
    "\n",
    "#sg, sd = w_add_parameters(sd, w, models)\n",
    "\n",
    "inner_state = sd\n",
    "nm = nn.Linear(10,20)\n",
    "nm.load_state_dict(sd)\n",
    "inner_optimizer = torch.optim.SGD(nm.parameters(),lr=0.05)\n",
    "inner_state = copy.deepcopy(nm.state_dict())\n",
    "for _ in range(10):\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = nm(inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizer.step()\n",
    "final_state = nm.state_dict()\n",
    "delta_theta = OrderedDict({k:inner_state[k]-final_state[k] for k in nm.state_dict().keys()})\n",
    "\n",
    "lv = list(sg.values())\n",
    "param_list = list(attn_model.parameters())\n",
    "param_list.extend(emb_layer.parameters())\n",
    "params_grads = torch.autograd.grad(lv,param_list,grad_outputs=list(delta_theta.values()),retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_state = copy.deepcopy(attn_model.state_dict())\n",
    "emb_state = copy.deepcopy(emb_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "at_state_1 = copy.deepcopy(attn_model.state_dict())\n",
    "emb_state_1 = copy.deepcopy(emb_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "optimizer = torch.optim.SGD(\n",
    "            [\n",
    "                {'params': [p for p in param_list]},\n",
    "            ], lr=lr, momentum=0.9\n",
    "        )\n",
    "for p, g in zip(param_list, params_grads):\n",
    "            p.grad = g\n",
    "torch.nn.utils.clip_grad_norm_(param_list, 50)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in param_list:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_optimizer = torch.optim.SGD(nm.parameters(),lr=0.05)\n",
    "inner_state = copy.deepcopy(nm.state_dict())\n",
    "print(inner_state)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for _ in range(10):\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = nm(inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizer.step()\n",
    "final_state = nm.state_dict()\n",
    "delta_theta = OrderedDict({k:inner_state[k]-final_state[k] for k in nm.state_dict().keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inner_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算JVP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "attn_optimizer = torch.optim.SGD(\n",
    "    [\n",
    "        {'params' : [p for p in attn_model.parameters()]} ,\n",
    "        {'params' : [p for p in emb_layer.parameters()]}\n",
    "    ], lr=lr, momentum=0.9)\n",
    "\n",
    "param_list = list(attn_model.parameters())\n",
    "param_list.extend(emb_layer.parameters())\n",
    "attn_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = [\n",
    "            torch.autograd.grad([p for p in model.parameters()], param_list , grad_outputs=list(delta_theta.values()))\\\n",
    "            for model, delta_theta in zip(models,delta_thetas)\n",
    "        ]      \n",
    "for grad in grads:\n",
    "    for param, g in zip(param_list, grad):\n",
    "        param.grad += grad \n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(param_list, 50)\n",
    "attn_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_states = model[0]\n",
    "final_states = .state_dict()\n",
    "\n",
    "# calculating delta theta\n",
    "delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
    "\n",
    "# calculating phi gradient\n",
    "hnet_grads = torch.autograd.grad(\n",
    "    list(weights.values()), hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
    ")\n",
    "\n",
    "print(torch.cat(g_1,dim=0).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0].state_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = nn.Linear(10,20)\n",
    "mo.load_state_dict(new_ms[0].state_dict())\n",
    "print(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mo.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "res = copy.deepcopy(model1)\n",
    "for param in res.parameters():\n",
    "    param.data.zero_()\n",
    "\n",
    "for rp, p in zip(res.parameters(), model1.parameters()):\n",
    "    rp.data += p.data.clone()\n",
    "    \n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beb37d7b78bf3788f54259aa41c6c1e59fac34bf95ae5ff22b978ccc20cf7a1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
