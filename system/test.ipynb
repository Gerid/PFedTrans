{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import logging\n",
    "\n",
    "from flcore.servers.serveravg import FedAvg\n",
    "from flcore.servers.serverpFedMe import pFedMe\n",
    "from flcore.servers.serverperavg import PerAvg\n",
    "from flcore.servers.serverprox import FedProx\n",
    "from flcore.servers.serverfomo import FedFomo\n",
    "from flcore.servers.serveramp import FedAMP\n",
    "from flcore.servers.servermtl import FedMTL\n",
    "from flcore.servers.serverlocal import Local\n",
    "from flcore.servers.serverper import FedPer\n",
    "from flcore.servers.serverapfl import APFL\n",
    "from flcore.servers.serverditto import Ditto\n",
    "from flcore.servers.serverrep import FedRep\n",
    "from flcore.servers.serverphp import FedPHP\n",
    "from flcore.servers.serverbn import FedBN\n",
    "from flcore.servers.serverrod import FedROD\n",
    "from flcore.servers.serverproto import FedProto\n",
    "from flcore.servers.serverdyn import FedDyn\n",
    "from flcore.servers.servermoon import MOON\n",
    "from flcore.servers.serverbabu import FedBABU\n",
    "from flcore.servers.serverapple import APPLE\n",
    "from flcore.servers.serverfedtrans import FedTrans \n",
    "\n",
    "from flcore.trainmodel.models import *\n",
    "\n",
    "from flcore.trainmodel.bilstm import BiLSTM_TextClassification\n",
    "# from flcore.trainmodel.resnet import resnet18 as resnet\n",
    "from flcore.trainmodel.alexnet import alexnet\n",
    "from flcore.trainmodel.mobilenet_v2 import mobilenet_v2\n",
    "from utils.result_utils import average_data\n",
    "from utils.mem_utils import MemReporter\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# hyper-params for Text tasks\n",
    "vocab_size = 98635\n",
    "max_len=200\n",
    "hidden_dim=32\n",
    "\n",
    "def run(args):\n",
    "\n",
    "    time_list = []\n",
    "    reporter = MemReporter()\n",
    "    model_str = args.model\n",
    "\n",
    "    for i in range(args.prev, args.times):\n",
    "        print(f\"\\n============= Running time: {i}th =============\")\n",
    "        print(\"Creating server and clients ...\")\n",
    "        start = time.time()\n",
    "\n",
    "        # Generate args.model\n",
    "        if model_str == \"mlr\":\n",
    "            if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = Mclr_Logistic(1*28*28, num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "                args.model = Mclr_Logistic(3*32*32, num_classes=args.num_classes).to(args.device)\n",
    "            else:\n",
    "                args.model = Mclr_Logistic(60, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"cnn\":\n",
    "            if args.dataset[:5] == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = FedAvgCNN(in_features=1, num_classes=args.num_classes, dim=1024).to(args.device)\n",
    "            elif args.dataset == \"omniglot\":\n",
    "                args.model = FedAvgCNN(in_features=1, num_classes=args.num_classes, dim=33856).to(args.device)\n",
    "            elif args.dataset[:5] == \"Cifar\":\n",
    "                args.model = FedAvgCNN(in_features=3, num_classes=args.num_classes, dim=1600).to(args.device)\n",
    "                # args.model = CifarNet(num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Digit5\":\n",
    "                args.model = Digit5CNN().to(args.device)\n",
    "            else:\n",
    "                args.model = FedAvgCNN(in_features=3, num_classes=args.num_classes, dim=10816).to(args.device)\n",
    "\n",
    "        elif model_str == \"dnn\": # non-convex\n",
    "            if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = DNN(1*28*28, 100, num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "                args.model = DNN(3*32*32, 100, num_classes=args.num_classes).to(args.device)\n",
    "            else:\n",
    "                args.model = DNN(60, 20, num_classes=args.num_classes).to(args.device)\n",
    "        \n",
    "        elif model_str == \"resnet\":\n",
    "            args.model = torchvision.models.resnet18(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = torchvision.models.resnet18(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = resnet18(num_classes=args.num_classes, has_bn=True, bn_block_num=4).to(args.device)\n",
    "\n",
    "        elif model_str == \"alexnet\":\n",
    "            args.model = alexnet(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = alexnet(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "        elif model_str == \"googlenet\":\n",
    "            args.model = torchvision.models.googlenet(pretrained=False, aux_logits=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = torchvision.models.googlenet(pretrained=True, aux_logits=False).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"mobilenet_v2\":\n",
    "            args.model = mobilenet_v2(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = mobilenet_v2(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "        elif model_str == \"lstm\":\n",
    "            args.model = LSTMNet(hidden_dim=hidden_dim, vocab_size=vocab_size, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"bilstm\":\n",
    "            args.model = BiLSTM_TextClassification(input_size=vocab_size, hidden_size=hidden_dim, output_size=args.num_classes, \n",
    "                        num_layers=1, embedding_dropout=0, lstm_dropout=0, attention_dropout=0, \n",
    "                        embedding_length=hidden_dim).to(args.device)\n",
    "\n",
    "        elif model_str == \"fastText\":\n",
    "            args.model = fastText(hidden_dim=hidden_dim, vocab_size=vocab_size, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"TextCNN\":\n",
    "            args.model = TextCNN(hidden_dim=hidden_dim, max_len=max_len, vocab_size=vocab_size, \n",
    "                            num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"Transformer\":\n",
    "            args.model = TransformerModel(ntoken=vocab_size, d_model=hidden_dim, nhead=2, d_hid=hidden_dim, nlayers=2, \n",
    "                            num_classes=args.num_classes).to(args.device)\n",
    "        \n",
    "        elif model_str == \"AmazonMLP\":\n",
    "            args.model = AmazonMLP().to(args.device)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        print(args.model)\n",
    "\n",
    "        # select algorithm\n",
    "        if args.algorithm == \"FedAvg\":\n",
    "            server = FedAvg(args, i)\n",
    "\n",
    "        elif args.algorithm == \"Local\":\n",
    "            server = Local(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedMTL\":\n",
    "            server = FedMTL(args, i)\n",
    "\n",
    "        elif args.algorithm == \"PerAvg\":\n",
    "            server = PerAvg(args, i)\n",
    "\n",
    "        elif args.algorithm == \"pFedMe\":\n",
    "            server = pFedMe(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedProx\":\n",
    "            server = FedProx(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedFomo\":\n",
    "            server = FedFomo(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedAMP\":\n",
    "            server = FedAMP(args, i)\n",
    "\n",
    "        elif args.algorithm == \"APFL\":\n",
    "            server = APFL(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedPer\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedPer(args, i)\n",
    "\n",
    "        elif args.algorithm == \"Ditto\":\n",
    "            server = Ditto(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedRep\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedRep(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedPHP\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedPHP(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedBN\":\n",
    "            server = FedBN(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedROD\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedROD(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedProto\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedProto(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedDyn\":\n",
    "            server = FedDyn(args, i)\n",
    "\n",
    "        elif args.algorithm == \"MOON\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = MOON(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedBABU\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedBABU(args, i)\n",
    "\n",
    "        elif args.algorithm == \"APPLE\":\n",
    "            server = APPLE(args, i)\n",
    "            \n",
    "        elif args.algorithm == \"FedTrans\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedTrans(args, i)\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    return server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_start = time.time()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # general\n",
    "    parser.add_argument('-go', \"--goal\", type=str, default=\"test\", \n",
    "                        help=\"The goal for this experiment\")\n",
    "    parser.add_argument('-dev', \"--device\", type=str, default=\"cuda\",\n",
    "                        choices=[\"cpu\", \"cuda\"])\n",
    "    parser.add_argument('-did', \"--device_id\", type=str, default=\"0\")\n",
    "    parser.add_argument('-data', \"--dataset\", type=str, default=\"mnist\")\n",
    "    parser.add_argument('-nb', \"--num_classes\", type=int, default=10)\n",
    "    parser.add_argument('-m', \"--model\", type=str, default=\"cnn\")\n",
    "    parser.add_argument('-p', \"--head\", type=str, default=\"cnn\")\n",
    "    parser.add_argument('-lbs', \"--batch_size\", type=int, default=10)\n",
    "    parser.add_argument('-lr', \"--local_learning_rate\", type=float, default=0.005,\n",
    "                        help=\"Local learning rate\")\n",
    "    parser.add_argument('-gr', \"--global_rounds\", type=int, default=1000)\n",
    "    parser.add_argument('-ls', \"--local_steps\", type=int, default=1)\n",
    "    parser.add_argument('-algo', \"--algorithm\", type=str, default=\"FedAvg\")\n",
    "    parser.add_argument('-jr', \"--join_ratio\", type=float, default=1.0,\n",
    "                        help=\"Ratio of clients per round\")\n",
    "    parser.add_argument('-rjr', \"--random_join_ratio\", type=bool, default=False,\n",
    "                        help=\"Random ratio of clients per round\")\n",
    "    parser.add_argument('-nc', \"--num_clients\", type=int, default=2,\n",
    "                        help=\"Total number of clients\")\n",
    "    parser.add_argument('-pv', \"--prev\", type=int, default=0,\n",
    "                        help=\"Previous Running times\")\n",
    "    parser.add_argument('-t', \"--times\", type=int, default=1,\n",
    "                        help=\"Running times\")\n",
    "    parser.add_argument('-eg', \"--eval_gap\", type=int, default=1,\n",
    "                        help=\"Rounds gap for evaluation\")\n",
    "    parser.add_argument('-dp', \"--privacy\", type=bool, default=False,\n",
    "                        help=\"differential privacy\")\n",
    "    parser.add_argument('-dps', \"--dp_sigma\", type=float, default=0.0)\n",
    "    parser.add_argument('-sfn', \"--save_folder_name\", type=str, default='models')\n",
    "    # practical\n",
    "    parser.add_argument('-cdr', \"--client_drop_rate\", type=float, default=0.0,\n",
    "                        help=\"Rate for clients that train but drop out\")\n",
    "    parser.add_argument('-tsr', \"--train_slow_rate\", type=float, default=0.0,\n",
    "                        help=\"The rate for slow clients when training locally\")\n",
    "    parser.add_argument('-ssr', \"--send_slow_rate\", type=float, default=0.0,\n",
    "                        help=\"The rate for slow clients when sending global model\")\n",
    "    parser.add_argument('-ts', \"--time_select\", type=bool, default=False,\n",
    "                        help=\"Whether to group and select clients at each round according to time cost\")\n",
    "    parser.add_argument('-tth', \"--time_threthold\", type=float, default=10000,\n",
    "                        help=\"The threthold for droping slow clients\")\n",
    "    # pFedMe / PerAvg / FedProx / FedAMP / FedPHP\n",
    "    parser.add_argument('-bt', \"--beta\", type=float, default=0.0,\n",
    "                        help=\"Average moving parameter for pFedMe, Second learning rate of Per-FedAvg, \\\n",
    "                        or L1 regularization weight of FedTransfer\")\n",
    "    parser.add_argument('-lam', \"--lamda\", type=float, default=1.0,\n",
    "                        help=\"Regularization weight for pFedMe and FedAMP\")\n",
    "    parser.add_argument('-mu', \"--mu\", type=float, default=0,\n",
    "                        help=\"Proximal rate for FedProx\")\n",
    "    parser.add_argument('-K', \"--K\", type=int, default=5,\n",
    "                        help=\"Number of personalized training steps for pFedMe\")\n",
    "    parser.add_argument('-lrp', \"--p_learning_rate\", type=float, default=0.01,\n",
    "                        help=\"personalized learning rate to caculate theta aproximately using K steps\")\n",
    "    # FedFomo\n",
    "    parser.add_argument('-M', \"--M\", type=int, default=5,\n",
    "                        help=\"Server only sends M client models to one client at each round\")\n",
    "    # FedMTL\n",
    "    parser.add_argument('-itk', \"--itk\", type=int, default=4000,\n",
    "                        help=\"The iterations for solving quadratic subproblems\")\n",
    "    # FedAMP\n",
    "    parser.add_argument('-alk', \"--alphaK\", type=float, default=1.0, \n",
    "                        help=\"lambda/sqrt(GLOABL-ITRATION) according to the paper\")\n",
    "    parser.add_argument('-sg', \"--sigma\", type=float, default=1.0)\n",
    "    # APFL\n",
    "    parser.add_argument('-al', \"--alpha\", type=float, default=1.0)\n",
    "    # Ditto / FedRep\n",
    "    parser.add_argument('-pls', \"--plocal_steps\", type=int, default=1)\n",
    "    # MOON\n",
    "    parser.add_argument('-ta', \"--tau\", type=float, default=1.0)\n",
    "    # FedBABU\n",
    "    parser.add_argument('-fts', \"--fine_tuning_steps\", type=int, default=1)\n",
    "    # APPLE\n",
    "    parser.add_argument('-dlr', \"--dr_learning_rate\", type=float, default=0.0)\n",
    "    parser.add_argument('-L', \"--L\", type=float, default=1.0)\n",
    "    #FedTrans\n",
    "    parser.add_argument('-ere', \"--every_recluster_eps\", type=int, default=5)\n",
    "    parser.add_argument('-ed', \"--emb_dim\", type=int, default=128)\n",
    "    parser.add_argument('-alr', \"--attn_learning_rate\", type=float, default=0.005)\n",
    "    parser.add_argument('-ncl', \"--num_cluster\", type=int, default=10)\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(args=[\"-data\",\"mnist\", \"-m\", \"cnn\", -algo FedTrans -gr 2500 -did 0 -go cnn -nc 2\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device_id\n",
    "\n",
    "    if args.device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"\\ncuda is not avaiable.\\n\")\n",
    "        args.device = \"cpu\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"Algorithm: {}\".format(args.algorithm))\n",
    "    print(\"Local batch size: {}\".format(args.batch_size))\n",
    "    print(\"Local steps: {}\".format(args.local_steps))\n",
    "    print(\"Local learing rate: {}\".format(args.local_learning_rate))\n",
    "    print(\"Total number of clients: {}\".format(args.num_clients))\n",
    "    print(\"Clients join in each round: {}\".format(args.join_ratio))\n",
    "    print(\"Client drop rate: {}\".format(args.client_drop_rate))\n",
    "    print(\"Time select: {}\".format(args.time_select))\n",
    "    print(\"Time threthold: {}\".format(args.time_threthold))\n",
    "    print(\"Global rounds: {}\".format(args.global_rounds))\n",
    "    print(\"Running times: {}\".format(args.times))\n",
    "    print(\"Dataset: {}\".format(args.dataset))\n",
    "    print(\"Local model: {}\".format(args.model))\n",
    "    print(\"Using device: {}\".format(args.device))\n",
    "\n",
    "    if args.device == \"cuda\":\n",
    "        print(\"Cuda device id: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    # if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "    #     generate_mnist('../dataset/mnist/', args.num_clients, 10, args.niid)\n",
    "    # elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "    #     generate_cifar10('../dataset/Cifar10/', args.num_clients, 10, args.niid)\n",
    "    # else:\n",
    "    #     generate_synthetic('../dataset/synthetic/', args.num_clients, 10, args.niid)\n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA],\n",
    "    #     profile_memory=True, \n",
    "    #     on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    #     ) as prof:\n",
    "    # with torch.autograd.profiler.profile(profile_memory=True) as prof:\n",
    "    server = run(args)\n",
    "\n",
    "    \n",
    "    # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
    "    # print(f\"\\nTotal time cost: {round(time.time()-total_start, 2)}s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "res = torch.load(\"cel.pt\")\n",
    "emb_list, weights = res\n",
    "print(emb_list, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.cat(emb_list, dim=0).squeeze(1)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flcore.servers.serverfedtrans import Attn_Model\n",
    "device = \"cuda:0\"\n",
    "x.to(device)\n",
    "attn_model = Attn_Model().to(device)\n",
    "weights = attn_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(\"res.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_e, iter_w = res['inter_clusters_res']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iter_e[1].size())\n",
    "\n",
    "x = torch.cat(iter_e, dim=0).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Attn_Model_C(nn.Module):\n",
    "    def __init__(self, emb_dim=128, attn_dim=128, num_heads=8):\n",
    "        super(Attn_Model_C, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.query = nn.Linear(emb_dim, attn_dim)\n",
    "        self.key = nn.Linear(emb_dim, attn_dim)\n",
    "        #self.inter_LN = nn.LayerNorm(attn_dim)\n",
    "\n",
    "        # 1-layer attention for simple verify\n",
    "\n",
    "    def forward(self, x, models=None, prev_models=None):\n",
    "        #x = self.inter_LN(x) \n",
    "        q = self.query(x)\n",
    "\n",
    "        k = self.key(x)\n",
    "        print(\"q:{}\\n{}\\nk:{}\".format(q,\"-\"*5,k))\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) \n",
    "        #scores = torch.matmul(q, k.transpose(-2, -1)) / (self.attn_dim ** 0.2)\n",
    "        print(scores)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attn_model_1 = Attn_Model_C().to(device)\n",
    "w = attn_model(x.to(device))\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in res['intra_clusters_res']:\n",
    "    if c is not None:\n",
    "        c_e = c[0]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    x = torch.cat(c_e, dim=0).squeeze(1)\n",
    "    w = attn_model_1(x.to(device))\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(res['intra_clusters_res'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"gm_avg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "ps = []\n",
    "param = [p.view(-1) for p in model.parameters()]\n",
    "ps = torch.concat(param, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试attn参数backward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model1 = torch.nn.Linear(10, 20)\n",
    "model2 = torch.nn.Linear(10, 20)\n",
    "emb_layer = torch.nn.Linear(220, 128)\n",
    "for p_1, p_2 in zip(model1.parameters(), model2.parameters()):\n",
    "    p_1.data += p_2.data\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn_Model(nn.Module):\n",
    "    def __init__(self, emb_dim=128, attn_dim=128, num_heads=8):\n",
    "        super(Attn_Model, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.query = nn.Linear(emb_dim, attn_dim)\n",
    "        self.key = nn.Linear(emb_dim, attn_dim)\n",
    "        #self.inter_LN = nn.LayerNorm(attn_dim)\n",
    "\n",
    "        # 1-layer attention for simple verify\n",
    "\n",
    "    def forward(self, x, models=None, prev_models=None):\n",
    "        #x = self.inter_LN(x) \n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) \n",
    "\n",
    "        #scaled coef removed since we want to diff weight matrix entries\n",
    "        #scores = torch.matmul(q, k.transpose(-2, -1)) / (self.attn_dim ** 0.5)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "##\n",
    "def w_add_params(global_model, m_weights, models_params, require_grad=False):\n",
    "    params = [] \n",
    "    res = copy.deepcopy(global_model)\n",
    "    for param in res.parameters():\n",
    "        param.data.zero_()\n",
    "            \n",
    "    for w, model_params in zip(m_weights, models_params):\n",
    "        for res_param, model_param in zip(res.parameters(), model_params):\n",
    "            if require_grad:\n",
    "                res_param.grad += model_param.grad.clone().detach() * w\n",
    "            res_param.data += model_param.data.clone().detach() * w\n",
    "    return res\n",
    "        \n",
    "        \n",
    "\n",
    "def emb(phead,emb_layer):\n",
    "    params = []\n",
    "    for p in phead.parameters():\n",
    "        params.append(p.flatten())\n",
    "    params = torch.cat(params)\n",
    "    return emb_layer(params)\n",
    "\n",
    "\n",
    "def weight_flatten(model):\n",
    "    params = []\n",
    "    for u in model.parameters():\n",
    "        params.append(u.view(-1))\n",
    "    params = torch.cat(params)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建模型以及计算weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [nn.Linear(10,20) for i in range(10)]\n",
    "flatten_models = [weight_flatten(model) for model in models]\n",
    "emb_layer = nn.Linear(len(flatten_models[0]), 128)\n",
    "models_emb = [emb(model, emb_layer) for model in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.cat([e.reshape(1,-1) for e in models_emb], dim=0)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0949, 0.1092, 0.0963, 0.0974, 0.1042, 0.0970, 0.1019, 0.1037, 0.1025,\n",
      "         0.0929],\n",
      "        [0.0994, 0.1079, 0.0966, 0.1017, 0.0970, 0.0882, 0.1052, 0.1042, 0.0983,\n",
      "         0.1015],\n",
      "        [0.0949, 0.1083, 0.0982, 0.1028, 0.1025, 0.0958, 0.1052, 0.0955, 0.0989,\n",
      "         0.0979],\n",
      "        [0.0967, 0.1031, 0.0998, 0.0946, 0.0983, 0.1035, 0.1029, 0.1024, 0.0975,\n",
      "         0.1012],\n",
      "        [0.1031, 0.1086, 0.0988, 0.0978, 0.0979, 0.0938, 0.0966, 0.0976, 0.1027,\n",
      "         0.1030],\n",
      "        [0.0930, 0.0988, 0.1004, 0.0998, 0.1028, 0.0996, 0.0999, 0.1036, 0.1012,\n",
      "         0.1007],\n",
      "        [0.0967, 0.1027, 0.1033, 0.1008, 0.0999, 0.0952, 0.1019, 0.1013, 0.1051,\n",
      "         0.0931],\n",
      "        [0.1039, 0.1081, 0.0989, 0.1026, 0.0988, 0.0896, 0.1023, 0.0951, 0.0994,\n",
      "         0.1012],\n",
      "        [0.0976, 0.1038, 0.0966, 0.0985, 0.0981, 0.0950, 0.1022, 0.1126, 0.1006,\n",
      "         0.0949],\n",
      "        [0.0973, 0.1048, 0.1025, 0.0967, 0.1062, 0.0977, 0.1013, 0.0957, 0.1002,\n",
      "         0.0977]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_model = Attn_Model()\n",
    "\n",
    "weights = attn_model(x)\n",
    "print(weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 暂时关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ms = []\n",
    "for i in range(weights.size()[0]):\n",
    "    w = [weights[i][j] for j in range(weights[i].size()[0])]\n",
    "    sd = copy.deepcopy(models[i].state_dict())\n",
    "    new_m = w_add_parameters(models[0], w, [model for model in models])\n",
    "    new_ms.append(new_m)\n",
    "print(len(new_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = nn.utils.parameters_to_vector(models[0].parameters())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 暂时关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = new_ms\n",
    "inner_optimizers = [torch.optim.SGD(models[i].parameters(),\\\n",
    "                                lr=0.005) for i in range(10)]\n",
    "inner_states = [m.state_dict() for m in models]\n",
    "\n",
    "for i in range(10):\n",
    "    #one-step local training\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = models[i](inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizers[i].step()\n",
    "    #print(\"cur_model:{}\".format(nn.utils.parameters_to_vector(models[0].parameters())))\n",
    "final_states = [m.state_dict() for m in models]\n",
    "delta_thetas = [OrderedDict({k: inner_states[i][k] - final_states[i][k] for k in models[i].state_dict().keys()}) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型参数修改函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0050,  0.1638,  0.1978, -0.2896, -0.2384,  0.1145,  0.2060,  0.3131,\n",
      "          0.2681, -0.0307],\n",
      "        [ 0.1209, -0.3055, -0.2936, -0.0020,  0.2846, -0.1863,  0.0252, -0.2609,\n",
      "         -0.0078,  0.2964],\n",
      "        [-0.0814, -0.0237,  0.0543, -0.1162, -0.2605, -0.2449,  0.1216,  0.1488,\n",
      "         -0.2326,  0.0494],\n",
      "        [-0.1145,  0.3135, -0.2695,  0.0320, -0.0358,  0.2624,  0.0850,  0.1804,\n",
      "          0.0408, -0.1685],\n",
      "        [-0.2717, -0.0233,  0.1323,  0.0124, -0.1795, -0.1091,  0.0885,  0.1501,\n",
      "          0.1632, -0.0206],\n",
      "        [ 0.0993, -0.1289,  0.0619,  0.1852, -0.2046, -0.2738,  0.0174,  0.0356,\n",
      "         -0.2800,  0.1538],\n",
      "        [ 0.0257,  0.0884,  0.0886, -0.0924, -0.1791, -0.1310,  0.1224, -0.0497,\n",
      "         -0.1280, -0.0931],\n",
      "        [ 0.1677, -0.2084, -0.1409, -0.1146,  0.0506, -0.1160, -0.0700, -0.1622,\n",
      "          0.0382,  0.0269],\n",
      "        [-0.1573,  0.0069, -0.1103, -0.1882,  0.1920,  0.0426,  0.0309,  0.1385,\n",
      "          0.0600,  0.0693],\n",
      "        [ 0.0156,  0.2593,  0.1430, -0.2951, -0.1246,  0.1072, -0.3090,  0.3123,\n",
      "         -0.1603, -0.1349],\n",
      "        [ 0.1508, -0.2570, -0.2907, -0.2548, -0.3011, -0.0587, -0.0291, -0.1652,\n",
      "          0.2954,  0.2124],\n",
      "        [ 0.0729, -0.0474,  0.3123,  0.2532, -0.2357, -0.1175, -0.0566, -0.2922,\n",
      "          0.2293, -0.1356],\n",
      "        [-0.1631, -0.0578, -0.2464, -0.2190,  0.3083, -0.0775, -0.2929,  0.0220,\n",
      "         -0.1479,  0.2531],\n",
      "        [-0.2239,  0.2190, -0.0559, -0.0858,  0.2813, -0.1302, -0.0838,  0.2329,\n",
      "          0.1068,  0.1022],\n",
      "        [-0.0840, -0.2723, -0.0062,  0.1504,  0.1461, -0.1596, -0.0529, -0.3120,\n",
      "          0.2684, -0.2124],\n",
      "        [ 0.2245,  0.0478, -0.0529, -0.1375, -0.1787,  0.1407,  0.0487,  0.0414,\n",
      "         -0.0757, -0.0659],\n",
      "        [ 0.1125, -0.2656, -0.1129, -0.0432, -0.0905,  0.3087,  0.2197, -0.0956,\n",
      "         -0.1388,  0.1997],\n",
      "        [-0.1341, -0.2363,  0.1643, -0.2711,  0.1658,  0.1286,  0.3068,  0.1339,\n",
      "         -0.2519, -0.2291],\n",
      "        [-0.2926,  0.2269, -0.1172, -0.1413, -0.0748,  0.0043, -0.2521,  0.0271,\n",
      "         -0.0769, -0.0998],\n",
      "        [ 0.1166, -0.2965, -0.1162,  0.0774, -0.2458,  0.1683, -0.0384, -0.0139,\n",
      "          0.0366,  0.2717]])), ('bias', tensor([-0.1452, -0.1129, -0.1697,  0.0234, -0.0747, -0.2723,  0.1669,  0.3053,\n",
      "        -0.0401, -0.2201, -0.2831, -0.0603,  0.1870, -0.2741, -0.1384,  0.0516,\n",
      "        -0.2240,  0.2447, -0.1187,  0.2617]))])\n",
      "OrderedDict([('weight', tensor([[-0.0005,  0.0156,  0.0188, -0.0275, -0.0226,  0.0109,  0.0196,  0.0297,\n",
      "          0.0255, -0.0029],\n",
      "        [ 0.0115, -0.0290, -0.0279, -0.0002,  0.0270, -0.0177,  0.0024, -0.0248,\n",
      "         -0.0007,  0.0281],\n",
      "        [-0.0077, -0.0022,  0.0052, -0.0110, -0.0247, -0.0233,  0.0115,  0.0141,\n",
      "         -0.0221,  0.0047],\n",
      "        [-0.0109,  0.0298, -0.0256,  0.0030, -0.0034,  0.0249,  0.0081,  0.0171,\n",
      "          0.0039, -0.0160],\n",
      "        [-0.0258, -0.0022,  0.0126,  0.0012, -0.0170, -0.0104,  0.0084,  0.0142,\n",
      "          0.0155, -0.0020],\n",
      "        [ 0.0094, -0.0122,  0.0059,  0.0176, -0.0194, -0.0260,  0.0017,  0.0034,\n",
      "         -0.0266,  0.0146],\n",
      "        [ 0.0024,  0.0084,  0.0084, -0.0088, -0.0170, -0.0124,  0.0116, -0.0047,\n",
      "         -0.0122, -0.0088],\n",
      "        [ 0.0159, -0.0198, -0.0134, -0.0109,  0.0048, -0.0110, -0.0066, -0.0154,\n",
      "          0.0036,  0.0026],\n",
      "        [-0.0149,  0.0007, -0.0105, -0.0179,  0.0182,  0.0040,  0.0029,  0.0132,\n",
      "          0.0057,  0.0066],\n",
      "        [ 0.0015,  0.0246,  0.0136, -0.0280, -0.0118,  0.0102, -0.0293,  0.0297,\n",
      "         -0.0152, -0.0128],\n",
      "        [ 0.0143, -0.0244, -0.0276, -0.0242, -0.0286, -0.0056, -0.0028, -0.0157,\n",
      "          0.0280,  0.0202],\n",
      "        [ 0.0069, -0.0045,  0.0296,  0.0240, -0.0224, -0.0112, -0.0054, -0.0277,\n",
      "          0.0218, -0.0129],\n",
      "        [-0.0155, -0.0055, -0.0234, -0.0208,  0.0293, -0.0074, -0.0278,  0.0021,\n",
      "         -0.0140,  0.0240],\n",
      "        [-0.0213,  0.0208, -0.0053, -0.0081,  0.0267, -0.0124, -0.0080,  0.0221,\n",
      "          0.0101,  0.0097],\n",
      "        [-0.0080, -0.0259, -0.0006,  0.0143,  0.0139, -0.0152, -0.0050, -0.0296,\n",
      "          0.0255, -0.0202],\n",
      "        [ 0.0213,  0.0045, -0.0050, -0.0131, -0.0170,  0.0134,  0.0046,  0.0039,\n",
      "         -0.0072, -0.0063],\n",
      "        [ 0.0107, -0.0252, -0.0107, -0.0041, -0.0086,  0.0293,  0.0209, -0.0091,\n",
      "         -0.0132,  0.0190],\n",
      "        [-0.0127, -0.0224,  0.0156, -0.0257,  0.0157,  0.0122,  0.0291,  0.0127,\n",
      "         -0.0239, -0.0217],\n",
      "        [-0.0278,  0.0215, -0.0111, -0.0134, -0.0071,  0.0004, -0.0239,  0.0026,\n",
      "         -0.0073, -0.0095],\n",
      "        [ 0.0111, -0.0281, -0.0110,  0.0073, -0.0233,  0.0160, -0.0036, -0.0013,\n",
      "          0.0035,  0.0258]], grad_fn=<MulBackward0>))])\n",
      "OrderedDict([('weight', tensor([[-0.0005,  0.0156,  0.0188, -0.0275, -0.0226,  0.0109,  0.0196,  0.0297,\n",
      "          0.0255, -0.0029],\n",
      "        [ 0.0115, -0.0290, -0.0279, -0.0002,  0.0270, -0.0177,  0.0024, -0.0248,\n",
      "         -0.0007,  0.0281],\n",
      "        [-0.0077, -0.0022,  0.0052, -0.0110, -0.0247, -0.0233,  0.0115,  0.0141,\n",
      "         -0.0221,  0.0047],\n",
      "        [-0.0109,  0.0298, -0.0256,  0.0030, -0.0034,  0.0249,  0.0081,  0.0171,\n",
      "          0.0039, -0.0160],\n",
      "        [-0.0258, -0.0022,  0.0126,  0.0012, -0.0170, -0.0104,  0.0084,  0.0142,\n",
      "          0.0155, -0.0020],\n",
      "        [ 0.0094, -0.0122,  0.0059,  0.0176, -0.0194, -0.0260,  0.0017,  0.0034,\n",
      "         -0.0266,  0.0146],\n",
      "        [ 0.0024,  0.0084,  0.0084, -0.0088, -0.0170, -0.0124,  0.0116, -0.0047,\n",
      "         -0.0122, -0.0088],\n",
      "        [ 0.0159, -0.0198, -0.0134, -0.0109,  0.0048, -0.0110, -0.0066, -0.0154,\n",
      "          0.0036,  0.0026],\n",
      "        [-0.0149,  0.0007, -0.0105, -0.0179,  0.0182,  0.0040,  0.0029,  0.0132,\n",
      "          0.0057,  0.0066],\n",
      "        [ 0.0015,  0.0246,  0.0136, -0.0280, -0.0118,  0.0102, -0.0293,  0.0297,\n",
      "         -0.0152, -0.0128],\n",
      "        [ 0.0143, -0.0244, -0.0276, -0.0242, -0.0286, -0.0056, -0.0028, -0.0157,\n",
      "          0.0280,  0.0202],\n",
      "        [ 0.0069, -0.0045,  0.0296,  0.0240, -0.0224, -0.0112, -0.0054, -0.0277,\n",
      "          0.0218, -0.0129],\n",
      "        [-0.0155, -0.0055, -0.0234, -0.0208,  0.0293, -0.0074, -0.0278,  0.0021,\n",
      "         -0.0140,  0.0240],\n",
      "        [-0.0213,  0.0208, -0.0053, -0.0081,  0.0267, -0.0124, -0.0080,  0.0221,\n",
      "          0.0101,  0.0097],\n",
      "        [-0.0080, -0.0259, -0.0006,  0.0143,  0.0139, -0.0152, -0.0050, -0.0296,\n",
      "          0.0255, -0.0202],\n",
      "        [ 0.0213,  0.0045, -0.0050, -0.0131, -0.0170,  0.0134,  0.0046,  0.0039,\n",
      "         -0.0072, -0.0063],\n",
      "        [ 0.0107, -0.0252, -0.0107, -0.0041, -0.0086,  0.0293,  0.0209, -0.0091,\n",
      "         -0.0132,  0.0190],\n",
      "        [-0.0127, -0.0224,  0.0156, -0.0257,  0.0157,  0.0122,  0.0291,  0.0127,\n",
      "         -0.0239, -0.0217],\n",
      "        [-0.0278,  0.0215, -0.0111, -0.0134, -0.0071,  0.0004, -0.0239,  0.0026,\n",
      "         -0.0073, -0.0095],\n",
      "        [ 0.0111, -0.0281, -0.0110,  0.0073, -0.0233,  0.0160, -0.0036, -0.0013,\n",
      "          0.0035,  0.0258]], grad_fn=<MulBackward0>)), ('bias', tensor([-0.0138, -0.0107, -0.0161,  0.0022, -0.0071, -0.0259,  0.0158,  0.0290,\n",
      "        -0.0038, -0.0209, -0.0269, -0.0057,  0.0178, -0.0260, -0.0131,  0.0049,\n",
      "        -0.0213,  0.0232, -0.0113,  0.0248], grad_fn=<MulBackward0>))])\n",
      "OrderedDict([('weight', tensor([[-1.2148e-02,  1.8088e-02,  1.2188e-02,  7.0017e-03, -5.6545e-02,\n",
      "          3.6220e-02, -3.1991e-03, -3.2660e-03,  1.6830e-02, -2.8473e-02],\n",
      "        [ 2.5666e-02, -2.4885e-02, -6.1564e-02, -3.2028e-02, -2.8389e-03,\n",
      "         -2.8295e-03, -1.3143e-02, -1.7878e-02,  1.1871e-03,  2.0359e-02],\n",
      "        [-1.3618e-02,  1.1402e-02, -6.3622e-03,  1.0383e-02, -5.6582e-02,\n",
      "         -4.0211e-02,  3.5543e-02,  2.5568e-02, -5.2486e-02, -1.1648e-02],\n",
      "        [-3.1087e-02,  5.4628e-02, -2.4145e-02,  3.4280e-02,  1.5735e-02,\n",
      "         -1.3714e-03,  3.3977e-02,  3.6244e-02, -1.9087e-02,  1.6113e-02],\n",
      "        [ 3.4280e-03,  1.0048e-02,  2.4128e-02, -2.2004e-02,  1.5959e-02,\n",
      "         -2.8609e-02, -1.7524e-02, -1.4765e-02,  1.9910e-02,  1.8988e-02],\n",
      "        [-1.0325e-02, -2.2784e-02,  3.0814e-02, -1.0287e-02, -5.2780e-02,\n",
      "         -4.8925e-02, -2.9260e-02,  1.0418e-02, -3.1150e-02,  2.8342e-02],\n",
      "        [-1.8539e-02, -1.9425e-02,  1.2653e-02, -3.7030e-02, -6.7562e-03,\n",
      "          9.9331e-03, -2.0466e-03, -2.8789e-02, -1.7032e-02,  3.1242e-03],\n",
      "        [ 1.3927e-02, -5.4107e-02, -1.8844e-02,  2.1954e-02, -1.9721e-02,\n",
      "         -2.1444e-02, -2.0876e-02, -1.2619e-03, -1.3885e-02, -4.5502e-03],\n",
      "        [ 1.1376e-02,  9.6688e-03, -1.8745e-02, -3.1745e-02, -5.0865e-05,\n",
      "         -8.7378e-03,  6.6622e-03, -2.0903e-02, -2.3511e-02,  3.4099e-02],\n",
      "        [-2.3481e-03,  1.7950e-02,  2.9739e-03, -3.2801e-02,  1.1957e-02,\n",
      "          3.0588e-02, -5.1318e-03,  2.0525e-02, -1.9041e-02, -1.4186e-02],\n",
      "        [ 3.9435e-02, -3.6912e-02, -5.2368e-02,  3.6277e-03, -4.7321e-02,\n",
      "         -3.3983e-04,  1.7322e-02,  6.3937e-03,  2.7025e-02,  2.7160e-02],\n",
      "        [-1.6193e-02,  4.8613e-03, -5.0457e-04,  4.4874e-02, -2.4609e-02,\n",
      "         -1.8047e-02,  2.7310e-02, -3.5418e-03,  3.2874e-02, -1.2999e-02],\n",
      "        [-1.8626e-02, -3.1462e-02, -1.0305e-02, -2.1333e-02,  4.0044e-02,\n",
      "          2.1303e-02, -1.1882e-02,  2.5366e-02, -3.7053e-02,  2.3356e-02],\n",
      "        [-1.6744e-02,  3.9397e-02, -1.1185e-03, -5.9906e-03,  1.5186e-02,\n",
      "          6.8967e-03,  3.3465e-04,  6.2192e-03,  3.2644e-02,  4.2530e-02],\n",
      "        [ 2.1121e-02, -5.8942e-02, -8.5133e-03,  2.8428e-02,  4.7828e-02,\n",
      "         -2.2586e-02, -2.9645e-02, -5.0279e-02,  5.2170e-02, -7.5717e-03],\n",
      "        [ 2.9165e-03,  2.1631e-02, -2.4273e-02,  1.5187e-02,  1.6866e-02,\n",
      "         -1.6114e-02,  2.6674e-02, -2.5466e-03, -1.6934e-02,  2.5769e-03],\n",
      "        [ 4.2815e-02, -4.0526e-02, -4.1334e-02, -2.8490e-02, -2.4329e-02,\n",
      "          9.0972e-03,  1.9067e-02,  2.2347e-02, -3.6474e-02, -5.7944e-03],\n",
      "        [-3.8269e-02, -3.7891e-02,  2.5972e-02, -5.2444e-03,  1.4575e-02,\n",
      "          4.6489e-02,  3.0852e-02,  9.2677e-03, -1.9644e-02, -4.3931e-02],\n",
      "        [-5.9159e-03,  5.8774e-03, -2.0297e-02, -1.6251e-03, -1.2488e-02,\n",
      "          3.0436e-02,  3.8322e-03, -2.0672e-02,  1.5519e-02,  7.2329e-03],\n",
      "        [-7.3948e-03, -3.8899e-02,  4.6417e-03, -1.0823e-02, -2.3222e-02,\n",
      "          4.5353e-02, -2.3169e-02,  5.2862e-03, -6.7336e-03,  2.7967e-02]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([-0.0138, -0.0107, -0.0161,  0.0022, -0.0071, -0.0259,  0.0158,  0.0290,\n",
      "        -0.0038, -0.0209, -0.0269, -0.0057,  0.0178, -0.0260, -0.0131,  0.0049,\n",
      "        -0.0213,  0.0232, -0.0113,  0.0248], grad_fn=<MulBackward0>))])\n",
      "OrderedDict([('weight', tensor([[-1.2148e-02,  1.8088e-02,  1.2188e-02,  7.0017e-03, -5.6545e-02,\n",
      "          3.6220e-02, -3.1991e-03, -3.2660e-03,  1.6830e-02, -2.8473e-02],\n",
      "        [ 2.5666e-02, -2.4885e-02, -6.1564e-02, -3.2028e-02, -2.8389e-03,\n",
      "         -2.8295e-03, -1.3143e-02, -1.7878e-02,  1.1871e-03,  2.0359e-02],\n",
      "        [-1.3618e-02,  1.1402e-02, -6.3622e-03,  1.0383e-02, -5.6582e-02,\n",
      "         -4.0211e-02,  3.5543e-02,  2.5568e-02, -5.2486e-02, -1.1648e-02],\n",
      "        [-3.1087e-02,  5.4628e-02, -2.4145e-02,  3.4280e-02,  1.5735e-02,\n",
      "         -1.3714e-03,  3.3977e-02,  3.6244e-02, -1.9087e-02,  1.6113e-02],\n",
      "        [ 3.4280e-03,  1.0048e-02,  2.4128e-02, -2.2004e-02,  1.5959e-02,\n",
      "         -2.8609e-02, -1.7524e-02, -1.4765e-02,  1.9910e-02,  1.8988e-02],\n",
      "        [-1.0325e-02, -2.2784e-02,  3.0814e-02, -1.0287e-02, -5.2780e-02,\n",
      "         -4.8925e-02, -2.9260e-02,  1.0418e-02, -3.1150e-02,  2.8342e-02],\n",
      "        [-1.8539e-02, -1.9425e-02,  1.2653e-02, -3.7030e-02, -6.7562e-03,\n",
      "          9.9331e-03, -2.0466e-03, -2.8789e-02, -1.7032e-02,  3.1242e-03],\n",
      "        [ 1.3927e-02, -5.4107e-02, -1.8844e-02,  2.1954e-02, -1.9721e-02,\n",
      "         -2.1444e-02, -2.0876e-02, -1.2619e-03, -1.3885e-02, -4.5502e-03],\n",
      "        [ 1.1376e-02,  9.6688e-03, -1.8745e-02, -3.1745e-02, -5.0865e-05,\n",
      "         -8.7378e-03,  6.6622e-03, -2.0903e-02, -2.3511e-02,  3.4099e-02],\n",
      "        [-2.3481e-03,  1.7950e-02,  2.9739e-03, -3.2801e-02,  1.1957e-02,\n",
      "          3.0588e-02, -5.1318e-03,  2.0525e-02, -1.9041e-02, -1.4186e-02],\n",
      "        [ 3.9435e-02, -3.6912e-02, -5.2368e-02,  3.6277e-03, -4.7321e-02,\n",
      "         -3.3983e-04,  1.7322e-02,  6.3937e-03,  2.7025e-02,  2.7160e-02],\n",
      "        [-1.6193e-02,  4.8613e-03, -5.0457e-04,  4.4874e-02, -2.4609e-02,\n",
      "         -1.8047e-02,  2.7310e-02, -3.5418e-03,  3.2874e-02, -1.2999e-02],\n",
      "        [-1.8626e-02, -3.1462e-02, -1.0305e-02, -2.1333e-02,  4.0044e-02,\n",
      "          2.1303e-02, -1.1882e-02,  2.5366e-02, -3.7053e-02,  2.3356e-02],\n",
      "        [-1.6744e-02,  3.9397e-02, -1.1185e-03, -5.9906e-03,  1.5186e-02,\n",
      "          6.8967e-03,  3.3465e-04,  6.2192e-03,  3.2644e-02,  4.2530e-02],\n",
      "        [ 2.1121e-02, -5.8942e-02, -8.5133e-03,  2.8428e-02,  4.7828e-02,\n",
      "         -2.2586e-02, -2.9645e-02, -5.0279e-02,  5.2170e-02, -7.5717e-03],\n",
      "        [ 2.9165e-03,  2.1631e-02, -2.4273e-02,  1.5187e-02,  1.6866e-02,\n",
      "         -1.6114e-02,  2.6674e-02, -2.5466e-03, -1.6934e-02,  2.5769e-03],\n",
      "        [ 4.2815e-02, -4.0526e-02, -4.1334e-02, -2.8490e-02, -2.4329e-02,\n",
      "          9.0972e-03,  1.9067e-02,  2.2347e-02, -3.6474e-02, -5.7944e-03],\n",
      "        [-3.8269e-02, -3.7891e-02,  2.5972e-02, -5.2444e-03,  1.4575e-02,\n",
      "          4.6489e-02,  3.0852e-02,  9.2677e-03, -1.9644e-02, -4.3931e-02],\n",
      "        [-5.9159e-03,  5.8774e-03, -2.0297e-02, -1.6251e-03, -1.2488e-02,\n",
      "          3.0436e-02,  3.8322e-03, -2.0672e-02,  1.5519e-02,  7.2329e-03],\n",
      "        [-7.3948e-03, -3.8899e-02,  4.6417e-03, -1.0823e-02, -2.3222e-02,\n",
      "          4.5353e-02, -2.3169e-02,  5.2862e-03, -6.7336e-03,  2.7967e-02]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0083, -0.0040,  0.0010, -0.0014,  0.0104, -0.0104, -0.0002,  0.0633,\n",
      "        -0.0313,  0.0042, -0.0211, -0.0319,  0.0346, -0.0067, -0.0357,  0.0294,\n",
      "        -0.0233, -0.0086,  0.0184,  0.0329], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[-9.0249e-04,  3.6788e-02,  3.1642e-02,  3.9203e-03, -3.5217e-02,\n",
      "          6.6250e-02, -1.8343e-02,  1.0102e-02, -8.1447e-03, -5.1618e-02],\n",
      "        [ 1.7202e-02, -3.3785e-02, -9.0035e-02, -5.5569e-02, -8.3571e-03,\n",
      "          1.3317e-02, -2.9305e-02, -6.0482e-03, -4.4061e-03, -6.6483e-03],\n",
      "        [-2.6610e-03, -5.9694e-03, -3.6770e-03, -1.0084e-02, -7.0195e-02,\n",
      "         -3.6368e-02,  3.4881e-02,  2.0961e-02, -2.5459e-02, -1.8016e-02],\n",
      "        [-1.8760e-02,  7.8564e-02, -1.2597e-02,  3.1131e-02,  1.7284e-03,\n",
      "         -9.1663e-03,  1.4738e-02,  1.5997e-02, -2.5847e-02,  2.6958e-02],\n",
      "        [ 2.7731e-03,  3.2290e-02,  2.6230e-02, -5.1333e-02, -9.7880e-04,\n",
      "         -5.1356e-02, -4.5330e-02, -2.4674e-02,  1.9949e-02,  1.1793e-02],\n",
      "        [-1.0344e-02, -3.9393e-02,  2.5107e-02, -1.6575e-03, -2.9580e-02,\n",
      "         -4.8497e-02, -4.7986e-02,  8.6138e-03, -1.1483e-03,  8.0171e-03],\n",
      "        [-6.6345e-04, -9.0710e-03,  2.8107e-02, -2.0049e-02, -6.4360e-03,\n",
      "          1.4993e-02, -2.6791e-02, -3.5010e-03,  1.2109e-02,  2.5374e-02],\n",
      "        [-7.1993e-03, -3.2744e-02, -3.1253e-02,  2.8978e-02, -6.0351e-03,\n",
      "          5.1892e-03, -5.0299e-02,  7.6564e-03,  1.6176e-02,  3.1998e-03],\n",
      "        [ 2.3342e-02,  1.2332e-02, -3.3205e-02, -1.8781e-02, -1.1099e-02,\n",
      "         -8.7836e-03,  2.5062e-02, -4.8153e-02, -1.7368e-02,  4.3705e-02],\n",
      "        [-2.9725e-02,  3.8860e-02,  2.3347e-02, -6.0456e-02, -1.6533e-02,\n",
      "          3.8496e-03, -3.0367e-02,  4.8089e-02, -8.8075e-03, -3.7684e-02],\n",
      "        [ 3.7103e-02, -2.6398e-02, -7.2852e-02, -2.5239e-02, -6.3859e-02,\n",
      "         -1.6836e-02,  2.2803e-02,  1.2652e-02,  1.9832e-02,  3.8340e-02],\n",
      "        [ 7.8741e-03,  1.7109e-02,  4.2592e-03,  4.6762e-02, -1.0360e-02,\n",
      "         -9.9318e-03,  4.4508e-02,  6.9279e-03,  4.8137e-02,  5.2404e-03],\n",
      "        [-3.0929e-02, -5.9137e-02,  3.3206e-03, -1.7622e-03,  3.0642e-02,\n",
      "          2.1110e-02, -1.7615e-02,  3.6893e-03, -1.5376e-02,  3.1323e-02],\n",
      "        [-1.6385e-03,  6.8809e-02,  2.1601e-02,  1.4731e-02,  3.7560e-02,\n",
      "          2.2342e-03,  7.8424e-04,  1.0210e-03,  3.3984e-02,  5.9444e-02],\n",
      "        [ 5.0798e-02, -4.7665e-02, -1.6371e-02,  1.3128e-02,  3.4328e-02,\n",
      "         -2.7756e-02, -6.0953e-04, -3.5723e-02,  7.3009e-02, -2.8866e-02],\n",
      "        [ 9.5082e-03,  2.9831e-02, -2.3646e-02,  2.4459e-04,  2.3669e-02,\n",
      "         -1.7817e-02, -3.0488e-03, -2.9867e-02, -4.2136e-02,  1.1269e-02],\n",
      "        [ 3.9151e-02, -4.6891e-02, -5.4548e-02, -4.8682e-02, -4.6342e-02,\n",
      "         -6.5617e-03,  1.8654e-02,  9.4659e-03, -2.2949e-02,  7.3758e-03],\n",
      "        [-3.0123e-02, -2.2446e-02,  2.4373e-04, -2.5321e-03, -1.2010e-02,\n",
      "          2.9154e-02,  3.5455e-02, -2.3878e-03,  5.9067e-03, -2.9277e-02],\n",
      "        [ 8.4470e-03,  3.2786e-02, -2.0821e-02, -6.4615e-03,  4.6685e-03,\n",
      "          3.8289e-02, -1.4689e-02, -1.7228e-02,  4.1327e-02,  6.0395e-05],\n",
      "        [-3.5681e-03, -2.6452e-02, -1.1941e-02,  5.2892e-03, -4.4971e-02,\n",
      "          2.2433e-02, -5.1537e-03, -1.6263e-02, -3.2437e-02,  7.1212e-03]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0083, -0.0040,  0.0010, -0.0014,  0.0104, -0.0104, -0.0002,  0.0633,\n",
      "        -0.0313,  0.0042, -0.0211, -0.0319,  0.0346, -0.0067, -0.0357,  0.0294,\n",
      "        -0.0233, -0.0086,  0.0184,  0.0329], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[-9.0249e-04,  3.6788e-02,  3.1642e-02,  3.9203e-03, -3.5217e-02,\n",
      "          6.6250e-02, -1.8343e-02,  1.0102e-02, -8.1447e-03, -5.1618e-02],\n",
      "        [ 1.7202e-02, -3.3785e-02, -9.0035e-02, -5.5569e-02, -8.3571e-03,\n",
      "          1.3317e-02, -2.9305e-02, -6.0482e-03, -4.4061e-03, -6.6483e-03],\n",
      "        [-2.6610e-03, -5.9694e-03, -3.6770e-03, -1.0084e-02, -7.0195e-02,\n",
      "         -3.6368e-02,  3.4881e-02,  2.0961e-02, -2.5459e-02, -1.8016e-02],\n",
      "        [-1.8760e-02,  7.8564e-02, -1.2597e-02,  3.1131e-02,  1.7284e-03,\n",
      "         -9.1663e-03,  1.4738e-02,  1.5997e-02, -2.5847e-02,  2.6958e-02],\n",
      "        [ 2.7731e-03,  3.2290e-02,  2.6230e-02, -5.1333e-02, -9.7880e-04,\n",
      "         -5.1356e-02, -4.5330e-02, -2.4674e-02,  1.9949e-02,  1.1793e-02],\n",
      "        [-1.0344e-02, -3.9393e-02,  2.5107e-02, -1.6575e-03, -2.9580e-02,\n",
      "         -4.8497e-02, -4.7986e-02,  8.6138e-03, -1.1483e-03,  8.0171e-03],\n",
      "        [-6.6345e-04, -9.0710e-03,  2.8107e-02, -2.0049e-02, -6.4360e-03,\n",
      "          1.4993e-02, -2.6791e-02, -3.5010e-03,  1.2109e-02,  2.5374e-02],\n",
      "        [-7.1993e-03, -3.2744e-02, -3.1253e-02,  2.8978e-02, -6.0351e-03,\n",
      "          5.1892e-03, -5.0299e-02,  7.6564e-03,  1.6176e-02,  3.1998e-03],\n",
      "        [ 2.3342e-02,  1.2332e-02, -3.3205e-02, -1.8781e-02, -1.1099e-02,\n",
      "         -8.7836e-03,  2.5062e-02, -4.8153e-02, -1.7368e-02,  4.3705e-02],\n",
      "        [-2.9725e-02,  3.8860e-02,  2.3347e-02, -6.0456e-02, -1.6533e-02,\n",
      "          3.8496e-03, -3.0367e-02,  4.8089e-02, -8.8075e-03, -3.7684e-02],\n",
      "        [ 3.7103e-02, -2.6398e-02, -7.2852e-02, -2.5239e-02, -6.3859e-02,\n",
      "         -1.6836e-02,  2.2803e-02,  1.2652e-02,  1.9832e-02,  3.8340e-02],\n",
      "        [ 7.8741e-03,  1.7109e-02,  4.2592e-03,  4.6762e-02, -1.0360e-02,\n",
      "         -9.9318e-03,  4.4508e-02,  6.9279e-03,  4.8137e-02,  5.2404e-03],\n",
      "        [-3.0929e-02, -5.9137e-02,  3.3206e-03, -1.7622e-03,  3.0642e-02,\n",
      "          2.1110e-02, -1.7615e-02,  3.6893e-03, -1.5376e-02,  3.1323e-02],\n",
      "        [-1.6385e-03,  6.8809e-02,  2.1601e-02,  1.4731e-02,  3.7560e-02,\n",
      "          2.2342e-03,  7.8424e-04,  1.0210e-03,  3.3984e-02,  5.9444e-02],\n",
      "        [ 5.0798e-02, -4.7665e-02, -1.6371e-02,  1.3128e-02,  3.4328e-02,\n",
      "         -2.7756e-02, -6.0953e-04, -3.5723e-02,  7.3009e-02, -2.8866e-02],\n",
      "        [ 9.5082e-03,  2.9831e-02, -2.3646e-02,  2.4459e-04,  2.3669e-02,\n",
      "         -1.7817e-02, -3.0488e-03, -2.9867e-02, -4.2136e-02,  1.1269e-02],\n",
      "        [ 3.9151e-02, -4.6891e-02, -5.4548e-02, -4.8682e-02, -4.6342e-02,\n",
      "         -6.5617e-03,  1.8654e-02,  9.4659e-03, -2.2949e-02,  7.3758e-03],\n",
      "        [-3.0123e-02, -2.2446e-02,  2.4373e-04, -2.5321e-03, -1.2010e-02,\n",
      "          2.9154e-02,  3.5455e-02, -2.3878e-03,  5.9067e-03, -2.9277e-02],\n",
      "        [ 8.4470e-03,  3.2786e-02, -2.0821e-02, -6.4615e-03,  4.6685e-03,\n",
      "          3.8289e-02, -1.4689e-02, -1.7228e-02,  4.1327e-02,  6.0395e-05],\n",
      "        [-3.5681e-03, -2.6452e-02, -1.1941e-02,  5.2892e-03, -4.4971e-02,\n",
      "          2.2433e-02, -5.1537e-03, -1.6263e-02, -3.2437e-02,  7.1212e-03]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0356,  0.0156, -0.0130, -0.0154, -0.0187, -0.0071, -0.0017,  0.0403,\n",
      "        -0.0335,  0.0146, -0.0211, -0.0146,  0.0165, -0.0278, -0.0101,  0.0087,\n",
      "        -0.0535, -0.0193,  0.0233,  0.0350], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[-0.0091,  0.0165,  0.0400,  0.0261, -0.0142,  0.0386, -0.0145, -0.0056,\n",
      "         -0.0264, -0.0357],\n",
      "        [ 0.0048, -0.0579, -0.0835, -0.0412,  0.0130, -0.0087, -0.0333, -0.0294,\n",
      "          0.0002, -0.0199],\n",
      "        [-0.0233, -0.0097, -0.0235, -0.0157, -0.0712, -0.0308,  0.0306,  0.0510,\n",
      "         -0.0547,  0.0071],\n",
      "        [-0.0494,  0.0857,  0.0052,  0.0200, -0.0273,  0.0046,  0.0428,  0.0347,\n",
      "         -0.0194,  0.0046],\n",
      "        [-0.0116,  0.0329,  0.0511, -0.0223, -0.0063, -0.0671, -0.0287, -0.0184,\n",
      "          0.0385, -0.0007],\n",
      "        [ 0.0011, -0.0546,  0.0312, -0.0110, -0.0077, -0.0597, -0.0730,  0.0247,\n",
      "          0.0235,  0.0246],\n",
      "        [-0.0052, -0.0170,  0.0274, -0.0496, -0.0190,  0.0252, -0.0038, -0.0109,\n",
      "          0.0290,  0.0112],\n",
      "        [-0.0278, -0.0043, -0.0375,  0.0368,  0.0051, -0.0058, -0.0710,  0.0117,\n",
      "          0.0329, -0.0195],\n",
      "        [ 0.0183,  0.0176, -0.0226, -0.0031, -0.0105, -0.0041,  0.0127, -0.0769,\n",
      "         -0.0399,  0.0580],\n",
      "        [-0.0176,  0.0308,  0.0223, -0.0312, -0.0453,  0.0177, -0.0569,  0.0334,\n",
      "         -0.0233, -0.0564],\n",
      "        [ 0.0154,  0.0011, -0.0467, -0.0316, -0.0477, -0.0070,  0.0451,  0.0035,\n",
      "         -0.0084,  0.0552],\n",
      "        [-0.0057, -0.0032, -0.0002,  0.0681, -0.0230, -0.0021,  0.0565,  0.0308,\n",
      "          0.0357,  0.0039],\n",
      "        [-0.0600, -0.0589, -0.0172,  0.0182,  0.0402,  0.0034,  0.0024,  0.0058,\n",
      "         -0.0255,  0.0330],\n",
      "        [ 0.0177,  0.0989,  0.0523, -0.0075,  0.0316, -0.0174,  0.0293, -0.0031,\n",
      "          0.0605,  0.0397],\n",
      "        [ 0.0340, -0.0300, -0.0284,  0.0381,  0.0371, -0.0231,  0.0003, -0.0183,\n",
      "          0.0919, -0.0241],\n",
      "        [-0.0126,  0.0239, -0.0506, -0.0279, -0.0012, -0.0299, -0.0299, -0.0270,\n",
      "         -0.0499,  0.0130],\n",
      "        [ 0.0458, -0.0176, -0.0635, -0.0394, -0.0207, -0.0171,  0.0089,  0.0023,\n",
      "         -0.0247,  0.0244],\n",
      "        [-0.0556, -0.0180,  0.0212,  0.0041,  0.0016,  0.0043,  0.0329,  0.0224,\n",
      "          0.0332, -0.0167],\n",
      "        [ 0.0254,  0.0607, -0.0177,  0.0022, -0.0173,  0.0216, -0.0212, -0.0373,\n",
      "          0.0394,  0.0226],\n",
      "        [ 0.0034, -0.0211, -0.0323, -0.0081, -0.0236,  0.0359,  0.0237, -0.0388,\n",
      "         -0.0170,  0.0231]], grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0356,  0.0156, -0.0130, -0.0154, -0.0187, -0.0071, -0.0017,  0.0403,\n",
      "        -0.0335,  0.0146, -0.0211, -0.0146,  0.0165, -0.0278, -0.0101,  0.0087,\n",
      "        -0.0535, -0.0193,  0.0233,  0.0350], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[-0.0091,  0.0165,  0.0400,  0.0261, -0.0142,  0.0386, -0.0145, -0.0056,\n",
      "         -0.0264, -0.0357],\n",
      "        [ 0.0048, -0.0579, -0.0835, -0.0412,  0.0130, -0.0087, -0.0333, -0.0294,\n",
      "          0.0002, -0.0199],\n",
      "        [-0.0233, -0.0097, -0.0235, -0.0157, -0.0712, -0.0308,  0.0306,  0.0510,\n",
      "         -0.0547,  0.0071],\n",
      "        [-0.0494,  0.0857,  0.0052,  0.0200, -0.0273,  0.0046,  0.0428,  0.0347,\n",
      "         -0.0194,  0.0046],\n",
      "        [-0.0116,  0.0329,  0.0511, -0.0223, -0.0063, -0.0671, -0.0287, -0.0184,\n",
      "          0.0385, -0.0007],\n",
      "        [ 0.0011, -0.0546,  0.0312, -0.0110, -0.0077, -0.0597, -0.0730,  0.0247,\n",
      "          0.0235,  0.0246],\n",
      "        [-0.0052, -0.0170,  0.0274, -0.0496, -0.0190,  0.0252, -0.0038, -0.0109,\n",
      "          0.0290,  0.0112],\n",
      "        [-0.0278, -0.0043, -0.0375,  0.0368,  0.0051, -0.0058, -0.0710,  0.0117,\n",
      "          0.0329, -0.0195],\n",
      "        [ 0.0183,  0.0176, -0.0226, -0.0031, -0.0105, -0.0041,  0.0127, -0.0769,\n",
      "         -0.0399,  0.0580],\n",
      "        [-0.0176,  0.0308,  0.0223, -0.0312, -0.0453,  0.0177, -0.0569,  0.0334,\n",
      "         -0.0233, -0.0564],\n",
      "        [ 0.0154,  0.0011, -0.0467, -0.0316, -0.0477, -0.0070,  0.0451,  0.0035,\n",
      "         -0.0084,  0.0552],\n",
      "        [-0.0057, -0.0032, -0.0002,  0.0681, -0.0230, -0.0021,  0.0565,  0.0308,\n",
      "          0.0357,  0.0039],\n",
      "        [-0.0600, -0.0589, -0.0172,  0.0182,  0.0402,  0.0034,  0.0024,  0.0058,\n",
      "         -0.0255,  0.0330],\n",
      "        [ 0.0177,  0.0989,  0.0523, -0.0075,  0.0316, -0.0174,  0.0293, -0.0031,\n",
      "          0.0605,  0.0397],\n",
      "        [ 0.0340, -0.0300, -0.0284,  0.0381,  0.0371, -0.0231,  0.0003, -0.0183,\n",
      "          0.0919, -0.0241],\n",
      "        [-0.0126,  0.0239, -0.0506, -0.0279, -0.0012, -0.0299, -0.0299, -0.0270,\n",
      "         -0.0499,  0.0130],\n",
      "        [ 0.0458, -0.0176, -0.0635, -0.0394, -0.0207, -0.0171,  0.0089,  0.0023,\n",
      "         -0.0247,  0.0244],\n",
      "        [-0.0556, -0.0180,  0.0212,  0.0041,  0.0016,  0.0043,  0.0329,  0.0224,\n",
      "          0.0332, -0.0167],\n",
      "        [ 0.0254,  0.0607, -0.0177,  0.0022, -0.0173,  0.0216, -0.0212, -0.0373,\n",
      "          0.0394,  0.0226],\n",
      "        [ 0.0034, -0.0211, -0.0323, -0.0081, -0.0236,  0.0359,  0.0237, -0.0388,\n",
      "         -0.0170,  0.0231]], grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0376, -0.0144,  0.0016, -0.0189, -0.0096,  0.0172, -0.0022,  0.0336,\n",
      "        -0.0437,  0.0409, -0.0481, -0.0314,  0.0331, -0.0163, -0.0061,  0.0064,\n",
      "        -0.0408, -0.0396,  0.0110,  0.0416], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 1.7601e-02,  1.2862e-02,  4.6300e-02, -3.8260e-03, -2.6430e-04,\n",
      "          5.5422e-02,  3.5032e-03, -2.5032e-02, -3.6560e-02, -6.0727e-02],\n",
      "        [ 3.4618e-02, -8.7219e-02, -7.0031e-02, -5.3318e-02,  2.7095e-02,\n",
      "         -1.4816e-02, -5.8725e-02, -1.6210e-02, -3.2280e-02, -2.9128e-02],\n",
      "        [-5.3574e-02,  9.8384e-03, -5.2838e-02,  9.0756e-05, -4.6647e-02,\n",
      "         -6.2152e-03,  3.7614e-02,  1.9259e-02, -4.0447e-02,  2.1583e-02],\n",
      "        [-3.1369e-02,  8.6650e-02, -2.7359e-02,  1.5960e-02, -3.7202e-02,\n",
      "          2.4059e-02,  2.3480e-02,  5.2977e-02, -3.9616e-03,  4.0586e-03],\n",
      "        [-2.5879e-03,  4.2897e-03,  7.1282e-02, -3.9736e-02,  2.2684e-02,\n",
      "         -6.2028e-02, -1.6036e-02, -2.3145e-02,  1.2356e-02, -6.7975e-03],\n",
      "        [-1.4920e-02, -5.7102e-02,  1.5891e-02, -2.0611e-03,  5.7755e-03,\n",
      "         -5.8877e-02, -1.0357e-01,  6.8695e-03,  3.6849e-02, -7.0542e-03],\n",
      "        [-1.1871e-02, -4.5169e-02,  5.7786e-02, -2.8794e-02,  7.8381e-03,\n",
      "          5.2526e-02,  1.1010e-02,  1.3675e-02,  1.6643e-02, -2.0742e-02],\n",
      "        [-3.3154e-02,  2.1582e-03, -9.9609e-03,  5.9844e-02,  1.5985e-02,\n",
      "          4.9810e-03, -6.4562e-02,  1.4697e-02,  3.4915e-02, -1.1518e-02],\n",
      "        [ 5.0255e-02,  2.9717e-02, -5.0975e-03, -4.7963e-03, -2.2889e-02,\n",
      "          6.9832e-03,  9.8256e-03, -9.3696e-02, -2.8977e-02,  4.5619e-02],\n",
      "        [-4.1618e-02,  5.9921e-02,  1.9565e-02, -4.7138e-02, -1.5410e-02,\n",
      "          3.5254e-03, -4.4508e-02,  5.6097e-02,  9.3041e-04, -4.6026e-02],\n",
      "        [-4.2184e-03,  3.0943e-02, -2.2290e-02, -3.3303e-02, -4.8898e-02,\n",
      "          1.4995e-02,  2.0818e-02,  1.9297e-02,  2.3780e-02,  5.5987e-02],\n",
      "        [-2.6574e-02,  7.2112e-03, -2.4495e-02,  5.6822e-02, -4.8947e-02,\n",
      "          3.0484e-02,  5.1730e-02,  5.5302e-02,  1.2477e-02, -2.9702e-03],\n",
      "        [-8.2086e-02, -8.6567e-02, -3.0430e-02,  4.7406e-02,  2.6003e-02,\n",
      "          3.8203e-03,  2.6330e-02,  3.7998e-02, -5.6079e-02,  5.4515e-02],\n",
      "        [ 4.2962e-02,  1.2341e-01,  6.0851e-02, -3.3221e-02,  1.5595e-02,\n",
      "         -3.6250e-02,  6.8030e-03,  2.5910e-02,  4.5104e-02,  6.4725e-02],\n",
      "        [ 4.5118e-02, -2.7601e-02,  1.7916e-03,  2.6905e-02,  2.4632e-02,\n",
      "          5.2798e-03, -1.9130e-02, -2.1329e-02,  8.6370e-02, -2.3941e-02],\n",
      "        [-9.2670e-03,  5.0747e-02, -4.3013e-02, -1.8565e-02, -1.3886e-03,\n",
      "         -1.7911e-02,  2.0109e-03,  3.0632e-04, -8.1819e-02,  3.9991e-02],\n",
      "        [ 2.4500e-02, -3.5210e-02, -4.6877e-02, -3.1714e-02, -2.7270e-02,\n",
      "         -4.3862e-02, -6.6494e-03, -1.9588e-02, -1.3040e-02,  1.1494e-02],\n",
      "        [-2.5890e-02,  4.1992e-03,  3.7443e-02,  8.7326e-03,  3.3170e-02,\n",
      "         -1.1805e-02,  3.9138e-02,  3.3258e-02,  4.8581e-02, -4.0500e-02],\n",
      "        [ 3.3581e-02,  7.7188e-02,  5.7496e-03,  3.1468e-02,  7.1701e-03,\n",
      "          3.0354e-02,  4.1225e-03, -2.4412e-02,  1.6398e-02,  8.1094e-03],\n",
      "        [-1.5437e-02, -5.2560e-02, -3.6392e-02, -4.1382e-03, -3.5055e-03,\n",
      "          1.1849e-02,  4.0221e-02, -4.2914e-02,  4.4964e-03, -4.0948e-04]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0376, -0.0144,  0.0016, -0.0189, -0.0096,  0.0172, -0.0022,  0.0336,\n",
      "        -0.0437,  0.0409, -0.0481, -0.0314,  0.0331, -0.0163, -0.0061,  0.0064,\n",
      "        -0.0408, -0.0396,  0.0110,  0.0416], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 1.7601e-02,  1.2862e-02,  4.6300e-02, -3.8260e-03, -2.6430e-04,\n",
      "          5.5422e-02,  3.5032e-03, -2.5032e-02, -3.6560e-02, -6.0727e-02],\n",
      "        [ 3.4618e-02, -8.7219e-02, -7.0031e-02, -5.3318e-02,  2.7095e-02,\n",
      "         -1.4816e-02, -5.8725e-02, -1.6210e-02, -3.2280e-02, -2.9128e-02],\n",
      "        [-5.3574e-02,  9.8384e-03, -5.2838e-02,  9.0756e-05, -4.6647e-02,\n",
      "         -6.2152e-03,  3.7614e-02,  1.9259e-02, -4.0447e-02,  2.1583e-02],\n",
      "        [-3.1369e-02,  8.6650e-02, -2.7359e-02,  1.5960e-02, -3.7202e-02,\n",
      "          2.4059e-02,  2.3480e-02,  5.2977e-02, -3.9616e-03,  4.0586e-03],\n",
      "        [-2.5879e-03,  4.2897e-03,  7.1282e-02, -3.9736e-02,  2.2684e-02,\n",
      "         -6.2028e-02, -1.6036e-02, -2.3145e-02,  1.2356e-02, -6.7975e-03],\n",
      "        [-1.4920e-02, -5.7102e-02,  1.5891e-02, -2.0611e-03,  5.7755e-03,\n",
      "         -5.8877e-02, -1.0357e-01,  6.8695e-03,  3.6849e-02, -7.0542e-03],\n",
      "        [-1.1871e-02, -4.5169e-02,  5.7786e-02, -2.8794e-02,  7.8381e-03,\n",
      "          5.2526e-02,  1.1010e-02,  1.3675e-02,  1.6643e-02, -2.0742e-02],\n",
      "        [-3.3154e-02,  2.1582e-03, -9.9609e-03,  5.9844e-02,  1.5985e-02,\n",
      "          4.9810e-03, -6.4562e-02,  1.4697e-02,  3.4915e-02, -1.1518e-02],\n",
      "        [ 5.0255e-02,  2.9717e-02, -5.0975e-03, -4.7963e-03, -2.2889e-02,\n",
      "          6.9832e-03,  9.8256e-03, -9.3696e-02, -2.8977e-02,  4.5619e-02],\n",
      "        [-4.1618e-02,  5.9921e-02,  1.9565e-02, -4.7138e-02, -1.5410e-02,\n",
      "          3.5254e-03, -4.4508e-02,  5.6097e-02,  9.3041e-04, -4.6026e-02],\n",
      "        [-4.2184e-03,  3.0943e-02, -2.2290e-02, -3.3303e-02, -4.8898e-02,\n",
      "          1.4995e-02,  2.0818e-02,  1.9297e-02,  2.3780e-02,  5.5987e-02],\n",
      "        [-2.6574e-02,  7.2112e-03, -2.4495e-02,  5.6822e-02, -4.8947e-02,\n",
      "          3.0484e-02,  5.1730e-02,  5.5302e-02,  1.2477e-02, -2.9702e-03],\n",
      "        [-8.2086e-02, -8.6567e-02, -3.0430e-02,  4.7406e-02,  2.6003e-02,\n",
      "          3.8203e-03,  2.6330e-02,  3.7998e-02, -5.6079e-02,  5.4515e-02],\n",
      "        [ 4.2962e-02,  1.2341e-01,  6.0851e-02, -3.3221e-02,  1.5595e-02,\n",
      "         -3.6250e-02,  6.8030e-03,  2.5910e-02,  4.5104e-02,  6.4725e-02],\n",
      "        [ 4.5118e-02, -2.7601e-02,  1.7916e-03,  2.6905e-02,  2.4632e-02,\n",
      "          5.2798e-03, -1.9130e-02, -2.1329e-02,  8.6370e-02, -2.3941e-02],\n",
      "        [-9.2670e-03,  5.0747e-02, -4.3013e-02, -1.8565e-02, -1.3886e-03,\n",
      "         -1.7911e-02,  2.0109e-03,  3.0632e-04, -8.1819e-02,  3.9991e-02],\n",
      "        [ 2.4500e-02, -3.5210e-02, -4.6877e-02, -3.1714e-02, -2.7270e-02,\n",
      "         -4.3862e-02, -6.6494e-03, -1.9588e-02, -1.3040e-02,  1.1494e-02],\n",
      "        [-2.5890e-02,  4.1992e-03,  3.7443e-02,  8.7326e-03,  3.3170e-02,\n",
      "         -1.1805e-02,  3.9138e-02,  3.3258e-02,  4.8581e-02, -4.0500e-02],\n",
      "        [ 3.3581e-02,  7.7188e-02,  5.7496e-03,  3.1468e-02,  7.1701e-03,\n",
      "          3.0354e-02,  4.1225e-03, -2.4412e-02,  1.6398e-02,  8.1094e-03],\n",
      "        [-1.5437e-02, -5.2560e-02, -3.6392e-02, -4.1382e-03, -3.5055e-03,\n",
      "          1.1849e-02,  4.0221e-02, -4.2914e-02,  4.4964e-03, -4.0948e-04]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0489,  0.0074,  0.0012, -0.0320,  0.0054,  0.0365,  0.0221,  0.0374,\n",
      "        -0.0553,  0.0587, -0.0807, -0.0033,  0.0007,  0.0137,  0.0039,  0.0125,\n",
      "        -0.0244, -0.0594,  0.0398,  0.0267], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 1.3633e-02,  2.9240e-02,  6.7720e-02, -2.0693e-02,  1.9714e-02,\n",
      "          3.4259e-02, -1.0314e-02, -1.6839e-03, -1.4649e-02, -8.2065e-02],\n",
      "        [ 5.6570e-03, -9.3977e-02, -6.8452e-02, -3.0462e-02,  5.7281e-03,\n",
      "         -3.6118e-02, -4.1443e-02, -1.7062e-02, -4.7607e-03, -1.7878e-02],\n",
      "        [-3.3582e-02,  1.0850e-02, -3.6966e-02,  9.1642e-03, -2.4312e-02,\n",
      "         -1.3480e-02,  6.7326e-02,  3.4584e-02, -1.4801e-02,  2.0669e-02],\n",
      "        [-1.8422e-02,  1.0156e-01, -7.3819e-03,  1.3348e-02, -7.5654e-03,\n",
      "         -1.1022e-03,  3.6001e-02,  4.9557e-02,  6.7597e-03,  1.4434e-03],\n",
      "        [ 2.5189e-02,  3.5780e-05,  6.8802e-02, -6.1460e-02, -2.2467e-03,\n",
      "         -4.7685e-02, -2.6932e-02, -3.6423e-02,  1.1131e-02, -1.3897e-02],\n",
      "        [-2.2114e-02, -6.2606e-02,  1.7641e-02, -2.2732e-02,  1.0842e-02,\n",
      "         -3.0167e-02, -8.8703e-02, -2.0049e-02,  1.9813e-02, -2.6626e-02],\n",
      "        [ 1.1458e-02, -6.6793e-02,  4.5935e-02, -3.0742e-02, -5.7176e-03,\n",
      "          4.1821e-02,  2.6447e-02,  1.2866e-02,  4.5814e-02, -4.6392e-02],\n",
      "        [-1.7287e-02,  2.0106e-02, -1.5111e-02,  8.0263e-02,  9.5466e-03,\n",
      "         -1.7773e-02, -9.1080e-02,  4.5538e-03,  2.0659e-02,  1.7586e-02],\n",
      "        [ 2.5538e-02,  5.8980e-02,  2.1297e-02, -1.0618e-02, -1.0732e-02,\n",
      "          2.7234e-02,  2.3856e-02, -1.1728e-01, -3.0596e-02,  6.1481e-02],\n",
      "        [-3.4312e-02,  8.8902e-02,  3.2168e-02, -3.2023e-02, -3.9501e-02,\n",
      "         -1.4877e-02, -4.7301e-02,  4.8635e-02,  2.9757e-02, -7.3436e-02],\n",
      "        [-3.1557e-02,  3.4816e-02, -1.2804e-02, -3.1613e-02, -4.0459e-02,\n",
      "          4.2139e-02,  3.7640e-02,  1.5565e-02,  3.3399e-02,  6.4113e-02],\n",
      "        [-2.1708e-02,  6.0063e-03, -3.3530e-02,  2.8759e-02, -7.4086e-02,\n",
      "          2.1024e-02,  7.9423e-02,  5.6205e-02, -6.8705e-03, -2.7499e-02],\n",
      "        [-8.5639e-02, -7.8436e-02, -3.1644e-02,  2.5743e-02, -1.4249e-03,\n",
      "         -2.4789e-02, -1.4894e-03,  5.8838e-02, -6.8169e-02,  7.1918e-02],\n",
      "        [ 3.2361e-02,  1.4235e-01,  7.4187e-02, -4.0492e-03,  1.7566e-02,\n",
      "         -2.3380e-02,  7.6699e-03,  2.8731e-02,  4.1878e-02,  7.1629e-02],\n",
      "        [ 3.1383e-02, -2.7071e-02,  9.0744e-03,  1.5006e-02,  9.1807e-03,\n",
      "         -2.4273e-02,  8.8383e-04, -3.9746e-04,  9.0385e-02, -2.8390e-02],\n",
      "        [-2.8153e-03,  4.1887e-02, -2.6132e-02, -2.6002e-02,  1.7139e-02,\n",
      "         -3.1451e-02, -1.3375e-02,  5.7659e-03, -8.2328e-02,  9.6084e-03],\n",
      "        [ 2.2401e-02, -4.2374e-02, -2.5744e-02, -3.4387e-02, -4.5646e-02,\n",
      "         -4.0675e-02, -1.2108e-02, -4.0930e-02, -3.1399e-02,  1.9256e-02],\n",
      "        [-4.9465e-02, -7.4581e-03,  2.8020e-02, -3.0306e-04,  3.8072e-02,\n",
      "         -1.6235e-02,  4.1991e-02,  4.8543e-02,  3.8467e-02, -3.8589e-02],\n",
      "        [ 3.1192e-02,  6.3665e-02,  3.0948e-02,  4.7389e-02, -1.8937e-02,\n",
      "          5.3975e-03,  3.0668e-02, -3.0993e-02,  1.2098e-02,  3.1943e-02],\n",
      "        [-3.6970e-02, -2.8031e-02, -1.8525e-02,  1.3861e-02,  1.2685e-02,\n",
      "          2.5875e-02,  6.1751e-02, -1.2840e-02, -2.7058e-03,  2.6168e-02]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0489,  0.0074,  0.0012, -0.0320,  0.0054,  0.0365,  0.0221,  0.0374,\n",
      "        -0.0553,  0.0587, -0.0807, -0.0033,  0.0007,  0.0137,  0.0039,  0.0125,\n",
      "        -0.0244, -0.0594,  0.0398,  0.0267], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 1.3633e-02,  2.9240e-02,  6.7720e-02, -2.0693e-02,  1.9714e-02,\n",
      "          3.4259e-02, -1.0314e-02, -1.6839e-03, -1.4649e-02, -8.2065e-02],\n",
      "        [ 5.6570e-03, -9.3977e-02, -6.8452e-02, -3.0462e-02,  5.7281e-03,\n",
      "         -3.6118e-02, -4.1443e-02, -1.7062e-02, -4.7607e-03, -1.7878e-02],\n",
      "        [-3.3582e-02,  1.0850e-02, -3.6966e-02,  9.1642e-03, -2.4312e-02,\n",
      "         -1.3480e-02,  6.7326e-02,  3.4584e-02, -1.4801e-02,  2.0669e-02],\n",
      "        [-1.8422e-02,  1.0156e-01, -7.3819e-03,  1.3348e-02, -7.5654e-03,\n",
      "         -1.1022e-03,  3.6001e-02,  4.9557e-02,  6.7597e-03,  1.4434e-03],\n",
      "        [ 2.5189e-02,  3.5780e-05,  6.8802e-02, -6.1460e-02, -2.2467e-03,\n",
      "         -4.7685e-02, -2.6932e-02, -3.6423e-02,  1.1131e-02, -1.3897e-02],\n",
      "        [-2.2114e-02, -6.2606e-02,  1.7641e-02, -2.2732e-02,  1.0842e-02,\n",
      "         -3.0167e-02, -8.8703e-02, -2.0049e-02,  1.9813e-02, -2.6626e-02],\n",
      "        [ 1.1458e-02, -6.6793e-02,  4.5935e-02, -3.0742e-02, -5.7176e-03,\n",
      "          4.1821e-02,  2.6447e-02,  1.2866e-02,  4.5814e-02, -4.6392e-02],\n",
      "        [-1.7287e-02,  2.0106e-02, -1.5111e-02,  8.0263e-02,  9.5466e-03,\n",
      "         -1.7773e-02, -9.1080e-02,  4.5538e-03,  2.0659e-02,  1.7586e-02],\n",
      "        [ 2.5538e-02,  5.8980e-02,  2.1297e-02, -1.0618e-02, -1.0732e-02,\n",
      "          2.7234e-02,  2.3856e-02, -1.1728e-01, -3.0596e-02,  6.1481e-02],\n",
      "        [-3.4312e-02,  8.8902e-02,  3.2168e-02, -3.2023e-02, -3.9501e-02,\n",
      "         -1.4877e-02, -4.7301e-02,  4.8635e-02,  2.9757e-02, -7.3436e-02],\n",
      "        [-3.1557e-02,  3.4816e-02, -1.2804e-02, -3.1613e-02, -4.0459e-02,\n",
      "          4.2139e-02,  3.7640e-02,  1.5565e-02,  3.3399e-02,  6.4113e-02],\n",
      "        [-2.1708e-02,  6.0063e-03, -3.3530e-02,  2.8759e-02, -7.4086e-02,\n",
      "          2.1024e-02,  7.9423e-02,  5.6205e-02, -6.8705e-03, -2.7499e-02],\n",
      "        [-8.5639e-02, -7.8436e-02, -3.1644e-02,  2.5743e-02, -1.4249e-03,\n",
      "         -2.4789e-02, -1.4894e-03,  5.8838e-02, -6.8169e-02,  7.1918e-02],\n",
      "        [ 3.2361e-02,  1.4235e-01,  7.4187e-02, -4.0492e-03,  1.7566e-02,\n",
      "         -2.3380e-02,  7.6699e-03,  2.8731e-02,  4.1878e-02,  7.1629e-02],\n",
      "        [ 3.1383e-02, -2.7071e-02,  9.0744e-03,  1.5006e-02,  9.1807e-03,\n",
      "         -2.4273e-02,  8.8383e-04, -3.9746e-04,  9.0385e-02, -2.8390e-02],\n",
      "        [-2.8153e-03,  4.1887e-02, -2.6132e-02, -2.6002e-02,  1.7139e-02,\n",
      "         -3.1451e-02, -1.3375e-02,  5.7659e-03, -8.2328e-02,  9.6084e-03],\n",
      "        [ 2.2401e-02, -4.2374e-02, -2.5744e-02, -3.4387e-02, -4.5646e-02,\n",
      "         -4.0675e-02, -1.2108e-02, -4.0930e-02, -3.1399e-02,  1.9256e-02],\n",
      "        [-4.9465e-02, -7.4581e-03,  2.8020e-02, -3.0306e-04,  3.8072e-02,\n",
      "         -1.6235e-02,  4.1991e-02,  4.8543e-02,  3.8467e-02, -3.8589e-02],\n",
      "        [ 3.1192e-02,  6.3665e-02,  3.0948e-02,  4.7389e-02, -1.8937e-02,\n",
      "          5.3975e-03,  3.0668e-02, -3.0993e-02,  1.2098e-02,  3.1943e-02],\n",
      "        [-3.6970e-02, -2.8031e-02, -1.8525e-02,  1.3861e-02,  1.2685e-02,\n",
      "          2.5875e-02,  6.1751e-02, -1.2840e-02, -2.7058e-03,  2.6168e-02]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0453,  0.0091, -0.0241, -0.0154, -0.0078,  0.0081,  0.0520,  0.0193,\n",
      "        -0.0482,  0.0369, -0.0712, -0.0175,  0.0060,  0.0250,  0.0063,  0.0093,\n",
      "        -0.0266, -0.0682,  0.0179,  0.0523], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 0.0333,  0.0431,  0.0361, -0.0239,  0.0278,  0.0476,  0.0148,  0.0115,\n",
      "         -0.0174, -0.1004],\n",
      "        [ 0.0311, -0.1029, -0.0440, -0.0232,  0.0315, -0.0471, -0.0465, -0.0124,\n",
      "         -0.0085,  0.0143],\n",
      "        [-0.0612,  0.0003, -0.0322,  0.0321, -0.0212, -0.0194,  0.0933,  0.0587,\n",
      "         -0.0266,  0.0407],\n",
      "        [-0.0164,  0.1248, -0.0083,  0.0009, -0.0217, -0.0246,  0.0355,  0.0502,\n",
      "          0.0270,  0.0155],\n",
      "        [ 0.0573, -0.0009,  0.0665, -0.0356, -0.0255, -0.0609, -0.0362, -0.0437,\n",
      "          0.0116, -0.0450],\n",
      "        [-0.0286, -0.0310,  0.0076, -0.0483,  0.0149, -0.0127, -0.0605, -0.0161,\n",
      "          0.0005, -0.0108],\n",
      "        [ 0.0056, -0.0628,  0.0552, -0.0288, -0.0019,  0.0218,  0.0124,  0.0177,\n",
      "          0.0446, -0.0533],\n",
      "        [-0.0384,  0.0005, -0.0328,  0.1121,  0.0207, -0.0234, -0.0835, -0.0122,\n",
      "          0.0345,  0.0242],\n",
      "        [ 0.0068,  0.0463,  0.0342, -0.0356,  0.0089,  0.0253,  0.0131, -0.0943,\n",
      "         -0.0271,  0.0498],\n",
      "        [-0.0261,  0.0655,  0.0056, -0.0408, -0.0710, -0.0300, -0.0351,  0.0767,\n",
      "         -0.0012, -0.0825],\n",
      "        [-0.0603,  0.0350,  0.0022, -0.0624, -0.0282,  0.0262,  0.0474,  0.0298,\n",
      "          0.0152,  0.0493],\n",
      "        [ 0.0052, -0.0244, -0.0501,  0.0168, -0.0494,  0.0487,  0.0760,  0.0381,\n",
      "         -0.0285, -0.0465],\n",
      "        [-0.0766, -0.0592, -0.0368,  0.0429,  0.0071,  0.0008, -0.0259,  0.0432,\n",
      "         -0.0974,  0.0533],\n",
      "        [ 0.0272,  0.1130,  0.0830, -0.0302, -0.0002, -0.0024,  0.0286,  0.0439,\n",
      "          0.0490,  0.0834],\n",
      "        [ 0.0372, -0.0022,  0.0125,  0.0464, -0.0180, -0.0292,  0.0290, -0.0047,\n",
      "          0.0692, -0.0503],\n",
      "        [ 0.0097,  0.0519, -0.0482, -0.0198,  0.0330, -0.0230,  0.0182,  0.0194,\n",
      "         -0.0842,  0.0321],\n",
      "        [ 0.0245, -0.0247, -0.0411, -0.0259, -0.0693, -0.0438, -0.0049, -0.0227,\n",
      "         -0.0197, -0.0057],\n",
      "        [-0.0317, -0.0096,  0.0256, -0.0042,  0.0106, -0.0302,  0.0670,  0.0359,\n",
      "          0.0662, -0.0485],\n",
      "        [ 0.0385,  0.0914,  0.0590,  0.0540, -0.0296,  0.0241,  0.0475, -0.0104,\n",
      "         -0.0087,  0.0491],\n",
      "        [-0.0520, -0.0508,  0.0074,  0.0214,  0.0210,  0.0046,  0.0363, -0.0063,\n",
      "         -0.0041,  0.0070]], grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0453,  0.0091, -0.0241, -0.0154, -0.0078,  0.0081,  0.0520,  0.0193,\n",
      "        -0.0482,  0.0369, -0.0712, -0.0175,  0.0060,  0.0250,  0.0063,  0.0093,\n",
      "        -0.0266, -0.0682,  0.0179,  0.0523], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 0.0333,  0.0431,  0.0361, -0.0239,  0.0278,  0.0476,  0.0148,  0.0115,\n",
      "         -0.0174, -0.1004],\n",
      "        [ 0.0311, -0.1029, -0.0440, -0.0232,  0.0315, -0.0471, -0.0465, -0.0124,\n",
      "         -0.0085,  0.0143],\n",
      "        [-0.0612,  0.0003, -0.0322,  0.0321, -0.0212, -0.0194,  0.0933,  0.0587,\n",
      "         -0.0266,  0.0407],\n",
      "        [-0.0164,  0.1248, -0.0083,  0.0009, -0.0217, -0.0246,  0.0355,  0.0502,\n",
      "          0.0270,  0.0155],\n",
      "        [ 0.0573, -0.0009,  0.0665, -0.0356, -0.0255, -0.0609, -0.0362, -0.0437,\n",
      "          0.0116, -0.0450],\n",
      "        [-0.0286, -0.0310,  0.0076, -0.0483,  0.0149, -0.0127, -0.0605, -0.0161,\n",
      "          0.0005, -0.0108],\n",
      "        [ 0.0056, -0.0628,  0.0552, -0.0288, -0.0019,  0.0218,  0.0124,  0.0177,\n",
      "          0.0446, -0.0533],\n",
      "        [-0.0384,  0.0005, -0.0328,  0.1121,  0.0207, -0.0234, -0.0835, -0.0122,\n",
      "          0.0345,  0.0242],\n",
      "        [ 0.0068,  0.0463,  0.0342, -0.0356,  0.0089,  0.0253,  0.0131, -0.0943,\n",
      "         -0.0271,  0.0498],\n",
      "        [-0.0261,  0.0655,  0.0056, -0.0408, -0.0710, -0.0300, -0.0351,  0.0767,\n",
      "         -0.0012, -0.0825],\n",
      "        [-0.0603,  0.0350,  0.0022, -0.0624, -0.0282,  0.0262,  0.0474,  0.0298,\n",
      "          0.0152,  0.0493],\n",
      "        [ 0.0052, -0.0244, -0.0501,  0.0168, -0.0494,  0.0487,  0.0760,  0.0381,\n",
      "         -0.0285, -0.0465],\n",
      "        [-0.0766, -0.0592, -0.0368,  0.0429,  0.0071,  0.0008, -0.0259,  0.0432,\n",
      "         -0.0974,  0.0533],\n",
      "        [ 0.0272,  0.1130,  0.0830, -0.0302, -0.0002, -0.0024,  0.0286,  0.0439,\n",
      "          0.0490,  0.0834],\n",
      "        [ 0.0372, -0.0022,  0.0125,  0.0464, -0.0180, -0.0292,  0.0290, -0.0047,\n",
      "          0.0692, -0.0503],\n",
      "        [ 0.0097,  0.0519, -0.0482, -0.0198,  0.0330, -0.0230,  0.0182,  0.0194,\n",
      "         -0.0842,  0.0321],\n",
      "        [ 0.0245, -0.0247, -0.0411, -0.0259, -0.0693, -0.0438, -0.0049, -0.0227,\n",
      "         -0.0197, -0.0057],\n",
      "        [-0.0317, -0.0096,  0.0256, -0.0042,  0.0106, -0.0302,  0.0670,  0.0359,\n",
      "          0.0662, -0.0485],\n",
      "        [ 0.0385,  0.0914,  0.0590,  0.0540, -0.0296,  0.0241,  0.0475, -0.0104,\n",
      "         -0.0087,  0.0491],\n",
      "        [-0.0520, -0.0508,  0.0074,  0.0214,  0.0210,  0.0046,  0.0363, -0.0063,\n",
      "         -0.0041,  0.0070]], grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0619, -0.0163, -0.0010, -0.0293, -0.0190, -0.0115,  0.0594, -0.0030,\n",
      "        -0.0597,  0.0269, -0.0457, -0.0331,  0.0012, -0.0007,  0.0212,  0.0267,\n",
      "        -0.0284, -0.0976,  0.0058,  0.0422], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 0.0281,  0.0349,  0.0138, -0.0398,  0.0090,  0.0412,  0.0367,  0.0352,\n",
      "         -0.0408, -0.1137],\n",
      "        [ 0.0324, -0.0801, -0.0610, -0.0046,  0.0630, -0.0244, -0.0310, -0.0073,\n",
      "         -0.0163,  0.0207],\n",
      "        [-0.0640, -0.0102, -0.0366,  0.0418, -0.0108, -0.0112,  0.0775,  0.0795,\n",
      "         -0.0479,  0.0561],\n",
      "        [-0.0401,  0.0936,  0.0219,  0.0283, -0.0377, -0.0177,  0.0325,  0.0398,\n",
      "          0.0225,  0.0475],\n",
      "        [ 0.0801,  0.0181,  0.0703, -0.0491, -0.0191, -0.0293, -0.0321, -0.0208,\n",
      "         -0.0012, -0.0412],\n",
      "        [-0.0140, -0.0068,  0.0342, -0.0755,  0.0228,  0.0117, -0.0827,  0.0100,\n",
      "          0.0214,  0.0038],\n",
      "        [ 0.0335, -0.0804,  0.0859, -0.0342,  0.0093,  0.0150, -0.0200,  0.0438,\n",
      "          0.0133, -0.0342],\n",
      "        [-0.0550,  0.0192, -0.0247,  0.1272,  0.0053, -0.0246, -0.0943, -0.0051,\n",
      "          0.0056,  0.0317],\n",
      "        [ 0.0243,  0.0552,  0.0419, -0.0255,  0.0066,  0.0305,  0.0050, -0.1231,\n",
      "          0.0017,  0.0427],\n",
      "        [-0.0184,  0.0645, -0.0045, -0.0648, -0.0965, -0.0009, -0.0536,  0.0575,\n",
      "         -0.0305, -0.0614],\n",
      "        [-0.0660,  0.0153,  0.0126, -0.0350,  0.0036,  0.0343,  0.0718,  0.0444,\n",
      "          0.0375,  0.0283],\n",
      "        [-0.0092, -0.0301, -0.0468,  0.0221, -0.0448,  0.0207,  0.0453,  0.0678,\n",
      "         -0.0513, -0.0612],\n",
      "        [-0.0972, -0.0406, -0.0239,  0.0587, -0.0244, -0.0092,  0.0064,  0.0132,\n",
      "         -0.1270,  0.0720],\n",
      "        [ 0.0246,  0.0859,  0.1034, -0.0507, -0.0080,  0.0071,  0.0471,  0.0549,\n",
      "          0.0500,  0.1151],\n",
      "        [ 0.0591, -0.0184,  0.0390,  0.0538, -0.0488,  0.0036, -0.0010, -0.0014,\n",
      "          0.0740, -0.0653],\n",
      "        [ 0.0181,  0.0807, -0.0530, -0.0160,  0.0153, -0.0407, -0.0044,  0.0379,\n",
      "         -0.0603,  0.0171],\n",
      "        [ 0.0438,  0.0064, -0.0669, -0.0271, -0.0980, -0.0540,  0.0003, -0.0481,\n",
      "         -0.0096, -0.0310],\n",
      "        [-0.0364,  0.0037,  0.0031, -0.0176,  0.0009, -0.0179,  0.0387,  0.0608,\n",
      "          0.0530, -0.0435],\n",
      "        [ 0.0568,  0.1193,  0.0620,  0.0459, -0.0236,  0.0393,  0.0299, -0.0376,\n",
      "          0.0130,  0.0648],\n",
      "        [-0.0322, -0.0486,  0.0348,  0.0346, -0.0015, -0.0227,  0.0374, -0.0221,\n",
      "          0.0209,  0.0091]], grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0619, -0.0163, -0.0010, -0.0293, -0.0190, -0.0115,  0.0594, -0.0030,\n",
      "        -0.0597,  0.0269, -0.0457, -0.0331,  0.0012, -0.0007,  0.0212,  0.0267,\n",
      "        -0.0284, -0.0976,  0.0058,  0.0422], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 0.0281,  0.0349,  0.0138, -0.0398,  0.0090,  0.0412,  0.0367,  0.0352,\n",
      "         -0.0408, -0.1137],\n",
      "        [ 0.0324, -0.0801, -0.0610, -0.0046,  0.0630, -0.0244, -0.0310, -0.0073,\n",
      "         -0.0163,  0.0207],\n",
      "        [-0.0640, -0.0102, -0.0366,  0.0418, -0.0108, -0.0112,  0.0775,  0.0795,\n",
      "         -0.0479,  0.0561],\n",
      "        [-0.0401,  0.0936,  0.0219,  0.0283, -0.0377, -0.0177,  0.0325,  0.0398,\n",
      "          0.0225,  0.0475],\n",
      "        [ 0.0801,  0.0181,  0.0703, -0.0491, -0.0191, -0.0293, -0.0321, -0.0208,\n",
      "         -0.0012, -0.0412],\n",
      "        [-0.0140, -0.0068,  0.0342, -0.0755,  0.0228,  0.0117, -0.0827,  0.0100,\n",
      "          0.0214,  0.0038],\n",
      "        [ 0.0335, -0.0804,  0.0859, -0.0342,  0.0093,  0.0150, -0.0200,  0.0438,\n",
      "          0.0133, -0.0342],\n",
      "        [-0.0550,  0.0192, -0.0247,  0.1272,  0.0053, -0.0246, -0.0943, -0.0051,\n",
      "          0.0056,  0.0317],\n",
      "        [ 0.0243,  0.0552,  0.0419, -0.0255,  0.0066,  0.0305,  0.0050, -0.1231,\n",
      "          0.0017,  0.0427],\n",
      "        [-0.0184,  0.0645, -0.0045, -0.0648, -0.0965, -0.0009, -0.0536,  0.0575,\n",
      "         -0.0305, -0.0614],\n",
      "        [-0.0660,  0.0153,  0.0126, -0.0350,  0.0036,  0.0343,  0.0718,  0.0444,\n",
      "          0.0375,  0.0283],\n",
      "        [-0.0092, -0.0301, -0.0468,  0.0221, -0.0448,  0.0207,  0.0453,  0.0678,\n",
      "         -0.0513, -0.0612],\n",
      "        [-0.0972, -0.0406, -0.0239,  0.0587, -0.0244, -0.0092,  0.0064,  0.0132,\n",
      "         -0.1270,  0.0720],\n",
      "        [ 0.0246,  0.0859,  0.1034, -0.0507, -0.0080,  0.0071,  0.0471,  0.0549,\n",
      "          0.0500,  0.1151],\n",
      "        [ 0.0591, -0.0184,  0.0390,  0.0538, -0.0488,  0.0036, -0.0010, -0.0014,\n",
      "          0.0740, -0.0653],\n",
      "        [ 0.0181,  0.0807, -0.0530, -0.0160,  0.0153, -0.0407, -0.0044,  0.0379,\n",
      "         -0.0603,  0.0171],\n",
      "        [ 0.0438,  0.0064, -0.0669, -0.0271, -0.0980, -0.0540,  0.0003, -0.0481,\n",
      "         -0.0096, -0.0310],\n",
      "        [-0.0364,  0.0037,  0.0031, -0.0176,  0.0009, -0.0179,  0.0387,  0.0608,\n",
      "          0.0530, -0.0435],\n",
      "        [ 0.0568,  0.1193,  0.0620,  0.0459, -0.0236,  0.0393,  0.0299, -0.0376,\n",
      "          0.0130,  0.0648],\n",
      "        [-0.0322, -0.0486,  0.0348,  0.0346, -0.0015, -0.0227,  0.0374, -0.0221,\n",
      "          0.0209,  0.0091]], grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0613, -0.0472, -0.0106, -0.0230, -0.0375, -0.0139,  0.0787,  0.0153,\n",
      "        -0.0319,  0.0542, -0.0570, -0.0351,  0.0177,  0.0032,  0.0263,  0.0453,\n",
      "        -0.0019, -0.0911, -0.0221,  0.0616], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 1.4372e-02,  2.8668e-02, -2.0872e-03, -3.9063e-02, -1.1326e-02,\n",
      "          7.3418e-02,  3.3262e-02,  1.5249e-02, -3.1936e-02, -8.3756e-02],\n",
      "        [ 5.8733e-02, -5.9189e-02, -8.1482e-02, -2.4777e-02,  8.9369e-02,\n",
      "         -2.9525e-02, -5.9480e-02,  1.6553e-02, -1.0680e-02,  4.9972e-02],\n",
      "        [-7.6893e-02,  1.4676e-02, -2.7167e-02,  4.3472e-02, -2.3685e-02,\n",
      "         -1.8208e-02,  1.0534e-01,  8.3369e-02, -4.2059e-02,  7.0783e-02],\n",
      "        [-5.2934e-02,  7.8817e-02,  5.9887e-03,  5.9115e-02, -3.6153e-02,\n",
      "         -1.8160e-03,  2.9566e-02,  6.3999e-02,  4.9597e-02,  5.7747e-02],\n",
      "        [ 8.2699e-02,  3.1111e-02,  5.9883e-02, -7.1432e-02, -4.5064e-02,\n",
      "         -1.4288e-03, -7.5450e-03, -1.2182e-02,  2.3932e-02, -5.4177e-02],\n",
      "        [-7.9627e-03, -1.8574e-02,  1.9308e-02, -7.0100e-02,  4.9261e-02,\n",
      "          2.6586e-02, -1.0486e-01,  3.4005e-02,  1.5065e-02,  3.0207e-02],\n",
      "        [ 3.3105e-02, -1.0643e-01,  5.9316e-02, -1.0874e-02,  1.7884e-02,\n",
      "          3.8211e-03, -3.7162e-02,  3.0830e-02, -1.4821e-02, -4.3355e-02],\n",
      "        [-2.3400e-02,  1.2588e-02, -2.0284e-02,  9.7901e-02,  2.4421e-02,\n",
      "         -1.9466e-02, -1.0665e-01, -7.1276e-03, -1.8315e-02,  4.2103e-03],\n",
      "        [ 2.4189e-02,  2.7138e-02,  2.8278e-02, -3.8865e-02,  2.8459e-02,\n",
      "          5.1993e-02, -2.7221e-02, -1.1039e-01, -8.2401e-03,  4.7195e-02],\n",
      "        [-4.4007e-03,  5.2607e-02,  8.0383e-03, -8.5125e-02, -1.1219e-01,\n",
      "         -1.4536e-02, -7.1866e-02,  5.0663e-02, -3.3440e-02, -9.1589e-02],\n",
      "        [-6.9810e-02, -1.3410e-02,  1.9286e-02, -2.8957e-02,  3.2714e-02,\n",
      "          2.0245e-02,  5.1073e-02,  5.4367e-02,  3.8051e-02,  3.1289e-03],\n",
      "        [ 6.8279e-04, -3.8116e-02, -4.1305e-02,  3.6388e-02, -1.3866e-02,\n",
      "          1.4076e-02,  6.3491e-02,  6.8466e-02, -3.9601e-02, -3.9076e-02],\n",
      "        [-1.2859e-01, -1.6520e-02, -5.1722e-02,  4.0986e-02, -5.5278e-02,\n",
      "          5.3279e-03,  3.5898e-02, -7.4166e-03, -1.2595e-01,  6.7511e-02],\n",
      "        [ 3.7845e-02,  9.8032e-02,  8.5943e-02, -6.2365e-02, -3.4024e-03,\n",
      "         -2.1061e-02,  2.9220e-02,  4.6144e-02,  3.3186e-02,  1.3108e-01],\n",
      "        [ 5.5564e-02, -4.8602e-02,  5.2848e-02,  8.3864e-02, -2.6411e-02,\n",
      "         -9.0620e-03, -1.1419e-02,  3.5705e-03,  4.5279e-02, -7.6073e-02],\n",
      "        [ 3.5677e-02,  5.0055e-02, -4.7575e-02, -4.5875e-02, -4.8962e-05,\n",
      "         -5.3762e-02, -3.5712e-02,  6.7657e-02, -7.2073e-02,  3.3250e-02],\n",
      "        [ 3.2745e-02,  2.6798e-02, -8.5519e-02, -4.3382e-02, -7.4499e-02,\n",
      "         -7.6654e-02, -1.9325e-02, -3.0461e-02, -2.3613e-02,  1.0216e-03],\n",
      "        [-5.5328e-02,  1.3101e-02, -4.0958e-03, -3.0719e-04,  2.0176e-02,\n",
      "         -3.6266e-02,  4.5205e-02,  7.4898e-02,  5.3774e-02, -2.1659e-02],\n",
      "        [ 6.0107e-02,  1.4511e-01,  7.0909e-02,  6.2226e-02, -2.0382e-02,\n",
      "          4.9988e-02,  5.5974e-02, -2.1771e-02,  9.9509e-03,  6.9133e-02],\n",
      "        [-2.3342e-02, -3.7522e-02,  3.2520e-02,  1.5921e-02,  1.7588e-02,\n",
      "         -2.5660e-02,  3.6806e-02,  4.9236e-03,  1.7147e-02,  3.3418e-04]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0613, -0.0472, -0.0106, -0.0230, -0.0375, -0.0139,  0.0787,  0.0153,\n",
      "        -0.0319,  0.0542, -0.0570, -0.0351,  0.0177,  0.0032,  0.0263,  0.0453,\n",
      "        -0.0019, -0.0911, -0.0221,  0.0616], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 1.4372e-02,  2.8668e-02, -2.0872e-03, -3.9063e-02, -1.1326e-02,\n",
      "          7.3418e-02,  3.3262e-02,  1.5249e-02, -3.1936e-02, -8.3756e-02],\n",
      "        [ 5.8733e-02, -5.9189e-02, -8.1482e-02, -2.4777e-02,  8.9369e-02,\n",
      "         -2.9525e-02, -5.9480e-02,  1.6553e-02, -1.0680e-02,  4.9972e-02],\n",
      "        [-7.6893e-02,  1.4676e-02, -2.7167e-02,  4.3472e-02, -2.3685e-02,\n",
      "         -1.8208e-02,  1.0534e-01,  8.3369e-02, -4.2059e-02,  7.0783e-02],\n",
      "        [-5.2934e-02,  7.8817e-02,  5.9887e-03,  5.9115e-02, -3.6153e-02,\n",
      "         -1.8160e-03,  2.9566e-02,  6.3999e-02,  4.9597e-02,  5.7747e-02],\n",
      "        [ 8.2699e-02,  3.1111e-02,  5.9883e-02, -7.1432e-02, -4.5064e-02,\n",
      "         -1.4288e-03, -7.5450e-03, -1.2182e-02,  2.3932e-02, -5.4177e-02],\n",
      "        [-7.9627e-03, -1.8574e-02,  1.9308e-02, -7.0100e-02,  4.9261e-02,\n",
      "          2.6586e-02, -1.0486e-01,  3.4005e-02,  1.5065e-02,  3.0207e-02],\n",
      "        [ 3.3105e-02, -1.0643e-01,  5.9316e-02, -1.0874e-02,  1.7884e-02,\n",
      "          3.8211e-03, -3.7162e-02,  3.0830e-02, -1.4821e-02, -4.3355e-02],\n",
      "        [-2.3400e-02,  1.2588e-02, -2.0284e-02,  9.7901e-02,  2.4421e-02,\n",
      "         -1.9466e-02, -1.0665e-01, -7.1276e-03, -1.8315e-02,  4.2103e-03],\n",
      "        [ 2.4189e-02,  2.7138e-02,  2.8278e-02, -3.8865e-02,  2.8459e-02,\n",
      "          5.1993e-02, -2.7221e-02, -1.1039e-01, -8.2401e-03,  4.7195e-02],\n",
      "        [-4.4007e-03,  5.2607e-02,  8.0383e-03, -8.5125e-02, -1.1219e-01,\n",
      "         -1.4536e-02, -7.1866e-02,  5.0663e-02, -3.3440e-02, -9.1589e-02],\n",
      "        [-6.9810e-02, -1.3410e-02,  1.9286e-02, -2.8957e-02,  3.2714e-02,\n",
      "          2.0245e-02,  5.1073e-02,  5.4367e-02,  3.8051e-02,  3.1289e-03],\n",
      "        [ 6.8279e-04, -3.8116e-02, -4.1305e-02,  3.6388e-02, -1.3866e-02,\n",
      "          1.4076e-02,  6.3491e-02,  6.8466e-02, -3.9601e-02, -3.9076e-02],\n",
      "        [-1.2859e-01, -1.6520e-02, -5.1722e-02,  4.0986e-02, -5.5278e-02,\n",
      "          5.3279e-03,  3.5898e-02, -7.4166e-03, -1.2595e-01,  6.7511e-02],\n",
      "        [ 3.7845e-02,  9.8032e-02,  8.5943e-02, -6.2365e-02, -3.4024e-03,\n",
      "         -2.1061e-02,  2.9220e-02,  4.6144e-02,  3.3186e-02,  1.3108e-01],\n",
      "        [ 5.5564e-02, -4.8602e-02,  5.2848e-02,  8.3864e-02, -2.6411e-02,\n",
      "         -9.0620e-03, -1.1419e-02,  3.5705e-03,  4.5279e-02, -7.6073e-02],\n",
      "        [ 3.5677e-02,  5.0055e-02, -4.7575e-02, -4.5875e-02, -4.8962e-05,\n",
      "         -5.3762e-02, -3.5712e-02,  6.7657e-02, -7.2073e-02,  3.3250e-02],\n",
      "        [ 3.2745e-02,  2.6798e-02, -8.5519e-02, -4.3382e-02, -7.4499e-02,\n",
      "         -7.6654e-02, -1.9325e-02, -3.0461e-02, -2.3613e-02,  1.0216e-03],\n",
      "        [-5.5328e-02,  1.3101e-02, -4.0958e-03, -3.0719e-04,  2.0176e-02,\n",
      "         -3.6266e-02,  4.5205e-02,  7.4898e-02,  5.3774e-02, -2.1659e-02],\n",
      "        [ 6.0107e-02,  1.4511e-01,  7.0909e-02,  6.2226e-02, -2.0382e-02,\n",
      "          4.9988e-02,  5.5974e-02, -2.1771e-02,  9.9509e-03,  6.9133e-02],\n",
      "        [-2.3342e-02, -3.7522e-02,  3.2520e-02,  1.5921e-02,  1.7588e-02,\n",
      "         -2.5660e-02,  3.6806e-02,  4.9236e-03,  1.7147e-02,  3.3418e-04]],\n",
      "       grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0843, -0.0567,  0.0083, -0.0526, -0.0462,  0.0167,  0.0661, -0.0127,\n",
      "        -0.0004,  0.0777, -0.0655, -0.0187,  0.0421,  0.0019,  0.0433,  0.0565,\n",
      "         0.0086, -0.0954, -0.0343,  0.0763], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 0.0270,  0.0532, -0.0069, -0.0212, -0.0238,  0.0506,  0.0110,  0.0075,\n",
      "         -0.0101, -0.0802],\n",
      "        [ 0.0636, -0.0722, -0.0716, -0.0199,  0.1045, -0.0195, -0.0579,  0.0312,\n",
      "         -0.0145,  0.0756],\n",
      "        [-0.0672,  0.0267, -0.0350,  0.0508, -0.0243,  0.0088,  0.1288,  0.0651,\n",
      "         -0.0621,  0.0733],\n",
      "        [-0.0516,  0.0984, -0.0215,  0.0750, -0.0093,  0.0213,  0.0490,  0.0761,\n",
      "          0.0611,  0.0643],\n",
      "        [ 0.1081,  0.0170,  0.0828, -0.0630, -0.0517,  0.0032, -0.0100, -0.0028,\n",
      "          0.0316, -0.0731],\n",
      "        [ 0.0013, -0.0419,  0.0356, -0.0472,  0.0378,  0.0265, -0.0882,  0.0411,\n",
      "          0.0121,  0.0202],\n",
      "        [ 0.0275, -0.1075,  0.0323,  0.0108, -0.0016,  0.0287, -0.0309,  0.0338,\n",
      "          0.0081, -0.0203],\n",
      "        [-0.0119,  0.0221, -0.0051,  0.0751,  0.0209, -0.0173, -0.0805, -0.0340,\n",
      "         -0.0141,  0.0126],\n",
      "        [ 0.0404,  0.0501,  0.0033, -0.0174,  0.0021,  0.0524, -0.0438, -0.1172,\n",
      "         -0.0211,  0.0453],\n",
      "        [ 0.0188,  0.0679,  0.0284, -0.0707, -0.0862, -0.0063, -0.0451,  0.0601,\n",
      "         -0.0267, -0.0857],\n",
      "        [-0.0666,  0.0102,  0.0066, -0.0569,  0.0048, -0.0012,  0.0673,  0.0761,\n",
      "          0.0600,  0.0145],\n",
      "        [ 0.0063, -0.0298, -0.0635,  0.0202, -0.0404, -0.0060,  0.0404,  0.0401,\n",
      "         -0.0591, -0.0530],\n",
      "        [-0.1217, -0.0131, -0.0375,  0.0394, -0.0790,  0.0114,  0.0331,  0.0045,\n",
      "         -0.1177,  0.0926],\n",
      "        [ 0.0582,  0.1050,  0.1116, -0.0470,  0.0235, -0.0148,  0.0473,  0.0424,\n",
      "          0.0554,  0.1439],\n",
      "        [ 0.0464, -0.0579,  0.0649,  0.0779, -0.0098,  0.0160, -0.0122, -0.0140,\n",
      "          0.0194, -0.0516],\n",
      "        [ 0.0535,  0.0624, -0.0354, -0.0338, -0.0063, -0.0484, -0.0282,  0.0418,\n",
      "         -0.0615,  0.0605],\n",
      "        [ 0.0086,  0.0059, -0.0941, -0.0638, -0.0582, -0.0717, -0.0100, -0.0239,\n",
      "         -0.0274,  0.0181],\n",
      "        [-0.0471,  0.0308, -0.0079,  0.0243, -0.0086, -0.0411,  0.0651,  0.0722,\n",
      "          0.0713,  0.0036],\n",
      "        [ 0.0474,  0.1617,  0.0648,  0.0892, -0.0440,  0.0505,  0.0491,  0.0003,\n",
      "          0.0221,  0.0456],\n",
      "        [-0.0147, -0.0585,  0.0367, -0.0025,  0.0422, -0.0533,  0.0502,  0.0017,\n",
      "         -0.0066, -0.0221]], grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0843, -0.0567,  0.0083, -0.0526, -0.0462,  0.0167,  0.0661, -0.0127,\n",
      "        -0.0004,  0.0777, -0.0655, -0.0187,  0.0421,  0.0019,  0.0433,  0.0565,\n",
      "         0.0086, -0.0954, -0.0343,  0.0763], grad_fn=<AddBackward0>))])\n",
      "OrderedDict([('weight', tensor([[ 0.0270,  0.0532, -0.0069, -0.0212, -0.0238,  0.0506,  0.0110,  0.0075,\n",
      "         -0.0101, -0.0802],\n",
      "        [ 0.0636, -0.0722, -0.0716, -0.0199,  0.1045, -0.0195, -0.0579,  0.0312,\n",
      "         -0.0145,  0.0756],\n",
      "        [-0.0672,  0.0267, -0.0350,  0.0508, -0.0243,  0.0088,  0.1288,  0.0651,\n",
      "         -0.0621,  0.0733],\n",
      "        [-0.0516,  0.0984, -0.0215,  0.0750, -0.0093,  0.0213,  0.0490,  0.0761,\n",
      "          0.0611,  0.0643],\n",
      "        [ 0.1081,  0.0170,  0.0828, -0.0630, -0.0517,  0.0032, -0.0100, -0.0028,\n",
      "          0.0316, -0.0731],\n",
      "        [ 0.0013, -0.0419,  0.0356, -0.0472,  0.0378,  0.0265, -0.0882,  0.0411,\n",
      "          0.0121,  0.0202],\n",
      "        [ 0.0275, -0.1075,  0.0323,  0.0108, -0.0016,  0.0287, -0.0309,  0.0338,\n",
      "          0.0081, -0.0203],\n",
      "        [-0.0119,  0.0221, -0.0051,  0.0751,  0.0209, -0.0173, -0.0805, -0.0340,\n",
      "         -0.0141,  0.0126],\n",
      "        [ 0.0404,  0.0501,  0.0033, -0.0174,  0.0021,  0.0524, -0.0438, -0.1172,\n",
      "         -0.0211,  0.0453],\n",
      "        [ 0.0188,  0.0679,  0.0284, -0.0707, -0.0862, -0.0063, -0.0451,  0.0601,\n",
      "         -0.0267, -0.0857],\n",
      "        [-0.0666,  0.0102,  0.0066, -0.0569,  0.0048, -0.0012,  0.0673,  0.0761,\n",
      "          0.0600,  0.0145],\n",
      "        [ 0.0063, -0.0298, -0.0635,  0.0202, -0.0404, -0.0060,  0.0404,  0.0401,\n",
      "         -0.0591, -0.0530],\n",
      "        [-0.1217, -0.0131, -0.0375,  0.0394, -0.0790,  0.0114,  0.0331,  0.0045,\n",
      "         -0.1177,  0.0926],\n",
      "        [ 0.0582,  0.1050,  0.1116, -0.0470,  0.0235, -0.0148,  0.0473,  0.0424,\n",
      "          0.0554,  0.1439],\n",
      "        [ 0.0464, -0.0579,  0.0649,  0.0779, -0.0098,  0.0160, -0.0122, -0.0140,\n",
      "          0.0194, -0.0516],\n",
      "        [ 0.0535,  0.0624, -0.0354, -0.0338, -0.0063, -0.0484, -0.0282,  0.0418,\n",
      "         -0.0615,  0.0605],\n",
      "        [ 0.0086,  0.0059, -0.0941, -0.0638, -0.0582, -0.0717, -0.0100, -0.0239,\n",
      "         -0.0274,  0.0181],\n",
      "        [-0.0471,  0.0308, -0.0079,  0.0243, -0.0086, -0.0411,  0.0651,  0.0722,\n",
      "          0.0713,  0.0036],\n",
      "        [ 0.0474,  0.1617,  0.0648,  0.0892, -0.0440,  0.0505,  0.0491,  0.0003,\n",
      "          0.0221,  0.0456],\n",
      "        [-0.0147, -0.0585,  0.0367, -0.0025,  0.0422, -0.0533,  0.0502,  0.0017,\n",
      "         -0.0066, -0.0221]], grad_fn=<AddBackward0>)), ('bias', tensor([ 0.0798, -0.0394,  0.0140, -0.0781, -0.0177, -0.0125,  0.0899, -0.0235,\n",
      "         0.0196,  0.0678, -0.0858, -0.0309,  0.0281,  0.0296,  0.0183,  0.0598,\n",
      "         0.0374, -0.0862, -0.0052,  0.0649], grad_fn=<AddBackward0>))])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = models[0]\n",
    "# 我想修改的模型state\n",
    "sd = model.state_dict()\n",
    "w = weights[0]\n",
    "# 用一个带grad的weight去aggregate模型参数\n",
    "# 把参数放回state，保存带梯度的参数到另外一个list，用于梯度计算\n",
    "print(sd)\n",
    "\n",
    "def w_add_parameters(sd, w, models):\n",
    "    sg = OrderedDict()\n",
    "    for w_i, model in zip(w, models):\n",
    "        for key in sd.keys():\n",
    "            if key not in sg.keys():\n",
    "                sg[key] = w_i * model.state_dict()[key]\n",
    "            else:\n",
    "                sg[key] = sg[key] + w_i * model.state_dict()[key]\n",
    "            print(sg)\n",
    "            sd[key] = sd[key] + sg[key].data\n",
    "    return sg, sd\n",
    "sg, sd = w_add_parameters(sd, w, models)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = models[0]\n",
    "## 我想修改的模型state\n",
    "#sd = model.state_dict()\n",
    "#w = weights[0]\n",
    "## 用一个带grad的weight去aggregate模型参数\n",
    "## 把参数放回state，保存带梯度的参数到另外一个list，用于梯度计算\n",
    "\n",
    "#sg, sd = w_add_parameters(sd, w, models)\n",
    "\n",
    "inner_state = sd\n",
    "nm = nn.Linear(10,20)\n",
    "nm.load_state_dict(sd)\n",
    "inner_optimizer = torch.optim.SGD(nm.parameters(),lr=0.05)\n",
    "inner_state = copy.deepcopy(nm.state_dict())\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for _ in range(10):\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = nm(inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizer.step()\n",
    "final_state = nm.state_dict()\n",
    "delta_theta = OrderedDict({k:inner_state[k]-final_state[k] for k in nm.state_dict().keys()})\n",
    "\n",
    "lv = list(sg.values())\n",
    "param_list = list(attn_model.parameters())\n",
    "param_list.extend(emb_layer.parameters())\n",
    "params_grads = torch.autograd.grad(lv,param_list,grad_outputs=list(delta_theta.values()),retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_state = copy.deepcopy(attn_model.state_dict())\n",
    "emb_state = copy.deepcopy(emb_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "at_state_1 = copy.deepcopy(attn_model.state_dict())\n",
    "emb_state_1 = copy.deepcopy(emb_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "optimizer = torch.optim.SGD(\n",
    "            [\n",
    "                {'params': [p for p in param_list]},\n",
    "            ], lr=lr, momentum=0.9\n",
    "        )\n",
    "for p, g in zip(param_list, params_grads):\n",
    "            p.grad = g\n",
    "torch.nn.utils.clip_grad_norm_(param_list, 50)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0596, -0.0055, -0.0820,  ...,  0.0398,  0.0013, -0.0192],\n",
      "        [ 0.0537, -0.0340,  0.0132,  ..., -0.0637, -0.0669, -0.0751],\n",
      "        [-0.0758, -0.0026, -0.0646,  ...,  0.0128, -0.0626, -0.0382],\n",
      "        ...,\n",
      "        [-0.0881,  0.0153,  0.0417,  ..., -0.0513, -0.0082,  0.0336],\n",
      "        [ 0.0765, -0.0372,  0.0602,  ..., -0.0526, -0.0588, -0.0729],\n",
      "        [ 0.0314,  0.0745,  0.0402,  ...,  0.0479, -0.0870, -0.0139]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0369,  0.0166,  0.0364,  0.0662,  0.0228, -0.0603,  0.0404,  0.0351,\n",
      "        -0.0509, -0.0375, -0.0142, -0.0340,  0.0036,  0.0808, -0.0126,  0.0247,\n",
      "        -0.0654,  0.0039, -0.0823,  0.0466,  0.0618, -0.0353, -0.0590,  0.0427,\n",
      "         0.0516, -0.0812,  0.0179,  0.0416,  0.0355, -0.0761, -0.0680,  0.0132,\n",
      "         0.0456, -0.0294, -0.0699, -0.0569,  0.0221, -0.0599,  0.0681, -0.0205,\n",
      "         0.0211,  0.0297,  0.0735, -0.0771, -0.0574, -0.0278,  0.0342,  0.0684,\n",
      "         0.0447,  0.0416,  0.0176, -0.0583,  0.0542,  0.0788, -0.0146, -0.0777,\n",
      "         0.0474,  0.0070,  0.0288, -0.0491, -0.0227, -0.0792, -0.0304, -0.0299,\n",
      "        -0.0629, -0.0295,  0.0623, -0.0460, -0.0160,  0.0156,  0.0849, -0.0495,\n",
      "        -0.0607, -0.0279,  0.0368, -0.0342, -0.0762,  0.0599,  0.0455,  0.0490,\n",
      "        -0.0338,  0.0706, -0.0038,  0.0481, -0.0395,  0.0491, -0.0293, -0.0498,\n",
      "        -0.0020, -0.0760,  0.0352,  0.0475, -0.0368, -0.0437,  0.0707, -0.0202,\n",
      "         0.0800, -0.0747,  0.0773,  0.0776, -0.0339,  0.0437,  0.0773,  0.0237,\n",
      "        -0.0396, -0.0462,  0.0654, -0.0038, -0.0741, -0.0498,  0.0628, -0.0084,\n",
      "        -0.0732, -0.0235, -0.0294, -0.0599,  0.0653,  0.0880, -0.0246, -0.0303,\n",
      "        -0.0540,  0.0258, -0.0227,  0.0469,  0.0188,  0.0261,  0.0506, -0.0645],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0687,  0.0872,  0.0677,  ..., -0.0045,  0.0299,  0.0621],\n",
      "        [ 0.0811, -0.0529,  0.0397,  ...,  0.0063, -0.0312,  0.0346],\n",
      "        [ 0.0509, -0.0040,  0.0706,  ..., -0.0806,  0.0058, -0.0503],\n",
      "        ...,\n",
      "        [-0.0158, -0.0045,  0.0062,  ..., -0.0208, -0.0592, -0.0728],\n",
      "        [-0.0855,  0.0329,  0.0513,  ..., -0.0762, -0.0737,  0.0627],\n",
      "        [ 0.0128, -0.0196,  0.0257,  ..., -0.0619, -0.0700,  0.0596]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0593,  0.0302, -0.0260, -0.0591,  0.0738,  0.0587, -0.0776, -0.0843,\n",
      "        -0.0882, -0.0660, -0.0402, -0.0591,  0.0392, -0.0386,  0.0167,  0.0473,\n",
      "        -0.0795, -0.0393,  0.0369,  0.0701,  0.0738,  0.0273,  0.0462,  0.0838,\n",
      "        -0.0571,  0.0611,  0.0228,  0.0072,  0.0474, -0.0357, -0.0372,  0.0745,\n",
      "        -0.0075,  0.0597,  0.0830,  0.0192,  0.0519, -0.0812,  0.0640, -0.0011,\n",
      "        -0.0192,  0.0588,  0.0200, -0.0725, -0.0599,  0.0604,  0.0063, -0.0229,\n",
      "        -0.0260, -0.0235,  0.0390,  0.0246, -0.0399, -0.0417, -0.0871, -0.0351,\n",
      "         0.0566, -0.0022,  0.0872, -0.0015, -0.0455,  0.0651, -0.0356,  0.0245,\n",
      "         0.0003,  0.0830,  0.0570,  0.0698,  0.0764,  0.0473, -0.0220, -0.0690,\n",
      "         0.0262, -0.0563,  0.0879, -0.0073, -0.0705,  0.0497,  0.0269, -0.0249,\n",
      "        -0.0705, -0.0133, -0.0860, -0.0630,  0.0549,  0.0763, -0.0742, -0.0088,\n",
      "        -0.0721,  0.0680, -0.0242,  0.0449,  0.0222, -0.0182, -0.0868, -0.0093,\n",
      "        -0.0331,  0.0343, -0.0038,  0.0450, -0.0719, -0.0617, -0.0557, -0.0054,\n",
      "        -0.0737, -0.0362,  0.0834,  0.0183, -0.0609, -0.0499, -0.0714, -0.0131,\n",
      "         0.0664, -0.0753,  0.0169, -0.0844, -0.0488, -0.0018,  0.0042, -0.0787,\n",
      "        -0.0104,  0.0052, -0.0718, -0.0674, -0.0344, -0.0688, -0.0349, -0.0119],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0225,  0.0082, -0.0172,  ...,  0.0089,  0.0467, -0.0511],\n",
      "        [-0.0430,  0.0504, -0.0444,  ..., -0.0147, -0.0375, -0.0533],\n",
      "        [ 0.0269,  0.0619,  0.0110,  ...,  0.0342,  0.0483, -0.0141],\n",
      "        ...,\n",
      "        [ 0.0295,  0.0662,  0.0593,  ..., -0.0090,  0.0358,  0.0331],\n",
      "        [-0.0128,  0.0260, -0.0440,  ...,  0.0618,  0.0343,  0.0336],\n",
      "        [-0.0326,  0.0570, -0.0026,  ..., -0.0460,  0.0246, -0.0570]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0419, -0.0027,  0.0297,  0.0313,  0.0629,  0.0572, -0.0059, -0.0157,\n",
      "        -0.0393, -0.0182, -0.0513, -0.0199,  0.0647, -0.0498, -0.0086, -0.0546,\n",
      "         0.0611,  0.0154,  0.0141,  0.0297,  0.0631, -0.0459,  0.0348, -0.0262,\n",
      "        -0.0178, -0.0078,  0.0485, -0.0304, -0.0004,  0.0487, -0.0257, -0.0287,\n",
      "        -0.0100,  0.0363,  0.0270,  0.0252, -0.0103, -0.0541,  0.0493,  0.0031,\n",
      "         0.0114, -0.0340, -0.0264, -0.0267, -0.0480,  0.0594,  0.0410, -0.0082,\n",
      "        -0.0205, -0.0124,  0.0178,  0.0427, -0.0055,  0.0360,  0.0514,  0.0234,\n",
      "        -0.0283, -0.0676, -0.0010,  0.0022,  0.0196, -0.0646,  0.0503, -0.0026,\n",
      "        -0.0191,  0.0524,  0.0103, -0.0379, -0.0107,  0.0417, -0.0211,  0.0003,\n",
      "        -0.0359,  0.0530, -0.0209, -0.0188,  0.0151, -0.0512,  0.0588,  0.0015,\n",
      "        -0.0369,  0.0644, -0.0071,  0.0431,  0.0280,  0.0329,  0.0214, -0.0083,\n",
      "         0.0314, -0.0550, -0.0644, -0.0328, -0.0498, -0.0650,  0.0026,  0.0268,\n",
      "        -0.0073, -0.0232,  0.0573, -0.0504,  0.0302, -0.0432, -0.0266,  0.0414,\n",
      "        -0.0073,  0.0420, -0.0088, -0.0380,  0.0193, -0.0160, -0.0566, -0.0236,\n",
      "        -0.0170,  0.0200,  0.0340, -0.0372, -0.0274,  0.0038, -0.0181,  0.0549,\n",
      "         0.0350, -0.0527,  0.0579, -0.0391, -0.0538,  0.0403,  0.0428, -0.0603],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in param_list:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 8.6763e-02,  5.0674e-01,  5.6286e-01, -4.6617e-01,  7.3899e-03,\n",
      "         -1.5633e-01,  8.0342e-02,  6.8927e-01, -7.0312e-02,  8.0618e-01],\n",
      "        [ 8.0148e-01,  1.9437e-01, -1.3462e-01,  4.1985e-02,  3.9067e-01,\n",
      "          5.9569e-01,  2.2514e-02,  3.4177e-02, -1.7175e-01, -4.6753e-01],\n",
      "        [-5.8992e-01,  7.6580e-01, -1.0324e-01,  6.3775e-01,  5.9429e-01,\n",
      "          9.7903e-01, -9.6332e-02,  4.1814e-01,  4.2213e-01, -1.3955e-01],\n",
      "        [ 5.3512e-01,  7.2916e-01, -1.8101e-01,  6.6332e-01, -4.6389e-01,\n",
      "          3.0013e-01, -5.6127e-02,  3.0249e-01,  3.0823e-01, -1.3205e-02],\n",
      "        [ 5.6927e-01, -2.9877e-01, -1.0755e-01,  1.3930e-02, -3.3587e-01,\n",
      "          5.8621e-01,  7.0345e-03,  4.9483e-01, -2.8349e-01,  1.6613e-01],\n",
      "        [-6.5812e-02,  2.3943e-01,  6.9834e-01,  1.4897e-01, -2.6245e-01,\n",
      "          2.5660e-01,  1.6445e-01, -6.8086e-01, -3.7563e-01,  7.6416e-01],\n",
      "        [ 3.8369e-01,  8.3430e-01, -4.9478e-01,  6.0002e-01,  2.2698e-01,\n",
      "         -1.8692e-02,  2.5742e-01,  5.0467e-01, -3.9162e-01,  1.3370e-01],\n",
      "        [-4.1498e-02, -9.2880e-01,  6.6562e-01,  4.3367e-02,  4.0807e-01,\n",
      "          2.2511e-01, -4.2822e-01, -7.5826e-01,  1.5248e-01,  3.1065e-01],\n",
      "        [ 6.9832e-02,  7.7515e-02, -3.9383e-01,  2.3177e-01, -6.0546e-02,\n",
      "         -2.6967e-01, -2.0927e-01,  4.4332e-02, -6.6995e-01,  5.5138e-01],\n",
      "        [-3.8279e-04, -8.1658e-01,  2.3228e-01,  5.8108e-01,  1.3556e-01,\n",
      "          9.9858e-02,  1.3974e-01, -6.3909e-01,  7.3003e-01,  3.8772e-01],\n",
      "        [-1.2552e-01,  9.5053e-01,  3.1067e-01,  1.2441e-01,  3.1324e-01,\n",
      "          8.7812e-01, -2.0799e-01, -2.3449e-01, -5.9586e-02,  1.5117e-01],\n",
      "        [ 5.1048e-02, -6.7507e-03,  1.0058e+00,  4.4590e-01,  1.8181e-01,\n",
      "          5.8694e-01, -4.6332e-01, -2.7633e-02,  3.5897e-01, -6.5668e-01],\n",
      "        [ 4.7018e-01, -1.2302e-01,  1.3290e-01,  6.4875e-01, -3.4769e-01,\n",
      "          8.6371e-01,  2.6937e-01, -8.7761e-02,  6.9805e-01, -2.7603e-01],\n",
      "        [-1.1569e-01, -2.3872e-01,  9.5267e-01,  1.2551e-01, -7.8661e-01,\n",
      "         -9.1681e-01,  4.3421e-01, -1.8202e-01,  4.7761e-01, -2.1325e-01],\n",
      "        [ 1.6121e-01, -2.0592e-01,  3.5393e-01,  9.0147e-02, -5.8018e-02,\n",
      "          6.9540e-01, -5.5026e-01,  3.5566e-01, -6.7544e-01,  4.1841e-01],\n",
      "        [-5.5985e-01, -3.4374e-01,  4.5494e-02, -3.9294e-01, -4.7434e-01,\n",
      "         -1.1786e-01, -7.0351e-01, -2.0304e-01,  1.1002e-01,  5.3609e-01],\n",
      "        [ 5.3264e-01, -3.1206e-01,  8.1986e-01,  2.6350e-02, -6.8779e-01,\n",
      "         -4.4878e-01,  7.4747e-01, -3.3410e-01, -9.8710e-03,  3.9546e-01],\n",
      "        [-3.9511e-02,  2.4562e-01, -1.9443e-01,  3.0725e-01,  4.5346e-01,\n",
      "          9.2222e-01, -9.1570e-02,  7.5726e-01, -1.8957e-01,  8.2463e-01],\n",
      "        [ 5.9354e-01, -3.0484e-01,  4.5528e-01,  3.0039e-01,  3.0551e-01,\n",
      "         -1.5680e-01, -1.7080e-01, -2.6123e-01,  5.6922e-01, -4.6052e-01],\n",
      "        [ 8.5354e-01,  8.8238e-02, -3.0384e-01,  2.3572e-01,  4.7109e-01,\n",
      "          9.1445e-01,  9.0044e-01,  2.9064e-01,  1.1300e+00,  6.3533e-01]])), ('bias', tensor([ 0.8554,  0.0758,  0.5391,  0.3438,  0.3579,  1.2682,  0.1505,  0.0119,\n",
      "         0.7097,  0.6913, -0.1243, -0.2760, -0.1258,  0.1083,  0.3708,  0.4665,\n",
      "         1.0597,  0.9293, -0.5960, -0.0583]))])\n"
     ]
    }
   ],
   "source": [
    "inner_optimizer = torch.optim.SGD(nm.parameters(),lr=0.05)\n",
    "inner_state = copy.deepcopy(nm.state_dict())\n",
    "print(inner_state)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for _ in range(10):\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = nm(inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizer.step()\n",
    "final_state = nm.state_dict()\n",
    "delta_theta = OrderedDict({k:inner_state[k]-final_state[k] for k in nm.state_dict().keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 8.6763e-02,  5.0674e-01,  5.6286e-01, -4.6617e-01,  7.3899e-03,\n",
      "         -1.5633e-01,  8.0342e-02,  6.8927e-01, -7.0312e-02,  8.0618e-01],\n",
      "        [ 8.0148e-01,  1.9437e-01, -1.3462e-01,  4.1985e-02,  3.9067e-01,\n",
      "          5.9569e-01,  2.2514e-02,  3.4177e-02, -1.7175e-01, -4.6753e-01],\n",
      "        [-5.8992e-01,  7.6580e-01, -1.0324e-01,  6.3775e-01,  5.9429e-01,\n",
      "          9.7903e-01, -9.6332e-02,  4.1814e-01,  4.2213e-01, -1.3955e-01],\n",
      "        [ 5.3512e-01,  7.2916e-01, -1.8101e-01,  6.6332e-01, -4.6389e-01,\n",
      "          3.0013e-01, -5.6127e-02,  3.0249e-01,  3.0823e-01, -1.3205e-02],\n",
      "        [ 5.6927e-01, -2.9877e-01, -1.0755e-01,  1.3930e-02, -3.3587e-01,\n",
      "          5.8621e-01,  7.0345e-03,  4.9483e-01, -2.8349e-01,  1.6613e-01],\n",
      "        [-6.5812e-02,  2.3943e-01,  6.9834e-01,  1.4897e-01, -2.6245e-01,\n",
      "          2.5660e-01,  1.6445e-01, -6.8086e-01, -3.7563e-01,  7.6416e-01],\n",
      "        [ 3.8369e-01,  8.3430e-01, -4.9478e-01,  6.0002e-01,  2.2698e-01,\n",
      "         -1.8692e-02,  2.5742e-01,  5.0467e-01, -3.9162e-01,  1.3370e-01],\n",
      "        [-4.1498e-02, -9.2880e-01,  6.6562e-01,  4.3367e-02,  4.0807e-01,\n",
      "          2.2511e-01, -4.2822e-01, -7.5826e-01,  1.5248e-01,  3.1065e-01],\n",
      "        [ 6.9832e-02,  7.7515e-02, -3.9383e-01,  2.3177e-01, -6.0546e-02,\n",
      "         -2.6967e-01, -2.0927e-01,  4.4332e-02, -6.6995e-01,  5.5138e-01],\n",
      "        [-3.8279e-04, -8.1658e-01,  2.3228e-01,  5.8108e-01,  1.3556e-01,\n",
      "          9.9858e-02,  1.3974e-01, -6.3909e-01,  7.3003e-01,  3.8772e-01],\n",
      "        [-1.2552e-01,  9.5053e-01,  3.1067e-01,  1.2441e-01,  3.1324e-01,\n",
      "          8.7812e-01, -2.0799e-01, -2.3449e-01, -5.9586e-02,  1.5117e-01],\n",
      "        [ 5.1048e-02, -6.7507e-03,  1.0058e+00,  4.4590e-01,  1.8181e-01,\n",
      "          5.8694e-01, -4.6332e-01, -2.7633e-02,  3.5897e-01, -6.5668e-01],\n",
      "        [ 4.7018e-01, -1.2302e-01,  1.3290e-01,  6.4875e-01, -3.4769e-01,\n",
      "          8.6371e-01,  2.6937e-01, -8.7761e-02,  6.9805e-01, -2.7603e-01],\n",
      "        [-1.1569e-01, -2.3872e-01,  9.5267e-01,  1.2551e-01, -7.8661e-01,\n",
      "         -9.1681e-01,  4.3421e-01, -1.8202e-01,  4.7761e-01, -2.1325e-01],\n",
      "        [ 1.6121e-01, -2.0592e-01,  3.5393e-01,  9.0147e-02, -5.8018e-02,\n",
      "          6.9540e-01, -5.5026e-01,  3.5566e-01, -6.7544e-01,  4.1841e-01],\n",
      "        [-5.5985e-01, -3.4374e-01,  4.5494e-02, -3.9294e-01, -4.7434e-01,\n",
      "         -1.1786e-01, -7.0351e-01, -2.0304e-01,  1.1002e-01,  5.3609e-01],\n",
      "        [ 5.3264e-01, -3.1206e-01,  8.1986e-01,  2.6350e-02, -6.8779e-01,\n",
      "         -4.4878e-01,  7.4747e-01, -3.3410e-01, -9.8710e-03,  3.9546e-01],\n",
      "        [-3.9511e-02,  2.4562e-01, -1.9443e-01,  3.0725e-01,  4.5346e-01,\n",
      "          9.2222e-01, -9.1570e-02,  7.5726e-01, -1.8957e-01,  8.2463e-01],\n",
      "        [ 5.9354e-01, -3.0484e-01,  4.5528e-01,  3.0039e-01,  3.0551e-01,\n",
      "         -1.5680e-01, -1.7080e-01, -2.6123e-01,  5.6922e-01, -4.6052e-01],\n",
      "        [ 8.5354e-01,  8.8238e-02, -3.0384e-01,  2.3572e-01,  4.7109e-01,\n",
      "          9.1445e-01,  9.0044e-01,  2.9064e-01,  1.1300e+00,  6.3533e-01]])), ('bias', tensor([ 0.8554,  0.0758,  0.5391,  0.3438,  0.3579,  1.2682,  0.1505,  0.0119,\n",
      "         0.7097,  0.6913, -0.1243, -0.2760, -0.1258,  0.1083,  0.3708,  0.4665,\n",
      "         1.0597,  0.9293, -0.5960, -0.0583]))])\n"
     ]
    }
   ],
   "source": [
    "print(inner_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算JVP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "attn_optimizer = torch.optim.SGD(\n",
    "    [\n",
    "        {'params' : [p for p in attn_model.parameters()]} ,\n",
    "        {'params' : [p for p in emb_layer.parameters()]}\n",
    "    ], lr=lr, momentum=0.9)\n",
    "\n",
    "param_list = list(attn_model.parameters())\n",
    "param_list.extend(emb_layer.parameters())\n",
    "attn_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = [\n",
    "            torch.autograd.grad([p for p in model.parameters()], param_list , grad_outputs=list(delta_theta.values()))\\\n",
    "            for model, delta_theta in zip(models,delta_thetas)\n",
    "        ]      \n",
    "for grad in grads:\n",
    "    for param, g in zip(param_list, grad):\n",
    "        param.grad += grad \n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(param_list, 50)\n",
    "attn_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_states = model[0]\n",
    "final_states = .state_dict()\n",
    "\n",
    "# calculating delta theta\n",
    "delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
    "\n",
    "# calculating phi gradient\n",
    "hnet_grads = torch.autograd.grad(\n",
    "    list(weights.values()), hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
    ")\n",
    "\n",
    "print(torch.cat(g_1,dim=0).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0].state_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = nn.Linear(10,20)\n",
    "mo.load_state_dict(new_ms[0].state_dict())\n",
    "print(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mo.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "res = copy.deepcopy(model1)\n",
    "for param in res.parameters():\n",
    "    param.data.zero_()\n",
    "\n",
    "for rp, p in zip(res.parameters(), model1.parameters()):\n",
    "    rp.data += p.data.clone()\n",
    "    \n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "293539041be06dcdbc1dd82d46367ecf96c14410c0014bf8043197a4a2571a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
