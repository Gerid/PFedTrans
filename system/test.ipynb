{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_34524/1273866419.py, line 332)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\MSI\\AppData\\Local\\Temp/ipykernel_34524/1273866419.py\"\u001b[1;36m, line \u001b[1;32m332\u001b[0m\n\u001b[1;33m    args = parser.parse_args(args=[\"-data\",\"mnist\", \"-m\", \"cnn\", -algo FedTrans -gr 2500 -did 0 -go cnn -nc 2\")\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import logging\n",
    "\n",
    "from flcore.servers.serveravg import FedAvg\n",
    "from flcore.servers.serverpFedMe import pFedMe\n",
    "from flcore.servers.serverperavg import PerAvg\n",
    "from flcore.servers.serverprox import FedProx\n",
    "from flcore.servers.serverfomo import FedFomo\n",
    "from flcore.servers.serveramp import FedAMP\n",
    "from flcore.servers.servermtl import FedMTL\n",
    "from flcore.servers.serverlocal import Local\n",
    "from flcore.servers.serverper import FedPer\n",
    "from flcore.servers.serverapfl import APFL\n",
    "from flcore.servers.serverditto import Ditto\n",
    "from flcore.servers.serverrep import FedRep\n",
    "from flcore.servers.serverphp import FedPHP\n",
    "from flcore.servers.serverbn import FedBN\n",
    "from flcore.servers.serverrod import FedROD\n",
    "from flcore.servers.serverproto import FedProto\n",
    "from flcore.servers.serverdyn import FedDyn\n",
    "from flcore.servers.servermoon import MOON\n",
    "from flcore.servers.serverbabu import FedBABU\n",
    "from flcore.servers.serverapple import APPLE\n",
    "from flcore.servers.serverfedtrans import FedTrans \n",
    "\n",
    "from flcore.trainmodel.models import *\n",
    "\n",
    "from flcore.trainmodel.bilstm import BiLSTM_TextClassification\n",
    "# from flcore.trainmodel.resnet import resnet18 as resnet\n",
    "from flcore.trainmodel.alexnet import alexnet\n",
    "from flcore.trainmodel.mobilenet_v2 import mobilenet_v2\n",
    "from utils.result_utils import average_data\n",
    "from utils.mem_utils import MemReporter\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# hyper-params for Text tasks\n",
    "vocab_size = 98635\n",
    "max_len=200\n",
    "hidden_dim=32\n",
    "\n",
    "def run(args):\n",
    "\n",
    "    time_list = []\n",
    "    reporter = MemReporter()\n",
    "    model_str = args.model\n",
    "\n",
    "    for i in range(args.prev, args.times):\n",
    "        print(f\"\\n============= Running time: {i}th =============\")\n",
    "        print(\"Creating server and clients ...\")\n",
    "        start = time.time()\n",
    "\n",
    "        # Generate args.model\n",
    "        if model_str == \"mlr\":\n",
    "            if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = Mclr_Logistic(1*28*28, num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "                args.model = Mclr_Logistic(3*32*32, num_classes=args.num_classes).to(args.device)\n",
    "            else:\n",
    "                args.model = Mclr_Logistic(60, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"cnn\":\n",
    "            if args.dataset[:5] == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = FedAvgCNN(in_features=1, num_classes=args.num_classes, dim=1024).to(args.device)\n",
    "            elif args.dataset == \"omniglot\":\n",
    "                args.model = FedAvgCNN(in_features=1, num_classes=args.num_classes, dim=33856).to(args.device)\n",
    "            elif args.dataset[:5] == \"Cifar\":\n",
    "                args.model = FedAvgCNN(in_features=3, num_classes=args.num_classes, dim=1600).to(args.device)\n",
    "                # args.model = CifarNet(num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Digit5\":\n",
    "                args.model = Digit5CNN().to(args.device)\n",
    "            else:\n",
    "                args.model = FedAvgCNN(in_features=3, num_classes=args.num_classes, dim=10816).to(args.device)\n",
    "\n",
    "        elif model_str == \"dnn\": # non-convex\n",
    "            if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "                args.model = DNN(1*28*28, 100, num_classes=args.num_classes).to(args.device)\n",
    "            elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "                args.model = DNN(3*32*32, 100, num_classes=args.num_classes).to(args.device)\n",
    "            else:\n",
    "                args.model = DNN(60, 20, num_classes=args.num_classes).to(args.device)\n",
    "        \n",
    "        elif model_str == \"resnet\":\n",
    "            args.model = torchvision.models.resnet18(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = torchvision.models.resnet18(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = resnet18(num_classes=args.num_classes, has_bn=True, bn_block_num=4).to(args.device)\n",
    "\n",
    "        elif model_str == \"alexnet\":\n",
    "            args.model = alexnet(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = alexnet(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "        elif model_str == \"googlenet\":\n",
    "            args.model = torchvision.models.googlenet(pretrained=False, aux_logits=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = torchvision.models.googlenet(pretrained=True, aux_logits=False).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"mobilenet_v2\":\n",
    "            args.model = mobilenet_v2(pretrained=False, num_classes=args.num_classes).to(args.device)\n",
    "            \n",
    "            # args.model = mobilenet_v2(pretrained=True).to(args.device)\n",
    "            # feature_dim = list(args.model.fc.parameters())[0].shape[1]\n",
    "            # args.model.fc = nn.Linear(feature_dim, args.num_classes).to(args.device)\n",
    "            \n",
    "        elif model_str == \"lstm\":\n",
    "            args.model = LSTMNet(hidden_dim=hidden_dim, vocab_size=vocab_size, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"bilstm\":\n",
    "            args.model = BiLSTM_TextClassification(input_size=vocab_size, hidden_size=hidden_dim, output_size=args.num_classes, \n",
    "                        num_layers=1, embedding_dropout=0, lstm_dropout=0, attention_dropout=0, \n",
    "                        embedding_length=hidden_dim).to(args.device)\n",
    "\n",
    "        elif model_str == \"fastText\":\n",
    "            args.model = fastText(hidden_dim=hidden_dim, vocab_size=vocab_size, num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"TextCNN\":\n",
    "            args.model = TextCNN(hidden_dim=hidden_dim, max_len=max_len, vocab_size=vocab_size, \n",
    "                            num_classes=args.num_classes).to(args.device)\n",
    "\n",
    "        elif model_str == \"Transformer\":\n",
    "            args.model = TransformerModel(ntoken=vocab_size, d_model=hidden_dim, nhead=2, d_hid=hidden_dim, nlayers=2, \n",
    "                            num_classes=args.num_classes).to(args.device)\n",
    "        \n",
    "        elif model_str == \"AmazonMLP\":\n",
    "            args.model = AmazonMLP().to(args.device)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        print(args.model)\n",
    "\n",
    "        # select algorithm\n",
    "        if args.algorithm == \"FedAvg\":\n",
    "            server = FedAvg(args, i)\n",
    "\n",
    "        elif args.algorithm == \"Local\":\n",
    "            server = Local(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedMTL\":\n",
    "            server = FedMTL(args, i)\n",
    "\n",
    "        elif args.algorithm == \"PerAvg\":\n",
    "            server = PerAvg(args, i)\n",
    "\n",
    "        elif args.algorithm == \"pFedMe\":\n",
    "            server = pFedMe(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedProx\":\n",
    "            server = FedProx(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedFomo\":\n",
    "            server = FedFomo(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedAMP\":\n",
    "            server = FedAMP(args, i)\n",
    "\n",
    "        elif args.algorithm == \"APFL\":\n",
    "            server = APFL(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedPer\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedPer(args, i)\n",
    "\n",
    "        elif args.algorithm == \"Ditto\":\n",
    "            server = Ditto(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedRep\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedRep(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedPHP\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedPHP(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedBN\":\n",
    "            server = FedBN(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedROD\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedROD(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedProto\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedProto(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedDyn\":\n",
    "            server = FedDyn(args, i)\n",
    "\n",
    "        elif args.algorithm == \"MOON\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = MOON(args, i)\n",
    "\n",
    "        elif args.algorithm == \"FedBABU\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedBABU(args, i)\n",
    "\n",
    "        elif args.algorithm == \"APPLE\":\n",
    "            server = APPLE(args, i)\n",
    "            \n",
    "        elif args.algorithm == \"FedTrans\":\n",
    "            args.head = copy.deepcopy(args.model.fc)\n",
    "            args.model.fc = nn.Identity()\n",
    "            args.model = LocalModel(args.model, args.head)\n",
    "            server = FedTrans(args, i)\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    return server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_start = time.time()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # general\n",
    "    parser.add_argument('-go', \"--goal\", type=str, default=\"test\", \n",
    "                        help=\"The goal for this experiment\")\n",
    "    parser.add_argument('-dev', \"--device\", type=str, default=\"cuda\",\n",
    "                        choices=[\"cpu\", \"cuda\"])\n",
    "    parser.add_argument('-did', \"--device_id\", type=str, default=\"0\")\n",
    "    parser.add_argument('-data', \"--dataset\", type=str, default=\"mnist\")\n",
    "    parser.add_argument('-nb', \"--num_classes\", type=int, default=10)\n",
    "    parser.add_argument('-m', \"--model\", type=str, default=\"cnn\")\n",
    "    parser.add_argument('-p', \"--head\", type=str, default=\"cnn\")\n",
    "    parser.add_argument('-lbs', \"--batch_size\", type=int, default=10)\n",
    "    parser.add_argument('-lr', \"--local_learning_rate\", type=float, default=0.005,\n",
    "                        help=\"Local learning rate\")\n",
    "    parser.add_argument('-gr', \"--global_rounds\", type=int, default=1000)\n",
    "    parser.add_argument('-ls', \"--local_steps\", type=int, default=1)\n",
    "    parser.add_argument('-algo', \"--algorithm\", type=str, default=\"FedAvg\")\n",
    "    parser.add_argument('-jr', \"--join_ratio\", type=float, default=1.0,\n",
    "                        help=\"Ratio of clients per round\")\n",
    "    parser.add_argument('-rjr', \"--random_join_ratio\", type=bool, default=False,\n",
    "                        help=\"Random ratio of clients per round\")\n",
    "    parser.add_argument('-nc', \"--num_clients\", type=int, default=2,\n",
    "                        help=\"Total number of clients\")\n",
    "    parser.add_argument('-pv', \"--prev\", type=int, default=0,\n",
    "                        help=\"Previous Running times\")\n",
    "    parser.add_argument('-t', \"--times\", type=int, default=1,\n",
    "                        help=\"Running times\")\n",
    "    parser.add_argument('-eg', \"--eval_gap\", type=int, default=1,\n",
    "                        help=\"Rounds gap for evaluation\")\n",
    "    parser.add_argument('-dp', \"--privacy\", type=bool, default=False,\n",
    "                        help=\"differential privacy\")\n",
    "    parser.add_argument('-dps', \"--dp_sigma\", type=float, default=0.0)\n",
    "    parser.add_argument('-sfn', \"--save_folder_name\", type=str, default='models')\n",
    "    # practical\n",
    "    parser.add_argument('-cdr', \"--client_drop_rate\", type=float, default=0.0,\n",
    "                        help=\"Rate for clients that train but drop out\")\n",
    "    parser.add_argument('-tsr', \"--train_slow_rate\", type=float, default=0.0,\n",
    "                        help=\"The rate for slow clients when training locally\")\n",
    "    parser.add_argument('-ssr', \"--send_slow_rate\", type=float, default=0.0,\n",
    "                        help=\"The rate for slow clients when sending global model\")\n",
    "    parser.add_argument('-ts', \"--time_select\", type=bool, default=False,\n",
    "                        help=\"Whether to group and select clients at each round according to time cost\")\n",
    "    parser.add_argument('-tth', \"--time_threthold\", type=float, default=10000,\n",
    "                        help=\"The threthold for droping slow clients\")\n",
    "    # pFedMe / PerAvg / FedProx / FedAMP / FedPHP\n",
    "    parser.add_argument('-bt', \"--beta\", type=float, default=0.0,\n",
    "                        help=\"Average moving parameter for pFedMe, Second learning rate of Per-FedAvg, \\\n",
    "                        or L1 regularization weight of FedTransfer\")\n",
    "    parser.add_argument('-lam', \"--lamda\", type=float, default=1.0,\n",
    "                        help=\"Regularization weight for pFedMe and FedAMP\")\n",
    "    parser.add_argument('-mu', \"--mu\", type=float, default=0,\n",
    "                        help=\"Proximal rate for FedProx\")\n",
    "    parser.add_argument('-K', \"--K\", type=int, default=5,\n",
    "                        help=\"Number of personalized training steps for pFedMe\")\n",
    "    parser.add_argument('-lrp', \"--p_learning_rate\", type=float, default=0.01,\n",
    "                        help=\"personalized learning rate to caculate theta aproximately using K steps\")\n",
    "    # FedFomo\n",
    "    parser.add_argument('-M', \"--M\", type=int, default=5,\n",
    "                        help=\"Server only sends M client models to one client at each round\")\n",
    "    # FedMTL\n",
    "    parser.add_argument('-itk', \"--itk\", type=int, default=4000,\n",
    "                        help=\"The iterations for solving quadratic subproblems\")\n",
    "    # FedAMP\n",
    "    parser.add_argument('-alk', \"--alphaK\", type=float, default=1.0, \n",
    "                        help=\"lambda/sqrt(GLOABL-ITRATION) according to the paper\")\n",
    "    parser.add_argument('-sg', \"--sigma\", type=float, default=1.0)\n",
    "    # APFL\n",
    "    parser.add_argument('-al', \"--alpha\", type=float, default=1.0)\n",
    "    # Ditto / FedRep\n",
    "    parser.add_argument('-pls', \"--plocal_steps\", type=int, default=1)\n",
    "    # MOON\n",
    "    parser.add_argument('-ta', \"--tau\", type=float, default=1.0)\n",
    "    # FedBABU\n",
    "    parser.add_argument('-fts', \"--fine_tuning_steps\", type=int, default=1)\n",
    "    # APPLE\n",
    "    parser.add_argument('-dlr', \"--dr_learning_rate\", type=float, default=0.0)\n",
    "    parser.add_argument('-L', \"--L\", type=float, default=1.0)\n",
    "    #FedTrans\n",
    "    parser.add_argument('-ere', \"--every_recluster_eps\", type=int, default=5)\n",
    "    parser.add_argument('-ed', \"--emb_dim\", type=int, default=128)\n",
    "    parser.add_argument('-alr', \"--attn_learning_rate\", type=float, default=0.005)\n",
    "    parser.add_argument('-ncl', \"--num_cluster\", type=int, default=10)\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(args=[\"-data\",\"mnist\", \"-m\", \"cnn\", -algo FedTrans -gr 2500 -did 0 -go cnn -nc 2\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device_id\n",
    "\n",
    "    if args.device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"\\ncuda is not avaiable.\\n\")\n",
    "        args.device = \"cpu\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"Algorithm: {}\".format(args.algorithm))\n",
    "    print(\"Local batch size: {}\".format(args.batch_size))\n",
    "    print(\"Local steps: {}\".format(args.local_steps))\n",
    "    print(\"Local learing rate: {}\".format(args.local_learning_rate))\n",
    "    print(\"Total number of clients: {}\".format(args.num_clients))\n",
    "    print(\"Clients join in each round: {}\".format(args.join_ratio))\n",
    "    print(\"Client drop rate: {}\".format(args.client_drop_rate))\n",
    "    print(\"Time select: {}\".format(args.time_select))\n",
    "    print(\"Time threthold: {}\".format(args.time_threthold))\n",
    "    print(\"Global rounds: {}\".format(args.global_rounds))\n",
    "    print(\"Running times: {}\".format(args.times))\n",
    "    print(\"Dataset: {}\".format(args.dataset))\n",
    "    print(\"Local model: {}\".format(args.model))\n",
    "    print(\"Using device: {}\".format(args.device))\n",
    "\n",
    "    if args.device == \"cuda\":\n",
    "        print(\"Cuda device id: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    # if args.dataset == \"mnist\" or args.dataset == \"fmnist\":\n",
    "    #     generate_mnist('../dataset/mnist/', args.num_clients, 10, args.niid)\n",
    "    # elif args.dataset == \"Cifar10\" or args.dataset == \"Cifar100\":\n",
    "    #     generate_cifar10('../dataset/Cifar10/', args.num_clients, 10, args.niid)\n",
    "    # else:\n",
    "    #     generate_synthetic('../dataset/synthetic/', args.num_clients, 10, args.niid)\n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA],\n",
    "    #     profile_memory=True, \n",
    "    #     on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    #     ) as prof:\n",
    "    # with torch.autograd.profiler.profile(profile_memory=True) as prof:\n",
    "    server = run(args)\n",
    "\n",
    "    \n",
    "    # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
    "    # print(f\"\\nTotal time cost: {round(time.time()-total_start, 2)}s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.0203,  0.0265,  0.0265,  0.0092,  0.0197,  0.0188,  0.0240, -0.0010,\n",
      "         -0.0064, -0.0164,  0.0229,  0.0131,  0.0095, -0.0219, -0.0100, -0.0188,\n",
      "          0.0344,  0.0322,  0.0388, -0.0056,  0.0178,  0.0253,  0.0166,  0.0336,\n",
      "          0.0009,  0.0038,  0.0136, -0.0013,  0.0142,  0.0146, -0.0167,  0.0278,\n",
      "         -0.0181, -0.0092,  0.0094,  0.0225,  0.0055, -0.0187,  0.0293,  0.0201,\n",
      "         -0.0229, -0.0218, -0.0211,  0.0274, -0.0080,  0.0148, -0.0203, -0.0275,\n",
      "         -0.0081,  0.0107,  0.0158,  0.0093, -0.0247, -0.0167,  0.0544, -0.0046,\n",
      "          0.0095, -0.0067, -0.0074,  0.0156,  0.0085,  0.0234, -0.0052,  0.0204,\n",
      "         -0.0222,  0.0008,  0.0158, -0.0140,  0.0019,  0.0103, -0.0138, -0.0159,\n",
      "         -0.0168,  0.0061, -0.0049, -0.0009,  0.0071, -0.0128,  0.0130, -0.0022,\n",
      "          0.0123, -0.0407,  0.0122,  0.0169, -0.0096, -0.0321,  0.0036,  0.0004,\n",
      "          0.0094,  0.0029,  0.0042,  0.0099, -0.0069,  0.0014, -0.0162, -0.0421,\n",
      "          0.0053, -0.0012, -0.0038, -0.0133,  0.0057,  0.0104, -0.0273, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0128,  0.0076,  0.0035,  0.0108, -0.0032, -0.0064,\n",
      "         -0.0117,  0.0003,  0.0038,  0.0132,  0.0046, -0.0169,  0.0249,  0.0177,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0297, -0.0004, -0.0013,  0.0098, -0.0093]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0265,  0.0265,  0.0091,  0.0198,  0.0188,  0.0240, -0.0010,\n",
      "         -0.0064, -0.0164,  0.0230,  0.0131,  0.0096, -0.0219, -0.0100, -0.0188,\n",
      "          0.0344,  0.0322,  0.0388, -0.0056,  0.0178,  0.0254,  0.0166,  0.0336,\n",
      "          0.0009,  0.0038,  0.0136, -0.0012,  0.0142,  0.0146, -0.0167,  0.0278,\n",
      "         -0.0181, -0.0092,  0.0094,  0.0225,  0.0056, -0.0188,  0.0293,  0.0201,\n",
      "         -0.0228, -0.0218, -0.0212,  0.0275, -0.0080,  0.0149, -0.0202, -0.0275,\n",
      "         -0.0081,  0.0108,  0.0158,  0.0094, -0.0247, -0.0167,  0.0544, -0.0047,\n",
      "          0.0095, -0.0066, -0.0073,  0.0158,  0.0084,  0.0234, -0.0052,  0.0203,\n",
      "         -0.0221,  0.0008,  0.0158, -0.0140,  0.0020,  0.0103, -0.0137, -0.0159,\n",
      "         -0.0169,  0.0060, -0.0050, -0.0009,  0.0071, -0.0127,  0.0131, -0.0022,\n",
      "          0.0124, -0.0407,  0.0123,  0.0170, -0.0096, -0.0321,  0.0035,  0.0004,\n",
      "          0.0094,  0.0029,  0.0043,  0.0099, -0.0068,  0.0013, -0.0162, -0.0421,\n",
      "          0.0053, -0.0011, -0.0038, -0.0134,  0.0056,  0.0105, -0.0273, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0129,  0.0077,  0.0034,  0.0108, -0.0032, -0.0065,\n",
      "         -0.0119,  0.0004,  0.0038,  0.0132,  0.0045, -0.0169,  0.0248,  0.0178,\n",
      "         -0.0003, -0.0218, -0.0234,  0.0297, -0.0004, -0.0013,  0.0097, -0.0093]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0264,  0.0265,  0.0091,  0.0198,  0.0189,  0.0240, -0.0010,\n",
      "         -0.0064, -0.0164,  0.0229,  0.0131,  0.0095, -0.0220, -0.0100, -0.0187,\n",
      "          0.0344,  0.0323,  0.0388, -0.0055,  0.0177,  0.0253,  0.0165,  0.0336,\n",
      "          0.0009,  0.0038,  0.0136, -0.0013,  0.0142,  0.0146, -0.0167,  0.0278,\n",
      "         -0.0182, -0.0092,  0.0095,  0.0225,  0.0055, -0.0188,  0.0293,  0.0202,\n",
      "         -0.0230, -0.0218, -0.0212,  0.0275, -0.0080,  0.0148, -0.0202, -0.0275,\n",
      "         -0.0082,  0.0108,  0.0158,  0.0094, -0.0247, -0.0167,  0.0544, -0.0046,\n",
      "          0.0094, -0.0066, -0.0074,  0.0157,  0.0085,  0.0235, -0.0052,  0.0203,\n",
      "         -0.0221,  0.0007,  0.0158, -0.0141,  0.0019,  0.0103, -0.0138, -0.0159,\n",
      "         -0.0169,  0.0060, -0.0049, -0.0009,  0.0071, -0.0127,  0.0130, -0.0021,\n",
      "          0.0124, -0.0407,  0.0123,  0.0170, -0.0096, -0.0321,  0.0035,  0.0004,\n",
      "          0.0094,  0.0029,  0.0043,  0.0100, -0.0069,  0.0014, -0.0162, -0.0420,\n",
      "          0.0053, -0.0011, -0.0037, -0.0133,  0.0056,  0.0104, -0.0273, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0129,  0.0076,  0.0035,  0.0107, -0.0033, -0.0064,\n",
      "         -0.0117,  0.0003,  0.0039,  0.0132,  0.0046, -0.0169,  0.0249,  0.0177,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0297, -0.0004, -0.0012,  0.0098, -0.0094]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0265,  0.0265,  0.0091,  0.0198,  0.0188,  0.0240, -0.0010,\n",
      "         -0.0064, -0.0164,  0.0229,  0.0131,  0.0095, -0.0220, -0.0100, -0.0188,\n",
      "          0.0344,  0.0323,  0.0388, -0.0055,  0.0177,  0.0253,  0.0166,  0.0336,\n",
      "          0.0009,  0.0038,  0.0136, -0.0013,  0.0142,  0.0146, -0.0167,  0.0278,\n",
      "         -0.0181, -0.0092,  0.0094,  0.0225,  0.0055, -0.0187,  0.0292,  0.0201,\n",
      "         -0.0229, -0.0218, -0.0211,  0.0275, -0.0079,  0.0148, -0.0202, -0.0275,\n",
      "         -0.0081,  0.0107,  0.0158,  0.0094, -0.0247, -0.0167,  0.0544, -0.0047,\n",
      "          0.0095, -0.0066, -0.0073,  0.0157,  0.0085,  0.0234, -0.0052,  0.0203,\n",
      "         -0.0221,  0.0008,  0.0158, -0.0140,  0.0019,  0.0103, -0.0138, -0.0159,\n",
      "         -0.0168,  0.0060, -0.0050, -0.0009,  0.0071, -0.0127,  0.0130, -0.0022,\n",
      "          0.0123, -0.0407,  0.0123,  0.0169, -0.0096, -0.0321,  0.0036,  0.0004,\n",
      "          0.0094,  0.0030,  0.0042,  0.0099, -0.0069,  0.0014, -0.0162, -0.0421,\n",
      "          0.0053, -0.0011, -0.0038, -0.0133,  0.0056,  0.0104, -0.0272, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0129,  0.0076,  0.0034,  0.0108, -0.0032, -0.0064,\n",
      "         -0.0118,  0.0003,  0.0039,  0.0132,  0.0046, -0.0169,  0.0249,  0.0178,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0296, -0.0005, -0.0013,  0.0098, -0.0094]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0265,  0.0265,  0.0092,  0.0197,  0.0188,  0.0240, -0.0010,\n",
      "         -0.0064, -0.0164,  0.0229,  0.0131,  0.0095, -0.0219, -0.0100, -0.0188,\n",
      "          0.0344,  0.0322,  0.0388, -0.0056,  0.0178,  0.0253,  0.0165,  0.0336,\n",
      "          0.0009,  0.0038,  0.0136, -0.0012,  0.0142,  0.0145, -0.0167,  0.0278,\n",
      "         -0.0181, -0.0092,  0.0094,  0.0225,  0.0055, -0.0187,  0.0292,  0.0201,\n",
      "         -0.0229, -0.0218, -0.0212,  0.0275, -0.0080,  0.0148, -0.0203, -0.0275,\n",
      "         -0.0081,  0.0108,  0.0158,  0.0093, -0.0247, -0.0167,  0.0544, -0.0046,\n",
      "          0.0095, -0.0066, -0.0074,  0.0157,  0.0085,  0.0234, -0.0052,  0.0203,\n",
      "         -0.0221,  0.0008,  0.0158, -0.0140,  0.0019,  0.0103, -0.0138, -0.0159,\n",
      "         -0.0168,  0.0061, -0.0049, -0.0009,  0.0071, -0.0128,  0.0130, -0.0022,\n",
      "          0.0123, -0.0407,  0.0123,  0.0170, -0.0096, -0.0321,  0.0036,  0.0004,\n",
      "          0.0095,  0.0029,  0.0042,  0.0100, -0.0069,  0.0014, -0.0162, -0.0421,\n",
      "          0.0053, -0.0011, -0.0038, -0.0133,  0.0056,  0.0104, -0.0272, -0.0179,\n",
      "         -0.0036, -0.0044, -0.0129,  0.0077,  0.0035,  0.0108, -0.0033, -0.0064,\n",
      "         -0.0118,  0.0003,  0.0039,  0.0132,  0.0046, -0.0169,  0.0249,  0.0178,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0296, -0.0005, -0.0013,  0.0098, -0.0093]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0265,  0.0265,  0.0091,  0.0198,  0.0188,  0.0240, -0.0010,\n",
      "         -0.0064, -0.0164,  0.0229,  0.0131,  0.0095, -0.0220, -0.0100, -0.0188,\n",
      "          0.0344,  0.0323,  0.0388, -0.0055,  0.0177,  0.0253,  0.0166,  0.0336,\n",
      "          0.0009,  0.0038,  0.0136, -0.0013,  0.0142,  0.0146, -0.0167,  0.0278,\n",
      "         -0.0181, -0.0092,  0.0094,  0.0225,  0.0055, -0.0187,  0.0292,  0.0201,\n",
      "         -0.0229, -0.0218, -0.0211,  0.0275, -0.0079,  0.0148, -0.0202, -0.0275,\n",
      "         -0.0081,  0.0107,  0.0158,  0.0094, -0.0247, -0.0167,  0.0544, -0.0047,\n",
      "          0.0095, -0.0066, -0.0073,  0.0157,  0.0085,  0.0234, -0.0052,  0.0203,\n",
      "         -0.0221,  0.0008,  0.0158, -0.0140,  0.0019,  0.0103, -0.0138, -0.0159,\n",
      "         -0.0168,  0.0060, -0.0050, -0.0009,  0.0071, -0.0127,  0.0130, -0.0022,\n",
      "          0.0123, -0.0407,  0.0123,  0.0169, -0.0096, -0.0321,  0.0036,  0.0004,\n",
      "          0.0094,  0.0030,  0.0042,  0.0099, -0.0069,  0.0014, -0.0162, -0.0421,\n",
      "          0.0053, -0.0011, -0.0038, -0.0133,  0.0056,  0.0104, -0.0272, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0129,  0.0076,  0.0034,  0.0108, -0.0032, -0.0064,\n",
      "         -0.0118,  0.0003,  0.0039,  0.0132,  0.0046, -0.0169,  0.0249,  0.0178,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0296, -0.0005, -0.0013,  0.0098, -0.0094]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0264,  0.0265,  0.0092,  0.0198,  0.0188,  0.0240, -0.0010,\n",
      "         -0.0063, -0.0164,  0.0229,  0.0131,  0.0095, -0.0219, -0.0100, -0.0188,\n",
      "          0.0344,  0.0323,  0.0388, -0.0056,  0.0177,  0.0253,  0.0166,  0.0336,\n",
      "          0.0009,  0.0037,  0.0136, -0.0012,  0.0143,  0.0145, -0.0168,  0.0278,\n",
      "         -0.0181, -0.0092,  0.0094,  0.0224,  0.0055, -0.0187,  0.0293,  0.0201,\n",
      "         -0.0230, -0.0218, -0.0211,  0.0275, -0.0080,  0.0148, -0.0203, -0.0275,\n",
      "         -0.0082,  0.0108,  0.0158,  0.0094, -0.0247, -0.0167,  0.0544, -0.0047,\n",
      "          0.0094, -0.0066, -0.0074,  0.0157,  0.0085,  0.0235, -0.0051,  0.0203,\n",
      "         -0.0221,  0.0008,  0.0157, -0.0140,  0.0020,  0.0103, -0.0138, -0.0158,\n",
      "         -0.0169,  0.0060, -0.0049, -0.0009,  0.0071, -0.0127,  0.0131, -0.0022,\n",
      "          0.0124, -0.0407,  0.0122,  0.0170, -0.0096, -0.0321,  0.0035,  0.0004,\n",
      "          0.0094,  0.0029,  0.0042,  0.0099, -0.0069,  0.0014, -0.0162, -0.0421,\n",
      "          0.0053, -0.0011, -0.0038, -0.0133,  0.0056,  0.0105, -0.0272, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0128,  0.0076,  0.0035,  0.0108, -0.0032, -0.0064,\n",
      "         -0.0117,  0.0003,  0.0039,  0.0133,  0.0046, -0.0169,  0.0249,  0.0178,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0297, -0.0004, -0.0013,  0.0098, -0.0093]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0264,  0.0265,  0.0091,  0.0198,  0.0189,  0.0240, -0.0010,\n",
      "         -0.0064, -0.0164,  0.0229,  0.0131,  0.0095, -0.0220, -0.0100, -0.0187,\n",
      "          0.0344,  0.0323,  0.0388, -0.0055,  0.0177,  0.0253,  0.0165,  0.0336,\n",
      "          0.0009,  0.0038,  0.0136, -0.0013,  0.0142,  0.0146, -0.0167,  0.0278,\n",
      "         -0.0182, -0.0092,  0.0095,  0.0225,  0.0055, -0.0188,  0.0293,  0.0202,\n",
      "         -0.0230, -0.0218, -0.0212,  0.0275, -0.0080,  0.0148, -0.0202, -0.0275,\n",
      "         -0.0082,  0.0108,  0.0158,  0.0094, -0.0247, -0.0167,  0.0544, -0.0046,\n",
      "          0.0094, -0.0066, -0.0074,  0.0157,  0.0085,  0.0235, -0.0052,  0.0203,\n",
      "         -0.0221,  0.0007,  0.0158, -0.0141,  0.0019,  0.0103, -0.0138, -0.0159,\n",
      "         -0.0169,  0.0060, -0.0049, -0.0009,  0.0071, -0.0127,  0.0130, -0.0021,\n",
      "          0.0124, -0.0407,  0.0123,  0.0170, -0.0096, -0.0321,  0.0035,  0.0004,\n",
      "          0.0094,  0.0029,  0.0043,  0.0100, -0.0069,  0.0014, -0.0162, -0.0420,\n",
      "          0.0053, -0.0011, -0.0037, -0.0133,  0.0056,  0.0104, -0.0273, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0129,  0.0076,  0.0035,  0.0107, -0.0033, -0.0064,\n",
      "         -0.0117,  0.0003,  0.0039,  0.0132,  0.0046, -0.0169,  0.0249,  0.0177,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0297, -0.0004, -0.0012,  0.0098, -0.0094]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0265,  0.0265,  0.0092,  0.0198,  0.0188,  0.0240, -0.0010,\n",
      "         -0.0063, -0.0164,  0.0229,  0.0131,  0.0095, -0.0219, -0.0100, -0.0188,\n",
      "          0.0344,  0.0322,  0.0388, -0.0056,  0.0178,  0.0253,  0.0165,  0.0336,\n",
      "          0.0009,  0.0037,  0.0136, -0.0013,  0.0142,  0.0145, -0.0167,  0.0277,\n",
      "         -0.0181, -0.0092,  0.0094,  0.0225,  0.0055, -0.0187,  0.0293,  0.0202,\n",
      "         -0.0229, -0.0218, -0.0211,  0.0275, -0.0079,  0.0148, -0.0202, -0.0275,\n",
      "         -0.0081,  0.0108,  0.0158,  0.0093, -0.0247, -0.0167,  0.0544, -0.0046,\n",
      "          0.0094, -0.0066, -0.0074,  0.0157,  0.0086,  0.0234, -0.0052,  0.0203,\n",
      "         -0.0221,  0.0008,  0.0158, -0.0140,  0.0020,  0.0103, -0.0138, -0.0159,\n",
      "         -0.0169,  0.0060, -0.0049, -0.0009,  0.0071, -0.0127,  0.0130, -0.0022,\n",
      "          0.0123, -0.0407,  0.0122,  0.0169, -0.0096, -0.0321,  0.0036,  0.0004,\n",
      "          0.0094,  0.0029,  0.0042,  0.0099, -0.0069,  0.0014, -0.0162, -0.0421,\n",
      "          0.0053, -0.0011, -0.0038, -0.0133,  0.0057,  0.0104, -0.0272, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0128,  0.0076,  0.0035,  0.0108, -0.0032, -0.0064,\n",
      "         -0.0118,  0.0003,  0.0039,  0.0133,  0.0046, -0.0169,  0.0249,  0.0177,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0296, -0.0005, -0.0013,  0.0098, -0.0094]],\n",
      "       device='cuda:0', requires_grad=True), tensor([[-0.0203,  0.0265,  0.0265,  0.0092,  0.0198,  0.0188,  0.0240, -0.0010,\n",
      "         -0.0063, -0.0164,  0.0229,  0.0131,  0.0095, -0.0219, -0.0100, -0.0188,\n",
      "          0.0344,  0.0322,  0.0388, -0.0056,  0.0178,  0.0253,  0.0165,  0.0336,\n",
      "          0.0009,  0.0037,  0.0136, -0.0013,  0.0142,  0.0145, -0.0167,  0.0277,\n",
      "         -0.0181, -0.0092,  0.0094,  0.0225,  0.0055, -0.0187,  0.0293,  0.0202,\n",
      "         -0.0229, -0.0218, -0.0211,  0.0275, -0.0079,  0.0148, -0.0202, -0.0275,\n",
      "         -0.0081,  0.0108,  0.0158,  0.0093, -0.0247, -0.0167,  0.0544, -0.0046,\n",
      "          0.0094, -0.0066, -0.0074,  0.0157,  0.0086,  0.0234, -0.0052,  0.0203,\n",
      "         -0.0221,  0.0008,  0.0158, -0.0140,  0.0020,  0.0103, -0.0138, -0.0159,\n",
      "         -0.0169,  0.0060, -0.0049, -0.0009,  0.0071, -0.0127,  0.0130, -0.0022,\n",
      "          0.0123, -0.0407,  0.0122,  0.0169, -0.0096, -0.0321,  0.0036,  0.0004,\n",
      "          0.0094,  0.0029,  0.0042,  0.0099, -0.0069,  0.0014, -0.0162, -0.0421,\n",
      "          0.0053, -0.0011, -0.0038, -0.0133,  0.0057,  0.0104, -0.0272, -0.0180,\n",
      "         -0.0036, -0.0044, -0.0128,  0.0076,  0.0035,  0.0108, -0.0032, -0.0064,\n",
      "         -0.0118,  0.0003,  0.0039,  0.0133,  0.0046, -0.0169,  0.0249,  0.0177,\n",
      "         -0.0003, -0.0218, -0.0235,  0.0296, -0.0005, -0.0013,  0.0098, -0.0093]],\n",
      "       device='cuda:0', requires_grad=True)] tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "res = torch.load(\"cel.pt\")\n",
    "emb_list, weights = res\n",
    "print(emb_list, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(weights.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.cat(emb_list, dim=0).squeeze(1)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flcore.servers.serverfedtrans import Attn_Model\n",
    "device = \"cuda:0\"\n",
    "x.to(device)\n",
    "attn_model = Attn_Model().to(device)\n",
    "weights = attn_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0204, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "psub_res = torch.load(\"psub_res.pth\")\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = torch.dot(a, b)\n",
    "    norm_a = torch.norm(a)\n",
    "    norm_b = torch.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "print(cosine_similarity(psub_res[1],psub_res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10100])\n"
     ]
    }
   ],
   "source": [
    "print(psub_res[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = torch.load(\"models\\Cifar10\\FedTrans_server_attn.pt\")\n",
    "intra_attn_model = attn_model['intra_attn_model']\n",
    "\n",
    "psub_res = [p.unsqueeze(0) for p in psub_res]\n",
    "\n",
    "x = torch.cat(psub_res, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10100])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flcore.servers.serverfedtrans import Attn_Model\n",
    "attn_model2 = Attn_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: weight, Size: torch.Size([1024, 10100])\n",
      "Layer: bias, Size: torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for name, param in emb_layer.named_parameters():\n",
    "    print(f\"Layer: {name}, Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = nn.Linear(x.size(1),intra_attn_model.query.weight.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list = [psub for psub in psub_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_e, iter_w = res['inter_clusters_res']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iter_e[1].size())\n",
    "\n",
    "x = torch.cat(iter_e, dim=0).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Attn_Model_C(nn.Module):\n",
    "    def __init__(self, emb_dim=128, attn_dim=128, num_heads=8):\n",
    "        super(Attn_Model_C, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.query = nn.Linear(emb_dim, attn_dim)\n",
    "        self.key = nn.Linear(emb_dim, attn_dim)\n",
    "        #self.inter_LN = nn.LayerNorm(attn_dim)\n",
    "\n",
    "        # 1-layer attention for simple verify\n",
    "\n",
    "    def forward(self, x, models=None, prev_models=None):\n",
    "        #x = self.inter_LN(x) \n",
    "        q = self.query(x)\n",
    "\n",
    "        k = self.key(x)\n",
    "        print(\"q:{}\\n{}\\nk:{}\".format(q,\"-\"*5,k))\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) \n",
    "        #scores = torch.matmul(q, k.transpose(-2, -1)) / (self.attn_dim ** 0.2)\n",
    "        print(scores)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attn_model_1 = Attn_Model_C().to(device)\n",
    "w = attn_model(x.to(device))\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in res['intra_clusters_res']:\n",
    "    if c is not None:\n",
    "        c_e = c[0]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    x = torch.cat(c_e, dim=0).squeeze(1)\n",
    "    w = attn_model_1(x.to(device))\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(res['intra_clusters_res'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"gm_avg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "ps = []\n",
    "param = [p.view(-1) for p in model.parameters()]\n",
    "ps = torch.concat(param, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试attn参数backward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model1 = torch.nn.Linear(10, 20)\n",
    "model2 = torch.nn.Linear(10, 20)\n",
    "emb_layer = torch.nn.Linear(220, 128)\n",
    "for p_1, p_2 in zip(model1.parameters(), model2.parameters()):\n",
    "    p_1.data += p_2.data\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sd1:OrderedDict([('weight', tensor([[-0.2774, -0.2567, -0.1063,  0.2227,  0.2273, -0.2888, -0.0162, -0.2818,\n",
      "         -0.2191, -0.2335],\n",
      "        [-0.1504, -0.0964,  0.2403,  0.2067, -0.0128, -0.0865,  0.0328,  0.2412,\n",
      "         -0.2551, -0.2395],\n",
      "        [ 0.0140, -0.0507, -0.2754, -0.2450,  0.0605,  0.3154, -0.1677,  0.0574,\n",
      "         -0.0959, -0.1168],\n",
      "        [-0.1130, -0.2067, -0.1491, -0.1524,  0.0207,  0.0843,  0.2871, -0.2830,\n",
      "         -0.1689,  0.2578],\n",
      "        [ 0.1576, -0.0371, -0.0695, -0.2472,  0.2229,  0.1555, -0.1405, -0.1537,\n",
      "         -0.0910,  0.1616],\n",
      "        [-0.0249, -0.2841,  0.2248, -0.2295, -0.1144,  0.1063, -0.2562, -0.0930,\n",
      "         -0.2208,  0.0830],\n",
      "        [ 0.0579, -0.0092,  0.3107,  0.2145,  0.0440,  0.2182, -0.0439,  0.0309,\n",
      "         -0.1744, -0.2256],\n",
      "        [-0.1732, -0.3109,  0.2855,  0.2518,  0.0639, -0.2686, -0.0803,  0.2862,\n",
      "          0.2051, -0.0335],\n",
      "        [-0.0442,  0.0963,  0.0724,  0.1717, -0.0988, -0.2718, -0.0267,  0.0407,\n",
      "          0.3117,  0.0997],\n",
      "        [ 0.2343,  0.0173,  0.3033, -0.1552,  0.2577, -0.1046, -0.2383,  0.0548,\n",
      "         -0.0366, -0.0491],\n",
      "        [ 0.0805, -0.2709,  0.1919,  0.3106, -0.0467,  0.1405,  0.0632,  0.2682,\n",
      "         -0.1752,  0.3035],\n",
      "        [ 0.0702,  0.1735,  0.0859, -0.1146,  0.0361, -0.1928,  0.0696,  0.1944,\n",
      "          0.0252,  0.1878],\n",
      "        [-0.2159, -0.1762,  0.0228,  0.0793, -0.2368,  0.1902,  0.2013, -0.0473,\n",
      "         -0.0893,  0.0676],\n",
      "        [ 0.0301,  0.2460,  0.2494,  0.0079, -0.2433, -0.1876,  0.1123,  0.3156,\n",
      "         -0.0744,  0.1884],\n",
      "        [ 0.1343,  0.1469, -0.2457,  0.0629,  0.0042,  0.2596, -0.1598,  0.3082,\n",
      "         -0.2743,  0.0798],\n",
      "        [ 0.1701,  0.2827, -0.2746,  0.2711,  0.0121, -0.2384,  0.1725, -0.1927,\n",
      "         -0.2032, -0.1528],\n",
      "        [ 0.2806,  0.1844, -0.0546,  0.2081, -0.0486, -0.0095, -0.1124, -0.3065,\n",
      "         -0.1977,  0.0735],\n",
      "        [ 0.2261, -0.1868,  0.0040, -0.1239,  0.0069,  0.2864,  0.2168,  0.0582,\n",
      "          0.1938,  0.0862],\n",
      "        [-0.2154,  0.2840, -0.0557, -0.3088, -0.1709, -0.0021,  0.0488,  0.3160,\n",
      "         -0.1526, -0.2648],\n",
      "        [-0.1226, -0.1405,  0.1336,  0.0537,  0.0047, -0.0874,  0.1435,  0.0455,\n",
      "          0.1026,  0.1128]])), ('bias', tensor([-0.2965,  0.0474, -0.2738,  0.0367,  0.1664, -0.2710, -0.2690, -0.1954,\n",
      "         0.1785,  0.0552,  0.0343,  0.1486, -0.0665, -0.0024,  0.1734, -0.0132,\n",
      "         0.0554, -0.1333, -0.0825, -0.0623]))])\n",
      "sd3:OrderedDict([('weight', tensor([[-0.2774, -0.2567, -0.1063,  0.2227,  0.2273, -0.2888, -0.0162, -0.2818,\n",
      "         -0.2191, -0.2335],\n",
      "        [-0.1504, -0.0964,  0.2403,  0.2067, -0.0128, -0.0865,  0.0328,  0.2412,\n",
      "         -0.2551, -0.2395],\n",
      "        [ 0.0140, -0.0507, -0.2754, -0.2450,  0.0605,  0.3154, -0.1677,  0.0574,\n",
      "         -0.0959, -0.1168],\n",
      "        [-0.1130, -0.2067, -0.1491, -0.1524,  0.0207,  0.0843,  0.2871, -0.2830,\n",
      "         -0.1689,  0.2578],\n",
      "        [ 0.1576, -0.0371, -0.0695, -0.2472,  0.2229,  0.1555, -0.1405, -0.1537,\n",
      "         -0.0910,  0.1616],\n",
      "        [-0.0249, -0.2841,  0.2248, -0.2295, -0.1144,  0.1063, -0.2562, -0.0930,\n",
      "         -0.2208,  0.0830],\n",
      "        [ 0.0579, -0.0092,  0.3107,  0.2145,  0.0440,  0.2182, -0.0439,  0.0309,\n",
      "         -0.1744, -0.2256],\n",
      "        [-0.1732, -0.3109,  0.2855,  0.2518,  0.0639, -0.2686, -0.0803,  0.2862,\n",
      "          0.2051, -0.0335],\n",
      "        [-0.0442,  0.0963,  0.0724,  0.1717, -0.0988, -0.2718, -0.0267,  0.0407,\n",
      "          0.3117,  0.0997],\n",
      "        [ 0.2343,  0.0173,  0.3033, -0.1552,  0.2577, -0.1046, -0.2383,  0.0548,\n",
      "         -0.0366, -0.0491],\n",
      "        [ 0.0805, -0.2709,  0.1919,  0.3106, -0.0467,  0.1405,  0.0632,  0.2682,\n",
      "         -0.1752,  0.3035],\n",
      "        [ 0.0702,  0.1735,  0.0859, -0.1146,  0.0361, -0.1928,  0.0696,  0.1944,\n",
      "          0.0252,  0.1878],\n",
      "        [-0.2159, -0.1762,  0.0228,  0.0793, -0.2368,  0.1902,  0.2013, -0.0473,\n",
      "         -0.0893,  0.0676],\n",
      "        [ 0.0301,  0.2460,  0.2494,  0.0079, -0.2433, -0.1876,  0.1123,  0.3156,\n",
      "         -0.0744,  0.1884],\n",
      "        [ 0.1343,  0.1469, -0.2457,  0.0629,  0.0042,  0.2596, -0.1598,  0.3082,\n",
      "         -0.2743,  0.0798],\n",
      "        [ 0.1701,  0.2827, -0.2746,  0.2711,  0.0121, -0.2384,  0.1725, -0.1927,\n",
      "         -0.2032, -0.1528],\n",
      "        [ 0.2806,  0.1844, -0.0546,  0.2081, -0.0486, -0.0095, -0.1124, -0.3065,\n",
      "         -0.1977,  0.0735],\n",
      "        [ 0.2261, -0.1868,  0.0040, -0.1239,  0.0069,  0.2864,  0.2168,  0.0582,\n",
      "          0.1938,  0.0862],\n",
      "        [-0.2154,  0.2840, -0.0557, -0.3088, -0.1709, -0.0021,  0.0488,  0.3160,\n",
      "         -0.1526, -0.2648],\n",
      "        [-0.1226, -0.1405,  0.1336,  0.0537,  0.0047, -0.0874,  0.1435,  0.0455,\n",
      "          0.1026,  0.1128]])), ('bias', tensor([-0.2965,  0.0474, -0.2738,  0.0367,  0.1664, -0.2710, -0.2690, -0.1954,\n",
      "         0.1785,  0.0552,  0.0343,  0.1486, -0.0665, -0.0024,  0.1734, -0.0132,\n",
      "         0.0554, -0.1333, -0.0825, -0.0623]))])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "model1 = torch.nn.Linear(10, 20)\n",
    "model2 = torch.nn.Linear(10, 20)\n",
    "sd1 = model1.state_dict()\n",
    "sd2 = model2.state_dict()\n",
    "sd3 = OrderedDict()\n",
    "for name, param in model1.named_parameters():\n",
    "    sd3[name] = param.data.clone()\n",
    "print(\"sd1:{}\\nsd3:{}\".format(sd1,sd3))\n",
    "#for name, param in model1.named_parameters():\n",
    "#    sd2[name] = param.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn_Model(nn.Module):\n",
    "    def __init__(self, emb_dim=128, attn_dim=128, num_heads=8):\n",
    "        super(Attn_Model, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.query = nn.Linear(emb_dim, attn_dim)\n",
    "        self.key = nn.Linear(emb_dim, attn_dim)\n",
    "        #self.inter_LN = nn.LayerNorm(attn_dim)\n",
    "\n",
    "        # 1-layer attention for simple verify\n",
    "\n",
    "    def forward(self, x, models=None, prev_models=None):\n",
    "        #x = self.inter_LN(x) \n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) \n",
    "\n",
    "        #scaled coef removed since we want to diff weight matrix entries\n",
    "        #scores = torch.matmul(q, k.transpose(-2, -1)) / (self.attn_dim ** 0.5)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "##\n",
    "def w_add_params(global_model, m_weights, models_params, require_grad=False):\n",
    "    params = [] \n",
    "    res = copy.deepcopy(global_model)\n",
    "    for param in res.parameters():\n",
    "        param.data.zero_()\n",
    "            \n",
    "    for w, model_params in zip(m_weights, models_params):\n",
    "        for res_param, model_param in zip(res.parameters(), model_params):\n",
    "            if require_grad:\n",
    "                res_param.grad += model_param.grad.clone().detach() * w\n",
    "            res_param.data += model_param.data.clone().detach() * w\n",
    "    return res\n",
    "        \n",
    "        \n",
    "\n",
    "def emb(phead,emb_layer):\n",
    "    params = []\n",
    "    for p in phead.parameters():\n",
    "        params.append(p.flatten())\n",
    "    params = torch.cat(params)\n",
    "    return emb_layer(params)\n",
    "\n",
    "\n",
    "def weight_flatten(model):\n",
    "    params = []\n",
    "    for u in model.parameters():\n",
    "        params.append(u.view(-1))\n",
    "    params = torch.cat(params)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建模型以及计算weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [nn.Linear(10,20) for i in range(10)]\n",
    "flatten_models = [weight_flatten(model) for model in models]\n",
    "emb_layer = nn.Linear(len(flatten_models[0]), 128)\n",
    "models_emb = [emb(model, emb_layer) for model in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([e.reshape(1,-1) for e in models_emb], dim=0)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = Attn_Model()\n",
    "\n",
    "weights = attn_model(x)\n",
    "print(weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 暂时关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ms = []\n",
    "for i in range(weights.size()[0]):\n",
    "    w = [weights[i][j] for j in range(weights[i].size()[0])]\n",
    "    sd = copy.deepcopy(models[i].state_dict())\n",
    "    new_m = w_add_parameters(models[0], w, [model for model in models])\n",
    "    new_ms.append(new_m)\n",
    "print(len(new_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = nn.utils.parameters_to_vector(models[0].parameters())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 暂时关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = new_ms\n",
    "inner_optimizers = [torch.optim.SGD(models[i].parameters(),\\\n",
    "                                lr=0.005) for i in range(10)]\n",
    "inner_states = [copy.deepcopy(m.state_dict()) for m in models]\n",
    "\n",
    "for i in range(10):\n",
    "    #one-step local training\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = models[i](inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizers[i].step()\n",
    "    #print(\"cur_model:{}\".format(nn.utils.parameters_to_vector(models[0].parameters())))\n",
    "final_states = [m.state_dict() for m in models]\n",
    "delta_thetas = [OrderedDict({k: inner_states[i][k] - final_states[i][k] for k in models[i].state_dict().keys()}) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型参数修改函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "model = models[0]\n",
    "# 我想修改的模型state\n",
    "sd = model.state_dict()\n",
    "w = weights[0]\n",
    "# 用一个带grad的weight去aggregate模型参数\n",
    "# 把参数放回state，保存带梯度的参数到另外一个list，用于梯度计算\n",
    "print(sd)\n",
    "\n",
    "def w_add_parameters(sd, w, models):\n",
    "    sg = OrderedDict()\n",
    "    for w_i, model in zip(w, models):\n",
    "        for key in sd.keys():\n",
    "            if key not in sg.keys():\n",
    "                sg[key] = w_i * model.state_dict()[key]\n",
    "            else:\n",
    "                sg[key] = sg[key] + w_i * model.state_dict()[key]\n",
    "            print(sg)\n",
    "            sd[key] = sd[key] + sg[key].data\n",
    "    return sg, sd\n",
    "sg, sd = w_add_parameters(sd, w, models)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = models[0]\n",
    "## 我想修改的模型state\n",
    "#sd = model.state_dict()\n",
    "#w = weights[0]\n",
    "## 用一个带grad的weight去aggregate模型参数\n",
    "## 把参数放回state，保存带梯度的参数到另外一个list，用于梯度计算\n",
    "\n",
    "#sg, sd = w_add_parameters(sd, w, models)\n",
    "\n",
    "inner_state = sd\n",
    "nm = nn.Linear(10,20)\n",
    "nm.load_state_dict(sd)\n",
    "inner_optimizer = torch.optim.SGD(nm.parameters(),lr=0.05)\n",
    "inner_state = copy.deepcopy(nm.state_dict())\n",
    "for _ in range(10):\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = nm(inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizer.step()\n",
    "final_state = nm.state_dict()\n",
    "delta_theta = OrderedDict({k:inner_state[k]-final_state[k] for k in nm.state_dict().keys()})\n",
    "\n",
    "lv = list(sg.values())\n",
    "param_list = list(attn_model.parameters())\n",
    "param_list.extend(emb_layer.parameters())\n",
    "params_grads = torch.autograd.grad(lv,param_list,grad_outputs=list(delta_theta.values()),retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_state = copy.deepcopy(attn_model.state_dict())\n",
    "emb_state = copy.deepcopy(emb_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "at_state_1 = copy.deepcopy(attn_model.state_dict())\n",
    "emb_state_1 = copy.deepcopy(emb_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "optimizer = torch.optim.SGD(\n",
    "            [\n",
    "                {'params': [p for p in param_list]},\n",
    "            ], lr=lr, momentum=0.9\n",
    "        )\n",
    "for p, g in zip(param_list, params_grads):\n",
    "            p.grad = g\n",
    "torch.nn.utils.clip_grad_norm_(param_list, 50)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in param_list:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_optimizer = torch.optim.SGD(nm.parameters(),lr=0.05)\n",
    "inner_state = copy.deepcopy(nm.state_dict())\n",
    "print(inner_state)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for _ in range(10):\n",
    "    inp = torch.rand([10])\n",
    "    y = torch.rand([20])\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    #print(\"prev_model:{}\".format(nn.utils.parameters_to_vector(models[i].parameters())))\n",
    "    output = nm(inp)\n",
    "    loss = mseloss(output, y)\n",
    "    loss.backward()\n",
    "    inner_optimizer.step()\n",
    "final_state = nm.state_dict()\n",
    "delta_theta = OrderedDict({k:inner_state[k]-final_state[k] for k in nm.state_dict().keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inner_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算JVP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "attn_optimizer = torch.optim.SGD(\n",
    "    [\n",
    "        {'params' : [p for p in attn_model.parameters()]} ,\n",
    "        {'params' : [p for p in emb_layer.parameters()]}\n",
    "    ], lr=lr, momentum=0.9)\n",
    "\n",
    "param_list = list(attn_model.parameters())\n",
    "param_list.extend(emb_layer.parameters())\n",
    "attn_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = [\n",
    "            torch.autograd.grad([p for p in model.parameters()], param_list , grad_outputs=list(delta_theta.values()))\\\n",
    "            for model, delta_theta in zip(models,delta_thetas)\n",
    "        ]      \n",
    "for grad in grads:\n",
    "    for param, g in zip(param_list, grad):\n",
    "        param.grad += grad \n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(param_list, 50)\n",
    "attn_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_states = model[0]\n",
    "final_states = .state_dict()\n",
    "\n",
    "# calculating delta theta\n",
    "delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
    "\n",
    "# calculating phi gradient\n",
    "hnet_grads = torch.autograd.grad(\n",
    "    list(weights.values()), hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
    ")\n",
    "\n",
    "print(torch.cat(g_1,dim=0).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0].state_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = nn.Linear(10,20)\n",
    "mo.load_state_dict(new_ms[0].state_dict())\n",
    "print(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mo.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "res = copy.deepcopy(model1)\n",
    "for param in res.parameters():\n",
    "    param.data.zero_()\n",
    "\n",
    "for rp, p in zip(res.parameters(), model1.parameters()):\n",
    "    rp.data += p.data.clone()\n",
    "    \n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/mnist\n"
     ]
    }
   ],
   "source": [
    "stri = \"../data\\\\mnist\"\n",
    "stri = stri.replace(\"\\\\\",\"/\")\n",
    "print(stri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.optim.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "run = api.run(\"gerid/fedtrans_exp/t1zr3o00\")\n",
    "max = 0\n",
    "std = 0\n",
    "for i, row in run.history().iterrows():\n",
    "    if max < row[\"test_acc\"]:\n",
    "        max = row[\"test_acc\"]\n",
    "        std = row[\"std_acc\"]\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7753406447324692 0.054822986927199446\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "print(max,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "        out, _ = self.attention(Q, K, V, attn_mask=mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = x + attn_output\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x, enc_out, mask=None):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = x + attn_output\n",
    "        attn_output = self.attention(x, enc_out, enc_out)\n",
    "        x = x + attn_output\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_encoder_layers, num_decoder_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_encoder_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_decoder_layers)])\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        for layer in self.encoder:\n",
    "            src = layer(src, src_mask)\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(tgt, src, tgt_mask)\n",
    "        return tgt\n",
    "\n",
    "# Example usage:\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "transformer = Transformer(d_model, num_heads, d_ff, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "# Dummy data\n",
    "src = torch.rand((10, 32, d_model))  # (seq_len, batch, d_model)\n",
    "tgt = torch.rand((20, 32, d_model))  # (seq_len, batch, d_model)\n",
    "\n",
    "out = transformer(src, tgt)\n",
    "print(out.shape)  # Should be torch.Size([20, 32, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (990,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37664/2121038954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Original Points'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Interpolated Points'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   3020\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \"\"\"\n\u001b[0;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (990,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 生成100个原始数据点\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 初始化插值后的数据点\n",
    "x_new = np.linspace(0, 10, 1000)\n",
    "y_new = []\n",
    "\n",
    "# 对每一对相邻的原始数据点进行随机插值\n",
    "for i in range(len(x) - 1):\n",
    "    x_range = x[i:i + 2]\n",
    "    y_range = y[i:i + 2]\n",
    "\n",
    "    # 在只有两个点的情况下，只使用线性插值\n",
    "    method = 'linear'\n",
    "    \n",
    "    f = interp1d(x_range, y_range, kind=method)\n",
    "    x_interp = np.linspace(x_range[0], x_range[1], 10)  # 在每对相邻点之间生成10个新点\n",
    "    y_interp = f(x_interp)\n",
    "    \n",
    "    y_new.extend(y_interp)\n",
    "\n",
    "# 画图\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label='Original Points')\n",
    "plt.plot(x_new, y_new, label='Interpolated Points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (90,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37664/207577325.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Original Points'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Interpolated Points'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   3020\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \"\"\"\n\u001b[0;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (90,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWL0lEQVR4nO3dfXBc113G8e+D7KSiHZBTa1JbtmOXGjeGgAWLW/AML4lbOcDEIoQ26RTcTjqeYRpeWjCx6R+ZCS1xMUMKTCj1pG7c0klaUlc1TYuaxCn9gyZkXYUoLwgLBxqvnViNo8IQkdjOjz/2KrNStJY392pX2vN8Znb23nPP3f3t2LOP7sueo4jAzMzS9QOtLsDMzFrLQWBmljgHgZlZ4hwEZmaJcxCYmSVuUasLeC2WLl0aq1evbnUZZmYLyuHDh78XEd3T2xdkEKxevZpyudzqMszMFhRJ/zVTu08NmZklzkFgZpY4B4GZWeIcBGZmiXMQmJklrpAgkLRP0klJj9XZLkl/JWlU0qOSfqpm2zZJR7LHtiLqMbOFb2Cowqbdh1iz8x427T7EwFCl1SW1raKOCO4Atpxj+5XA2uyxHfgkgKSLgJuAtwEbgZskLSmoJjNboAaGKuw6MExlfIIAKuMT7Dow7DCYI4UEQUR8Czh1ji5bgc9G1YNAl6RlQB9wb0SciojngXs5d6CYWQL2DI4wcfrslLaJ02fZMzjSooraW7OuEfQAT9esH8va6rW/iqTtksqSymNjY3NWqJm13vHxiYbaLZ8Fc7E4IvZGRCkiSt3dr/qFtJm1keVdnQ21Wz7NCoIKsLJmfUXWVq/dzBK2o28dnYs7prR1Lu5gR9+6FlXU3poVBAeB38ruHno78P2IOAEMAu+UtCS7SPzOrM3MEtbf28MtV19GT1cnAnq6Ornl6svo753xzLHlVMigc5LuBH4RWCrpGNU7gRYDRMTfAl8DfhkYBV4A3p9tOyXpT4CHs5e6OSLOddHZzBLR39vjL/4mKSQIIuK6WbYH8ME62/YB+4qow8zMGrdgLhabmdncWJDzESxkA0MV9gyOcHx8guVdnezoW+fDXzNrKQdBE03+WnLyhzKTv5YEHAZm1jI+NdRE/rWkmc1HDoIm8q8lzWw+chA0kX8taWbzkYOgifxrSTObj3yxuIkmLwj7riEzm08cBE3mX0ua2XzjU0NmZolzEJiZJc5BYGaWOF8jSJSHujCzSQ6CBHmoCzOr5VNDCfJQF2ZWy0GQIA91YWa1CgkCSVskjUgalbRzhu23Snoke/y7pPGabWdrth0soh47Nw91YWa1cgeBpA7gNuBKYD1wnaT1tX0i4kMRsSEiNgB/DRyo2TwxuS0irspbj83OQ12YWa0ijgg2AqMRcTQiXgLuAraeo/91wJ0FvK+9Rp4Y3MxqFXHXUA/wdM36MeBtM3WUdAmwBjhU0/w6SWXgDLA7Igbq7Lsd2A6watWq/FUnzkNdmNmkZl8svha4OyJqb1m5JCJKwHuAT0j6kZl2jIi9EVGKiFJ3d3czajUzS0IRQVABVtasr8jaZnIt004LRUQlez4KfBPoLaAmMzM7T0UEwcPAWklrJF1A9cv+VXf/SHorsAT4dk3bEkkXZstLgU3AEwXUZGZm5yn3NYKIOCPpBmAQ6AD2RcTjkm4GyhExGQrXAndFRNTsfinwKUkvUw2l3RHhIDAzayJN/V5eGEqlUpTL5VaXYWa2oEg6nF2TncK/LDYzS5yDwMwscQ4CM7PEOQjMzBLnIDAzS5yDwMwscQ4CM7PEOQjMzBLnIDAzS5yDwMwscQ4CM7PEOQjMzBLnIDAzS5yDwMwscQ4CM7PEFTF5vZlZ2xoYqrBncITj4xMs7+pkR986+nt7Wl1WoQo5IpC0RdKIpFFJO2fY/j5JY5IeyR4fqNm2TdKR7LGtiHrMzIowMFRh14FhKuMTBFAZn2DXgWEGhupNy74w5Q4CSR3AbcCVwHrgOknrZ+j6hYjYkD1uz/a9CLgJeBuwEbhJ0pK8NZmZFWHP4AgTp89OaZs4fZY9gyMtqmhuFHFEsBEYjYijEfEScBew9Tz37QPujYhTEfE8cC+wpYCazMxyOz4+0VD7QlVEEPQAT9esH8vapvt1SY9KulvSygb3RdJ2SWVJ5bGxsQLKNjM7t+VdnQ21L1TNumvoH4DVEfETVP/q39/oC0TE3ogoRUSpu7u78ALNzKbb0beOzsUdU9o6F3ewo29diyqaG0UEQQVYWbO+Imt7RUQ8FxEvZqu3Az99vvuambVKf28Pt1x9GT1dnQjo6erklqsva7u7hoq4ffRhYK2kNVS/xK8F3lPbQdKyiDiRrV4FPJktDwJ/WnOB+J3ArgJqMjMrRH9vT9t98U+XOwgi4oykG6h+qXcA+yLicUk3A+WIOAj8rqSrgDPAKeB92b6nJP0J1TABuDkiTuWtyczMzp8iotU1NKxUKkW5XG51GWZmC4qkwxFRmt7uISbMzBLnIDAzS5yDwMwscQ4CM7PEefRRM5sihdE2bSoHgZm9YnK0zcmB1iZH2wQcBm3Mp4bM7BWpjLZpUzkIzOwVqYy2aVM5CMzsFamMtmlTOQjM7BWpjLZpU/lisZm9YvKCsO8aSouDwMymSGG0TZvKp4bMzBLnIDAzS5yDwMwscQ4CM7PEFRIEkrZIGpE0KmnnDNs/LOkJSY9Kul/SJTXbzkp6JHscLKIeMzM7f7nvGpLUAdwGvAM4Bjws6WBEPFHTbQgoRcQLkn4b+DPg3dm2iYjYkLcOMzN7bYo4ItgIjEbE0Yh4CbgL2FrbISIeiIgXstUHgRUFvK+ZmRWgiCDoAZ6uWT+WtdVzPfD1mvXXSSpLelBSfwH1mJlZA5r6gzJJ7wVKwC/UNF8SERVJbwYOSRqOiP+YYd/twHaAVatWNaVeM7MUFHFEUAFW1qyvyNqmkLQZ+AhwVUS8ONkeEZXs+SjwTaB3pjeJiL0RUYqIUnd3dwFlm5kZFBMEDwNrJa2RdAFwLTDl7h9JvcCnqIbAyZr2JZIuzJaXApuA2ovMZmY2x3KfGoqIM5JuAAaBDmBfRDwu6WagHBEHgT3AG4C/lwTw3Yi4CrgU+JSkl6mG0u5pdxuZmdkcU0S0uoaGlUqlKJfLrS7DzGxBkXQ4IkrT2/3LYjOzxDkIzMwS5yAwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0ucg8DMLHEOAjOzxBUSBJK2SBqRNCpp5wzbL5T0hWz7Q5JW12zblbWPSOorop6ZDAxV2LT7EGt23sOm3YcYGKrM1VuZmS0ouYNAUgdwG3AlsB64TtL6ad2uB56PiLcAtwIfz/ZdT3Wy+x8DtgB/k71eoQaGKuw6MExlfIIAKuMT7Dow7DAwM6OYI4KNwGhEHI2Il4C7gK3T+mwF9mfLdwNXqDqL/Vbgroh4MSKeAkaz1yvUnsERJk6fndI2cfosewZHin4rM7MFp4gg6AGerlk/lrXN2CcizgDfB954nvsCIGm7pLKk8tjYWEMFHh+faKjdzCwlC+ZicUTsjYhSRJS6u7sb2nd5V2dD7WZmKSkiCCrAypr1FVnbjH0kLQJ+GHjuPPfNbUffOjoXT7300Lm4gx1964p+KzOzBaeIIHgYWCtpjaQLqF78PTitz0FgW7Z8DXAoIiJrvza7q2gNsBb4lwJqmqK/t4dbrr6Mnq5OBPR0dXLL1ZfR3zvjWSgzs6QsyvsCEXFG0g3AINAB7IuIxyXdDJQj4iDwaeBzkkaBU1TDgqzfF4EngDPAByPi7IxvlFN/b4+/+M3MZqDqH+YLS6lUinK53OoyzMwWFEmHI6I0vX3BXCw2M7O54SAwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0tc7rGGzF6rgaEKewZHOD4+wfKuTnb0rfN4UGYt4CCwlpicPnRy5rjJ6UMBh4FZk/nUkLWEpw81mz8cBNYSnj7UbP5wEFhLePpQs/nDQWAt4elDzeaPXEEg6SJJ90o6kj0vmaHPBknflvS4pEclvbtm2x2SnpL0SPbYkKceWzg8fajZ/JFrhjJJfwaciojdknYCSyLixml9fhSIiDgiaTlwGLg0IsYl3QF8NSLubuR9PUOZmVnj5mqGsq3A/mx5P9A/vUNE/HtEHMmWjwMnge6c72tmZgXJGwQXR8SJbPkZ4OJzdZa0EbgA+I+a5o9lp4xulXThOfbdLqksqTw2NpazbDMzmzRrEEi6T9JjMzy21vaL6jmmuueZJC0DPge8PyJezpp3AW8Ffga4CLixzu5ExN6IKEVEqbvbBxRmZkWZ9ZfFEbG53jZJz0paFhEnsi/6k3X6/RBwD/CRiHiw5rUnjyZelPQZ4A8bqt7MzHLLe2roILAtW94GfGV6B0kXAF8GPjv9onAWHkgS1esLj+Wsx8zMGpQ3CHYD75B0BNicrSOpJOn2rM+7gJ8H3jfDbaKflzQMDANLgY/mrMfMzBqU6/bRVvHto2ZmjZur20fNzGyBcxCYmSXOQWBmljgHgZlZ4hwEZmaJcxCYmSXOQWBmljgHgZlZ4hwEZmaJcxCYmSXOQWBmljgHgZlZ4hwEZmaJcxCYmSXOQWBmljgHgZlZ4nIFgaSLJN0r6Uj2vKROv7M1s5MdrGlfI+khSaOSvpBNa2lmZk2U94hgJ3B/RKwF7s/WZzIRERuyx1U17R8Hbo2ItwDPA9fnrMfMzBqUNwi2Avuz5f1UJ6A/L9mE9ZcDkxPaN7S/mZkVI28QXBwRJ7LlZ4CL6/R7naSypAcl9WdtbwTGI+JMtn4M6Kn3RpK2Z69RHhsby1m2mZlNWjRbB0n3AW+aYdNHalciIiRFnZe5JCIqkt4MHJI0DHy/kUIjYi+wF6qT1zeyr5mZ1TdrEETE5nrbJD0raVlEnJC0DDhZ5zUq2fNRSd8EeoEvAV2SFmVHBSuAymv4DGZmbW1gqMKewRGOj0+wvKuTHX3r6O+tewKlYXlPDR0EtmXL24CvTO8gaYmkC7PlpcAm4ImICOAB4Jpz7W9mlrKBoQq7DgxTGZ8ggMr4BLsODDMwVNzfzXmDYDfwDklHgM3ZOpJKkm7P+lwKlCX9K9Uv/t0R8US27Ubgw5JGqV4z+HTOeszM2sqewREmTp+d0jZx+ix7BkcKe49ZTw2dS0Q8B1wxQ3sZ+EC2/M/AZXX2PwpszFODmVk7Oz4+0VD7a+FfFpuZzWPLuzoban8tHARmZvPYjr51dC7umNLWubiDHX3rCnuPXKeGzMxsbk3eHTSXdw05CMzM5rn+3p5Cv/in86khM7PEOQjMzBLnIDAzS5yDwMwscb5YbDZPzPV4Mmb1OAjM5oHJ8WQmhxKYHE8GcBjYnPOpIbN5oBnjyZjV4yAwmweaMZ6MWT0OArN5oBnjyZjV4yAwmweaMZ6MWT2+WGw2DzRjPBmzehwEZvPEXI8nY1ZPrlNDki6SdK+kI9nzkhn6/JKkR2oe/yepP9t2h6SnarZtyFOPmZk1Lu81gp3A/RGxFrg/W58iIh6IiA0RsQG4HHgB+EZNlx2T2yPikZz1mJlZg/IGwVZgf7a8H+ifpf81wNcj4oWc72tmZgXJGwQXR8SJbPkZ4OJZ+l8L3Dmt7WOSHpV0q6QL6+0oabuksqTy2NhYjpLNzKzWrEEg6T5Jj83w2FrbLyICiHO8zjKqk9gP1jTvAt4K/AxwEXBjvf0jYm9ElCKi1N3dPVvZZmZ2nma9aygiNtfbJulZScsi4kT2RX/yHC/1LuDLEXG65rUnjyZelPQZ4A/Ps24zMytI3lNDB4Ft2fI24Cvn6Hsd004LZeGBJFG9vvBYznrMzKxBeYNgN/AOSUeAzdk6kkqSbp/sJGk1sBL4p2n7f17SMDAMLAU+mrMeMzNrUK4flEXEc8AVM7SXgQ/UrP8n8KpfykTE5Xne38zM8vNYQ2ZmiXMQmJklzkFgZpY4B4GZWeIcBGZmiXMQmJklzkFgZpY4B4GZWeIcBGZmiXMQmJklzkFgZpY4B4GZWeIcBGZmics1+qhZOxgYqrBncITj4xMs7+pkR986+ntfNViuWdtyEFjSBoYq7DowzMTpswBUxifYdWAYwGFgyfCpIUvansGRV0Jg0sTps+wZHGlRRWbNlysIJP2GpMclvSypdI5+WySNSBqVtLOmfY2kh7L2L0i6IE89Zo06Pj7RULtZO8p7RPAYcDXwrXodJHUAtwFXAuuB6yStzzZ/HLg1It4CPA9cn7Mes4Ys7+psqN2sHeUKgoh4MiJmO4beCIxGxNGIeAm4C9iaTVh/OXB31m8/1QnszZpmR986Ohd3TGnrXNzBjr51LarIrPmacbG4B3i6Zv0Y8DbgjcB4RJypaffVOWuqyQvCvmvIUjZrEEi6D3jTDJs+EhFfKb6kunVsB7YDrFq1qllvawno7+3xF78lbdYgiIjNOd+jAqysWV+RtT0HdElalB0VTLbXq2MvsBegVCpFzprMzCzTjNtHHwbWZncIXQBcCxyMiAAeAK7J+m0DmnaEYWZmVXlvH/01SceAnwXukTSYtS+X9DWA7K/9G4BB4EngixHxePYSNwIfljRK9ZrBp/PUY2ZmjVP1D/OFpVQqRblcbnUZZmYLiqTDEfGq33z5l8VmZolbkEcEksaA/3qNuy8FvldgOQuBP3Ma/JnbX97Pe0lEdE9vXJBBkIek8kyHRu3MnzkN/sztb64+r08NmZklzkFgZpa4FINgb6sLaAF/5jT4M7e/Ofm8yV0jMDOzqVI8IjAzsxoOAjOzxCUVBPVmSmtHklZKekDSE9kscr/X6pqaRVKHpCFJX211Lc0gqUvS3ZL+TdKTkn621TXNNUkfyv5fPybpTkmva3VNRZO0T9JJSY/VtF0k6V5JR7LnJUW8VzJBMMtMae3oDPAHEbEeeDvwwTb/vLV+j+q4Vqn4S+AfI+KtwE/S5p9dUg/wu0ApIn4c6KA6mGW7uQPYMq1tJ3B/RKwF7s/Wc0smCKgzU1qLa5ozEXEiIr6TLf8P1S+Hth90X9IK4FeA21tdSzNI+mHg58kGbIyIlyJivKVFNccioFPSIuAHgeMtrqdwEfEt4NS05q1UZ3OEAmd1TCkIZpopre2/GAEkrQZ6gYdaXEozfAL4I+DlFtfRLGuAMeAz2emw2yW9vtVFzaWIqAB/DnwXOAF8PyK+0dqqmubiiDiRLT8DXFzEi6YUBEmS9AbgS8DvR8R/t7qeuSTpV4GTEXG41bU00SLgp4BPRkQv8L8UdLpgvsrOi2+lGoLLgddLem9rq2q+bE6XQu7/TykI6s2U1rYkLaYaAp+PiAOtrqcJNgFXSfpPqqf+Lpf0d60tac4dA45FxOTR3t1Ug6GdbQaeioixiDgNHAB+rsU1NcuzkpYBZM8ni3jRlIJgxpnSWlzTnJEkqueNn4yIv2h1Pc0QEbsiYkVErKb673soItr6L8WIeAZ4WtK6rOkK4IkWltQM3wXeLukHs//nV9DmF8hrHKQ6myMUOKvjrHMWt4uIOCNpcqa0DmBfzUxp7WgT8JvAsKRHsrY/joivta4kmyO/A3w++wPnKPD+FtczpyLiIUl3A9+henfcEG041ISkO4FfBJZmM0HeBOwGvijpeqpD8b+rkPfyEBNmZmlL6dSQmZnNwEFgZpY4B4GZWeIcBGZmiXMQmJklzkFgZpY4B4GZWeL+H6ojUh3JDKiPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 生成100个原始数据点\n",
    "x = np.linspace(0, 10, 10)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 初始化插值后的数据点\n",
    "x_new = np.linspace(0, 10, 1000)\n",
    "y_new = []\n",
    "\n",
    "# 对每一对相邻的原始数据点进行随机插值\n",
    "for i in range(len(x) - 1):\n",
    "    x_range = x[i:i + 2]\n",
    "    y_range = y[i:i + 2]\n",
    "\n",
    "    # 在只有两个点的情况下，只使用线性插值\n",
    "    method = 'linear'\n",
    "    \n",
    "    f = interp1d(x_range, y_range, kind=method)\n",
    "    x_interp = np.linspace(x_range[0], x_range[1], 10)  # 在每对相邻点之间生成10个新点\n",
    "    y_interp = f(x_interp)\n",
    "    \n",
    "    # 添加随机噪声\n",
    "    noise = 0.1 * np.random.normal(size=len(y_interp))\n",
    "    y_interp += noise\n",
    "    \n",
    "    y_new.extend(y_interp)\n",
    "\n",
    "# 画图\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label='Original Points')\n",
    "plt.plot(x_new, y_new, label='Interpolated Points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (900,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37664/972833831.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Original Points'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Interpolated Points'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   3020\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \"\"\"\n\u001b[0;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (900,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 生成10个原始数据点\n",
    "x = np.linspace(0, 10, 10)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 初始化插值后的数据点\n",
    "x_new = np.linspace(0, 10, 1000)\n",
    "y_new = []\n",
    "\n",
    "# 对每一对相邻的原始数据点进行随机插值\n",
    "for i in range(len(x) - 1):\n",
    "    x_range = x[i:i + 2]\n",
    "    y_range = y[i:i + 2]\n",
    "\n",
    "    # 在只有两个点的情况下，只使用线性插值\n",
    "    method = 'linear'\n",
    "    \n",
    "    f = interp1d(x_range, y_range, kind=method)\n",
    "    x_interp = np.linspace(x_range[0], x_range[1], 100)  # 在每对相邻点之间生成100个新点\n",
    "    y_interp = f(x_interp)\n",
    "    \n",
    "    # 添加随机噪声\n",
    "    noise = 0.1 * np.random.normal(size=len(y_interp))\n",
    "    y_interp += noise\n",
    "    \n",
    "    y_new.extend(y_interp)\n",
    "\n",
    "# 画图\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label='Original Points', c='red')\n",
    "plt.plot(x_new, y_new, label='Interpolated Points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 生成10个原始数据点\n",
    "x = np.linspace(0, 10, 10)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 初始化插值后的数据点列表\n",
    "y_new = []\n",
    "\n",
    "# 对每一对相邻的原始数据点进行随机插值\n",
    "for i in range(len(x) - 1):\n",
    "    x_range = x[i:i + 2]\n",
    "    y_range = y[i:i + 2]\n",
    "\n",
    "    # 使用线性插值\n",
    "    method = 'linear'\n",
    "    \n",
    "    f = interp1d(x_range, y_range, kind=method)\n",
    "    x_interp = np.linspace(x_range[0], x_range[1], 111)  # 在每对相邻点之间生成111个新点\n",
    "    y_interp = f(x_interp)\n",
    "    \n",
    "    # 添加随机噪声\n",
    "    noise = 0.1 * np.random.normal(size=len(y_interp))\n",
    "    y_interp += noise\n",
    "    \n",
    "    # 除了最后一次迭代，去掉每次插值的最后一个点以避免重复\n",
    "    if i < len(x) - 2:\n",
    "        y_new.extend(y_interp[:-1])\n",
    "    else:\n",
    "        y_new.extend(y_interp)\n",
    "\n",
    "# 创建新的x轴数据点\n",
    "x_new = np.linspace(0, 10, len(y_new))\n",
    "\n",
    "# 画图\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label='Original Points', c='red')\n",
    "plt.plot(x_new, y_new, label='Interpolated Points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beb37d7b78bf3788f54259aa41c6c1e59fac34bf95ae5ff22b978ccc20cf7a1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
