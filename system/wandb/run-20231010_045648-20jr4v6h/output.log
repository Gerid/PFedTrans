
============= Running time: 0th =============
Creating server and clients ...
DNN(
  (fc1): Linear(in_features=3072, out_features=100, bias=True)
  (fc): Linear(in_features=100, out_features=100, bias=True)
)

Join ratio / total clients: 1.0 / 20
Finished creating server and clients.

-------------Round number: 0-------------

Evaluate global model
Averaged Train Loss: 7.3481
Averaged Test Accurancy: 0.0102
Std Test Accurancy: 0.0202
evaluate time cost:9.223414897918701s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:11.730351686477661s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.099432, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 250.27it/s, center_shift=0.037654, iteration=2, tol=0.000100]device is :cuda
[running kmeans]: 2it [00:00, 400.43it/s, center_shift=0.000000, iteration=3, tol=0.000100][running kmeans]: 3it [00:00, 500.24it/s, center_shift=0.000000, iteration=3, tol=0.000100]
form_cluster time cost:0.009980916976928711s
weights:tensor([[0.3337, 0.3327, 0.3336],
        [0.3337, 0.3327, 0.3336],
        [0.3337, 0.3327, 0.3336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2485, 0.2500, 0.2488, 0.2526],
        [0.2485, 0.2500, 0.2489, 0.2525],
        [0.2486, 0.2500, 0.2488, 0.2526],
        [0.2486, 0.2500, 0.2489, 0.2525]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3328, 0.3332, 0.3340],
        [0.3328, 0.3331, 0.3340],
        [0.3329, 0.3331, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5017, 0.4983],
        [0.5017, 0.4983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3341, 0.3333, 0.3326],
        [0.3341, 0.3333, 0.3326],
        [0.3341, 0.3333, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.01694488525390625s
iter time cost:20.998698949813843s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 1-------------

Evaluate global model
Averaged Train Loss: 4.1509
Averaged Test Accurancy: 0.0638
Std Test Accurancy: 0.0290
evaluate time cost:9.276655673980713s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:12.00171947479248s
attn_optimize time cost:0.03376412391662598s
weights:tensor([[0.3339, 0.3343, 0.3318],
        [0.3340, 0.3343, 0.3318],
        [0.3340, 0.3342, 0.3318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2503, 0.2495, 0.2511, 0.2492],
        [0.2503, 0.2494, 0.2511, 0.2492],
        [0.2503, 0.2494, 0.2511, 0.2491],
        [0.2503, 0.2495, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3325, 0.3358, 0.3317],
        [0.3325, 0.3358, 0.3317],
        [0.3325, 0.3358, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4999, 0.5001],
        [0.4998, 0.5002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3344, 0.3319, 0.3337],
        [0.3345, 0.3319, 0.3337],
        [0.3345, 0.3319, 0.3336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.028519392013549805s
iter time cost:21.353172302246094s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 2-------------

Evaluate global model
Averaged Train Loss: 4.1249
Averaged Test Accurancy: 0.0796
Std Test Accurancy: 0.0279
evaluate time cost:11.465810060501099s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.47313141822815s
attn_optimize time cost:0.023001909255981445s
weights:tensor([[0.3342, 0.3321, 0.3337],
        [0.3342, 0.3321, 0.3338],
        [0.3342, 0.3321, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2504, 0.2506, 0.2501, 0.2489],
        [0.2504, 0.2506, 0.2501, 0.2490],
        [0.2504, 0.2506, 0.2501, 0.2490],
        [0.2503, 0.2505, 0.2501, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3333, 0.3335, 0.3332],
        [0.3333, 0.3335, 0.3332],
        [0.3333, 0.3336, 0.3331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5024, 0.4976],
        [0.5024, 0.4976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3331, 0.3342, 0.3327],
        [0.3332, 0.3341, 0.3328],
        [0.3332, 0.3340, 0.3328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.01699972152709961s
iter time cost:21.99794316291809s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 3-------------

Evaluate global model
Averaged Train Loss: 3.9117
Averaged Test Accurancy: 0.1072
Std Test Accurancy: 0.0466
evaluate time cost:8.543938398361206s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.097207069396973s
attn_optimize time cost:0.08357715606689453s
weights:tensor([[0.3339, 0.3329, 0.3331],
        [0.3339, 0.3329, 0.3332],
        [0.3339, 0.3330, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2502, 0.2497, 0.2504, 0.2497],
        [0.2502, 0.2497, 0.2504, 0.2497],
        [0.2502, 0.2496, 0.2505, 0.2496],
        [0.2502, 0.2497, 0.2505, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3327, 0.3340, 0.3333],
        [0.3326, 0.3341, 0.3333],
        [0.3327, 0.3340, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5014, 0.4986],
        [0.5014, 0.4986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3331, 0.3340, 0.3329],
        [0.3331, 0.3340, 0.3328],
        [0.3331, 0.3340, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.030999422073364258s
iter time cost:17.796719551086426s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 4-------------

Evaluate global model
Averaged Train Loss: 3.6749
Averaged Test Accurancy: 0.1201
Std Test Accurancy: 0.0477
evaluate time cost:8.247926473617554s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.826080322265625s
attn_optimize time cost:0.08730673789978027s
weights:tensor([[0.3331, 0.3345, 0.3324],
        [0.3331, 0.3345, 0.3323],
        [0.3331, 0.3345, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2498, 0.2503, 0.2509, 0.2490],
        [0.2498, 0.2503, 0.2509, 0.2490],
        [0.2498, 0.2503, 0.2509, 0.2490],
        [0.2498, 0.2504, 0.2509, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3327, 0.3344, 0.3330],
        [0.3327, 0.3344, 0.3330],
        [0.3327, 0.3343, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5003, 0.4997],
        [0.5002, 0.4998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3328, 0.3326, 0.3346],
        [0.3329, 0.3325, 0.3346],
        [0.3329, 0.3326, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.10340428352355957s
iter time cost:18.375938653945923s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 5-------------

Evaluate global model
Averaged Train Loss: 3.3686
Averaged Test Accurancy: 0.1626
Std Test Accurancy: 0.0385
evaluate time cost:8.029250860214233s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.781854629516602s
attn_optimize time cost:0.07799959182739258s
weights:tensor([[0.3340, 0.3329, 0.3331],
        [0.3340, 0.3328, 0.3331],
        [0.3341, 0.3328, 0.3331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2497, 0.2499, 0.2505, 0.2499],
        [0.2497, 0.2499, 0.2505, 0.2499],
        [0.2498, 0.2499, 0.2505, 0.2498],
        [0.2497, 0.2498, 0.2506, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3329, 0.3341, 0.3330],
        [0.3330, 0.3341, 0.3330],
        [0.3329, 0.3341, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5016, 0.4984],
        [0.5016, 0.4984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3335, 0.3340, 0.3325],
        [0.3334, 0.3340, 0.3326],
        [0.3335, 0.3341, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.028873205184936523s
iter time cost:17.94362473487854s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 6-------------

Evaluate global model
Averaged Train Loss: 3.2387
Averaged Test Accurancy: 0.1637
Std Test Accurancy: 0.0414
evaluate time cost:7.936362981796265s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.653034448623657s
attn_optimize time cost:0.08348226547241211s
weights:tensor([[0.3333, 0.3326, 0.3341],
        [0.3333, 0.3326, 0.3341],
        [0.3334, 0.3325, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2512, 0.2509, 0.2500, 0.2479],
        [0.2511, 0.2509, 0.2500, 0.2479],
        [0.2511, 0.2510, 0.2500, 0.2479],
        [0.2512, 0.2508, 0.2500, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3333, 0.3323, 0.3344],
        [0.3333, 0.3323, 0.3344],
        [0.3332, 0.3324, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4997, 0.5003],
        [0.4996, 0.5004]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3340, 0.3331, 0.3329],
        [0.3341, 0.3331, 0.3329],
        [0.3340, 0.3330, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03361082077026367s
iter time cost:17.763863801956177s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 7-------------

Evaluate global model
Averaged Train Loss: 3.5025
Averaged Test Accurancy: 0.1481
Std Test Accurancy: 0.0494
evaluate time cost:8.058433532714844s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.087145328521729s
attn_optimize time cost:0.08470678329467773s
weights:tensor([[0.3328, 0.3340, 0.3332],
        [0.3329, 0.3339, 0.3332],
        [0.3329, 0.3339, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2511, 0.2499, 0.2508, 0.2481],
        [0.2511, 0.2500, 0.2508, 0.2481],
        [0.2511, 0.2498, 0.2508, 0.2482],
        [0.2511, 0.2500, 0.2508, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3327, 0.3352, 0.3321],
        [0.3326, 0.3353, 0.3321],
        [0.3327, 0.3353, 0.3320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5016, 0.4984],
        [0.5016, 0.4984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3325, 0.3332, 0.3343],
        [0.3325, 0.3332, 0.3343],
        [0.3325, 0.3332, 0.3343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.013999700546264648s
iter time cost:17.339996099472046s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 8-------------

Evaluate global model
Averaged Train Loss: 3.5162
Averaged Test Accurancy: 0.1687
Std Test Accurancy: 0.0644
evaluate time cost:8.13260006904602s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.444749355316162s
attn_optimize time cost:0.07782554626464844s
weights:tensor([[0.3341, 0.3314, 0.3345],
        [0.3340, 0.3314, 0.3346],
        [0.3341, 0.3314, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2498, 0.2490, 0.2514, 0.2498],
        [0.2497, 0.2490, 0.2514, 0.2498],
        [0.2497, 0.2491, 0.2514, 0.2498],
        [0.2499, 0.2490, 0.2513, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3337, 0.3331, 0.3332],
        [0.3337, 0.3331, 0.3332],
        [0.3337, 0.3332, 0.3331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5009, 0.4991],
        [0.5010, 0.4990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3331, 0.3324, 0.3345],
        [0.3330, 0.3325, 0.3345],
        [0.3331, 0.3325, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.030249595642089844s
iter time cost:17.785300731658936s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 9-------------

Evaluate global model
Averaged Train Loss: 3.7534
Averaged Test Accurancy: 0.1523
Std Test Accurancy: 0.0510
evaluate time cost:8.456294298171997s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.108194351196289s
attn_optimize time cost:0.09953474998474121s
weights:tensor([[0.3324, 0.3339, 0.3337],
        [0.3325, 0.3339, 0.3337],
        [0.3325, 0.3338, 0.3337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2507, 0.2499, 0.2497, 0.2498],
        [0.2506, 0.2499, 0.2497, 0.2497],
        [0.2507, 0.2499, 0.2497, 0.2497],
        [0.2506, 0.2499, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3327, 0.3339, 0.3335],
        [0.3326, 0.3339, 0.3335],
        [0.3326, 0.3339, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5021, 0.4979],
        [0.5022, 0.4978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3332, 0.3332, 0.3337],
        [0.3332, 0.3332, 0.3336],
        [0.3332, 0.3332, 0.3336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.026926755905151367s
iter time cost:18.773505449295044s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 10-------------

Evaluate global model
Averaged Train Loss: 3.4477
Averaged Test Accurancy: 0.1733
Std Test Accurancy: 0.0533
evaluate time cost:8.000274419784546s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.723978042602539s
attn_optimize time cost:0.08019638061523438s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.026438, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 335.25it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 498.43it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.2511, 0.2504, 0.2506, 0.2479],
        [0.2510, 0.2504, 0.2506, 0.2480],
        [0.2510, 0.2504, 0.2506, 0.2480],
        [0.2511, 0.2504, 0.2506, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3322, 0.3332, 0.3346],
        [0.3322, 0.3331, 0.3347],
        [0.3322, 0.3332, 0.3346]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3353, 0.3328, 0.3320],
        [0.3352, 0.3328, 0.3321],
        [0.3353, 0.3326, 0.3321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4986, 0.5014],
        [0.4986, 0.5014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3330, 0.3335, 0.3336],
        [0.3330, 0.3334, 0.3336],
        [0.3329, 0.3335, 0.3336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.016206741333007812s
iter time cost:17.95329236984253s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 11-------------

Evaluate global model
Averaged Train Loss: 3.0735
Averaged Test Accurancy: 0.2130
Std Test Accurancy: 0.0414
evaluate time cost:7.666847467422485s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.828810453414917s
attn_optimize time cost:0.10454392433166504s
weights:tensor([[0.2505, 0.2497, 0.2507, 0.2491],
        [0.2505, 0.2497, 0.2507, 0.2491],
        [0.2505, 0.2497, 0.2507, 0.2491],
        [0.2505, 0.2496, 0.2507, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3335, 0.3321, 0.3344],
        [0.3334, 0.3322, 0.3344],
        [0.3335, 0.3322, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3325, 0.3330, 0.3345],
        [0.3325, 0.3330, 0.3345],
        [0.3325, 0.3330, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4989, 0.5011],
        [0.4990, 0.5010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3332, 0.3332, 0.3336],
        [0.3332, 0.3332, 0.3336],
        [0.3332, 0.3332, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.053411006927490234s
iter time cost:17.76519799232483s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 12-------------

Evaluate global model
Averaged Train Loss: 3.2487
Averaged Test Accurancy: 0.1994
Std Test Accurancy: 0.0515
evaluate time cost:8.116005420684814s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.027265071868896s
attn_optimize time cost:0.0942692756652832s
weights:tensor([[0.2498, 0.2503, 0.2503, 0.2496],
        [0.2498, 0.2504, 0.2503, 0.2496],
        [0.2498, 0.2504, 0.2503, 0.2496],
        [0.2498, 0.2503, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3335, 0.3342, 0.3323],
        [0.3335, 0.3342, 0.3323],
        [0.3334, 0.3340, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3331, 0.3344, 0.3325],
        [0.3331, 0.3344, 0.3325],
        [0.3331, 0.3344, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5003, 0.4997],
        [0.5004, 0.4996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3324, 0.3339, 0.3337],
        [0.3325, 0.3338, 0.3337],
        [0.3325, 0.3338, 0.3337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.028245925903320312s
iter time cost:18.36224627494812s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 13-------------

Evaluate global model
Averaged Train Loss: 3.2414
Averaged Test Accurancy: 0.2160
Std Test Accurancy: 0.0530
evaluate time cost:7.853973388671875s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.36519718170166s
attn_optimize time cost:0.10579466819763184s
weights:tensor([[0.2499, 0.2495, 0.2509, 0.2497],
        [0.2499, 0.2495, 0.2509, 0.2498],
        [0.2499, 0.2495, 0.2509, 0.2497],
        [0.2499, 0.2495, 0.2508, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3351, 0.3333, 0.3316],
        [0.3351, 0.3333, 0.3316],
        [0.3350, 0.3333, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3354, 0.3319, 0.3327],
        [0.3354, 0.3319, 0.3327],
        [0.3354, 0.3319, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4991, 0.5009],
        [0.4991, 0.5009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3342, 0.3328, 0.3330],
        [0.3343, 0.3328, 0.3329],
        [0.3342, 0.3328, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.10982131958007812s
iter time cost:18.551079988479614s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 14-------------

Evaluate global model
Averaged Train Loss: 3.0258
Averaged Test Accurancy: 0.2164
Std Test Accurancy: 0.0468
evaluate time cost:7.879448175430298s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.949885368347168s
attn_optimize time cost:0.08247017860412598s
weights:tensor([[0.2507, 0.2483, 0.2501, 0.2509],
        [0.2507, 0.2483, 0.2502, 0.2509],
        [0.2506, 0.2483, 0.2502, 0.2509],
        [0.2507, 0.2483, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3344, 0.3307, 0.3350],
        [0.3343, 0.3305, 0.3351],
        [0.3343, 0.3307, 0.3349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3346, 0.3332, 0.3322],
        [0.3346, 0.3333, 0.3321],
        [0.3346, 0.3332, 0.3322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5005, 0.4995],
        [0.5006, 0.4994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3355, 0.3307, 0.3338],
        [0.3356, 0.3306, 0.3337],
        [0.3356, 0.3307, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.10767102241516113s
iter time cost:18.147406578063965s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 15-------------

Evaluate global model
Averaged Train Loss: 3.1184
Averaged Test Accurancy: 0.2197
Std Test Accurancy: 0.0540
evaluate time cost:7.849618434906006s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.533333539962769s
attn_optimize time cost:0.10952520370483398s
weights:tensor([[0.2497, 0.2514, 0.2495, 0.2495],
        [0.2497, 0.2514, 0.2495, 0.2495],
        [0.2497, 0.2514, 0.2494, 0.2495],
        [0.2497, 0.2514, 0.2494, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3323, 0.3365, 0.3312],
        [0.3323, 0.3365, 0.3312],
        [0.3323, 0.3365, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3331, 0.3331, 0.3339],
        [0.3331, 0.3331, 0.3338],
        [0.3331, 0.3331, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4983, 0.5017],
        [0.4982, 0.5018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3331, 0.3342, 0.3327],
        [0.3330, 0.3343, 0.3327],
        [0.3330, 0.3343, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.029003143310546875s
iter time cost:17.57331109046936s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 16-------------

Evaluate global model
Averaged Train Loss: 2.9161
Averaged Test Accurancy: 0.2309
Std Test Accurancy: 0.0561
evaluate time cost:7.8939337730407715s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.427096128463745s
attn_optimize time cost:0.09482955932617188s
weights:tensor([[0.2503, 0.2493, 0.2499, 0.2505],
        [0.2502, 0.2494, 0.2499, 0.2505],
        [0.2503, 0.2493, 0.2500, 0.2504],
        [0.2502, 0.2494, 0.2499, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3353, 0.3334, 0.3313],
        [0.3353, 0.3334, 0.3313],
        [0.3354, 0.3334, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3330, 0.3341, 0.3329],
        [0.3330, 0.3341, 0.3329],
        [0.3330, 0.3341, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4989, 0.5011],
        [0.4990, 0.5010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3331, 0.3331, 0.3338],
        [0.3331, 0.3331, 0.3338],
        [0.3330, 0.3331, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.027823209762573242s
iter time cost:17.542486667633057s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 17-------------

Evaluate global model
Averaged Train Loss: 3.0098
Averaged Test Accurancy: 0.2225
Std Test Accurancy: 0.0454
evaluate time cost:7.823053598403931s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.642581224441528s
attn_optimize time cost:0.09975981712341309s
weights:tensor([[0.2501, 0.2505, 0.2517, 0.2477],
        [0.2501, 0.2505, 0.2517, 0.2477],
        [0.2500, 0.2505, 0.2517, 0.2478],
        [0.2500, 0.2504, 0.2516, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3341, 0.3324, 0.3335],
        [0.3341, 0.3324, 0.3335],
        [0.3341, 0.3324, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3338, 0.3325, 0.3336],
        [0.3338, 0.3325, 0.3337],
        [0.3338, 0.3326, 0.3336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5009, 0.4991],
        [0.5010, 0.4990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3322, 0.3337, 0.3341],
        [0.3322, 0.3336, 0.3342],
        [0.3322, 0.3337, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.10176563262939453s
iter time cost:17.787847757339478s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 18-------------

Evaluate global model
Averaged Train Loss: 3.0213
Averaged Test Accurancy: 0.2117
Std Test Accurancy: 0.0460
evaluate time cost:7.718471527099609s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.486713886260986s
attn_optimize time cost:0.09352707862854004s
weights:tensor([[0.2507, 0.2496, 0.2502, 0.2495],
        [0.2507, 0.2496, 0.2502, 0.2496],
        [0.2506, 0.2496, 0.2502, 0.2496],
        [0.2507, 0.2496, 0.2503, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3324, 0.3331, 0.3345],
        [0.3324, 0.3331, 0.3345],
        [0.3323, 0.3332, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3341, 0.3320, 0.3339],
        [0.3342, 0.3319, 0.3339],
        [0.3342, 0.3320, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5005, 0.4995],
        [0.5006, 0.4994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3336, 0.3330, 0.3334],
        [0.3336, 0.3330, 0.3333],
        [0.3338, 0.3329, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.026118040084838867s
iter time cost:17.33583164215088s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 19-------------

Evaluate global model
Averaged Train Loss: 3.0619
Averaged Test Accurancy: 0.2235
Std Test Accurancy: 0.0666
evaluate time cost:7.812239646911621s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.037425756454468s
attn_optimize time cost:0.06904792785644531s
weights:tensor([[0.2498, 0.2505, 0.2491, 0.2507],
        [0.2498, 0.2504, 0.2492, 0.2507],
        [0.2497, 0.2504, 0.2491, 0.2507],
        [0.2498, 0.2504, 0.2491, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3345, 0.3334, 0.3321],
        [0.3344, 0.3335, 0.3322],
        [0.3344, 0.3334, 0.3322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3344, 0.3342, 0.3314],
        [0.3344, 0.3341, 0.3314],
        [0.3344, 0.3341, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5007, 0.4993],
        [0.5008, 0.4992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3332, 0.3329, 0.3339],
        [0.3332, 0.3329, 0.3339],
        [0.3332, 0.3329, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.02620720863342285s
iter time cost:17.977813959121704s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 20-------------

Evaluate global model
Averaged Train Loss: 3.2741
Averaged Test Accurancy: 0.2136
Std Test Accurancy: 0.0490
evaluate time cost:7.879474401473999s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.100013971328735s
attn_optimize time cost:0.07807183265686035s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.004548, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 249.99it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 499.98it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.2498, 0.2478, 0.2510, 0.2514],
        [0.2498, 0.2478, 0.2510, 0.2514],
        [0.2498, 0.2478, 0.2510, 0.2514],
        [0.2498, 0.2479, 0.2509, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1243, 0.1258, 0.1245, 0.1248, 0.1258, 0.1249, 0.1243, 0.1257],
        [0.1242, 0.1258, 0.1245, 0.1248, 0.1258, 0.1249, 0.1243, 0.1257],
        [0.1243, 0.1258, 0.1244, 0.1248, 0.1258, 0.1248, 0.1243, 0.1257],
        [0.1242, 0.1258, 0.1244, 0.1248, 0.1258, 0.1248, 0.1244, 0.1257],
        [0.1243, 0.1259, 0.1244, 0.1248, 0.1258, 0.1248, 0.1243, 0.1257],
        [0.1242, 0.1258, 0.1245, 0.1248, 0.1258, 0.1249, 0.1244, 0.1257],
        [0.1242, 0.1258, 0.1245, 0.1248, 0.1258, 0.1249, 0.1243, 0.1257],
        [0.1242, 0.1258, 0.1244, 0.1248, 0.1258, 0.1249, 0.1243, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.012510299682617188s
iter time cost:17.171752214431763s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 21-------------

Evaluate global model
Averaged Train Loss: 2.7979
Averaged Test Accurancy: 0.2454
Std Test Accurancy: 0.0446
evaluate time cost:7.699204683303833s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.866796016693115s
attn_optimize time cost:0.08866357803344727s
weights:tensor([[0.2489, 0.2503, 0.2515, 0.2492],
        [0.2489, 0.2504, 0.2515, 0.2492],
        [0.2489, 0.2503, 0.2516, 0.2493],
        [0.2490, 0.2502, 0.2516, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1253, 0.1242, 0.1252, 0.1252, 0.1249, 0.1260, 0.1248, 0.1244],
        [0.1253, 0.1243, 0.1251, 0.1252, 0.1249, 0.1260, 0.1248, 0.1244],
        [0.1253, 0.1242, 0.1252, 0.1252, 0.1249, 0.1260, 0.1248, 0.1244],
        [0.1253, 0.1243, 0.1252, 0.1252, 0.1249, 0.1259, 0.1248, 0.1244],
        [0.1253, 0.1243, 0.1252, 0.1252, 0.1249, 0.1260, 0.1248, 0.1244],
        [0.1253, 0.1243, 0.1252, 0.1252, 0.1249, 0.1259, 0.1248, 0.1244],
        [0.1253, 0.1242, 0.1252, 0.1251, 0.1249, 0.1260, 0.1248, 0.1244],
        [0.1253, 0.1242, 0.1252, 0.1252, 0.1249, 0.1260, 0.1248, 0.1244]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03203082084655762s
iter time cost:16.774264574050903s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 22-------------

Evaluate global model
Averaged Train Loss: 2.9293
Averaged Test Accurancy: 0.2384
Std Test Accurancy: 0.0499
evaluate time cost:7.734740972518921s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.944866180419922s
attn_optimize time cost:0.08089280128479004s
weights:tensor([[0.2508, 0.2488, 0.2501, 0.2503],
        [0.2508, 0.2489, 0.2501, 0.2502],
        [0.2507, 0.2487, 0.2502, 0.2504],
        [0.2508, 0.2489, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1252, 0.1245, 0.1246, 0.1253, 0.1253, 0.1247, 0.1247, 0.1257],
        [0.1252, 0.1245, 0.1246, 0.1254, 0.1253, 0.1247, 0.1247, 0.1257],
        [0.1252, 0.1246, 0.1246, 0.1253, 0.1253, 0.1246, 0.1247, 0.1257],
        [0.1252, 0.1245, 0.1245, 0.1253, 0.1253, 0.1247, 0.1247, 0.1257],
        [0.1252, 0.1245, 0.1245, 0.1254, 0.1253, 0.1247, 0.1247, 0.1257],
        [0.1252, 0.1245, 0.1246, 0.1254, 0.1252, 0.1247, 0.1247, 0.1257],
        [0.1252, 0.1245, 0.1246, 0.1253, 0.1253, 0.1247, 0.1247, 0.1257],
        [0.1252, 0.1246, 0.1246, 0.1253, 0.1253, 0.1247, 0.1247, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.09705352783203125s
iter time cost:17.972086668014526s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 23-------------

Evaluate global model
Averaged Train Loss: 3.0905
Averaged Test Accurancy: 0.2406
Std Test Accurancy: 0.0581
evaluate time cost:8.022610902786255s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.938359260559082s
attn_optimize time cost:0.09380125999450684s
weights:tensor([[0.2505, 0.2488, 0.2499, 0.2508],
        [0.2505, 0.2489, 0.2499, 0.2508],
        [0.2505, 0.2488, 0.2499, 0.2508],
        [0.2505, 0.2489, 0.2498, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1244, 0.1251, 0.1255, 0.1247, 0.1257, 0.1244, 0.1244, 0.1257],
        [0.1244, 0.1252, 0.1255, 0.1247, 0.1257, 0.1244, 0.1244, 0.1257],
        [0.1244, 0.1252, 0.1255, 0.1247, 0.1257, 0.1244, 0.1244, 0.1258],
        [0.1244, 0.1251, 0.1255, 0.1248, 0.1257, 0.1244, 0.1244, 0.1257],
        [0.1244, 0.1251, 0.1255, 0.1248, 0.1257, 0.1244, 0.1244, 0.1257],
        [0.1244, 0.1251, 0.1255, 0.1247, 0.1257, 0.1244, 0.1245, 0.1258],
        [0.1244, 0.1251, 0.1255, 0.1247, 0.1257, 0.1245, 0.1244, 0.1257],
        [0.1243, 0.1251, 0.1255, 0.1248, 0.1257, 0.1244, 0.1244, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.020998001098632812s
iter time cost:17.191784143447876s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 24-------------

Evaluate global model
Averaged Train Loss: 2.9371
Averaged Test Accurancy: 0.2411
Std Test Accurancy: 0.0502
evaluate time cost:7.635315179824829s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.710522413253784s
attn_optimize time cost:0.0810699462890625s
weights:tensor([[0.2493, 0.2492, 0.2517, 0.2499],
        [0.2491, 0.2492, 0.2518, 0.2498],
        [0.2492, 0.2492, 0.2518, 0.2499],
        [0.2492, 0.2492, 0.2517, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1248, 0.1255, 0.1254, 0.1252, 0.1250, 0.1252, 0.1246, 0.1243],
        [0.1248, 0.1256, 0.1254, 0.1252, 0.1250, 0.1252, 0.1247, 0.1243],
        [0.1248, 0.1255, 0.1254, 0.1252, 0.1249, 0.1253, 0.1246, 0.1243],
        [0.1248, 0.1256, 0.1254, 0.1252, 0.1250, 0.1252, 0.1246, 0.1243],
        [0.1248, 0.1256, 0.1254, 0.1251, 0.1250, 0.1252, 0.1246, 0.1243],
        [0.1248, 0.1256, 0.1254, 0.1251, 0.1250, 0.1252, 0.1246, 0.1243],
        [0.1248, 0.1256, 0.1254, 0.1252, 0.1250, 0.1252, 0.1247, 0.1243],
        [0.1248, 0.1256, 0.1254, 0.1252, 0.1250, 0.1252, 0.1246, 0.1243]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03119349479675293s
iter time cost:17.542570114135742s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 25-------------

Evaluate global model
Averaged Train Loss: 2.7575
Averaged Test Accurancy: 0.2636
Std Test Accurancy: 0.0568
evaluate time cost:7.826186418533325s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.388652324676514s
attn_optimize time cost:0.017986297607421875s
weights:tensor([[0.2493, 0.2504, 0.2511, 0.2492],
        [0.2493, 0.2504, 0.2511, 0.2492],
        [0.2493, 0.2504, 0.2511, 0.2492],
        [0.2493, 0.2504, 0.2511, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1241, 0.1261, 0.1238, 0.1255, 0.1253, 0.1266, 0.1238, 0.1248],
        [0.1241, 0.1261, 0.1238, 0.1254, 0.1252, 0.1266, 0.1239, 0.1248],
        [0.1241, 0.1261, 0.1238, 0.1255, 0.1253, 0.1266, 0.1238, 0.1248],
        [0.1242, 0.1261, 0.1238, 0.1255, 0.1253, 0.1266, 0.1239, 0.1248],
        [0.1242, 0.1261, 0.1238, 0.1255, 0.1253, 0.1266, 0.1238, 0.1248],
        [0.1242, 0.1261, 0.1238, 0.1254, 0.1253, 0.1265, 0.1238, 0.1248],
        [0.1242, 0.1261, 0.1238, 0.1254, 0.1252, 0.1266, 0.1238, 0.1248],
        [0.1242, 0.1261, 0.1238, 0.1254, 0.1253, 0.1266, 0.1238, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03399968147277832s
iter time cost:17.28503155708313s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 26-------------

Evaluate global model
Averaged Train Loss: 2.6776
Averaged Test Accurancy: 0.2735
Std Test Accurancy: 0.0541
evaluate time cost:7.988267660140991s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.62808084487915s
attn_optimize time cost:0.08508491516113281s
weights:tensor([[0.2490, 0.2484, 0.2510, 0.2516],
        [0.2490, 0.2484, 0.2509, 0.2517],
        [0.2490, 0.2484, 0.2510, 0.2516],
        [0.2490, 0.2484, 0.2509, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1245, 0.1243, 0.1256, 0.1248, 0.1250, 0.1263, 0.1248, 0.1247],
        [0.1245, 0.1243, 0.1255, 0.1248, 0.1249, 0.1263, 0.1248, 0.1247],
        [0.1245, 0.1244, 0.1255, 0.1248, 0.1249, 0.1264, 0.1248, 0.1247],
        [0.1246, 0.1243, 0.1256, 0.1248, 0.1249, 0.1263, 0.1248, 0.1246],
        [0.1245, 0.1243, 0.1255, 0.1248, 0.1250, 0.1263, 0.1248, 0.1247],
        [0.1245, 0.1243, 0.1255, 0.1248, 0.1250, 0.1263, 0.1248, 0.1247],
        [0.1245, 0.1243, 0.1255, 0.1248, 0.1250, 0.1263, 0.1248, 0.1247],
        [0.1245, 0.1243, 0.1255, 0.1248, 0.1250, 0.1263, 0.1249, 0.1247]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.031996726989746094s
iter time cost:16.82104516029358s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 27-------------

Evaluate global model
Averaged Train Loss: 2.8016
Averaged Test Accurancy: 0.2519
Std Test Accurancy: 0.0496
evaluate time cost:7.771025657653809s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.904959678649902s
attn_optimize time cost:0.08077692985534668s
weights:tensor([[0.2503, 0.2496, 0.2502, 0.2499],
        [0.2504, 0.2495, 0.2503, 0.2499],
        [0.2504, 0.2496, 0.2501, 0.2499],
        [0.2504, 0.2495, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1241, 0.1259, 0.1248, 0.1248, 0.1250, 0.1254, 0.1252, 0.1248],
        [0.1241, 0.1259, 0.1248, 0.1248, 0.1249, 0.1253, 0.1253, 0.1248],
        [0.1242, 0.1258, 0.1248, 0.1248, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1241, 0.1259, 0.1248, 0.1248, 0.1250, 0.1253, 0.1252, 0.1248],
        [0.1241, 0.1259, 0.1248, 0.1248, 0.1250, 0.1253, 0.1253, 0.1248],
        [0.1241, 0.1259, 0.1249, 0.1248, 0.1250, 0.1254, 0.1252, 0.1248],
        [0.1241, 0.1259, 0.1248, 0.1248, 0.1250, 0.1253, 0.1252, 0.1248],
        [0.1242, 0.1259, 0.1248, 0.1248, 0.1249, 0.1254, 0.1252, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.08608293533325195s
iter time cost:17.972358226776123s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 28-------------

Evaluate global model
Averaged Train Loss: 2.6266
Averaged Test Accurancy: 0.2575
Std Test Accurancy: 0.0408
evaluate time cost:7.8589653968811035s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.766647577285767s
attn_optimize time cost:0.08949565887451172s
weights:tensor([[0.2500, 0.2494, 0.2503, 0.2503],
        [0.2501, 0.2494, 0.2503, 0.2502],
        [0.2501, 0.2493, 0.2503, 0.2503],
        [0.2501, 0.2494, 0.2503, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1248, 0.1250, 0.1257, 0.1258, 0.1252, 0.1256, 0.1233, 0.1247],
        [0.1248, 0.1249, 0.1257, 0.1257, 0.1253, 0.1256, 0.1233, 0.1247],
        [0.1248, 0.1250, 0.1257, 0.1257, 0.1252, 0.1255, 0.1233, 0.1247],
        [0.1248, 0.1249, 0.1257, 0.1258, 0.1252, 0.1256, 0.1233, 0.1247],
        [0.1248, 0.1250, 0.1257, 0.1257, 0.1253, 0.1255, 0.1233, 0.1247],
        [0.1248, 0.1250, 0.1257, 0.1257, 0.1252, 0.1255, 0.1233, 0.1248],
        [0.1249, 0.1250, 0.1257, 0.1258, 0.1253, 0.1255, 0.1233, 0.1247],
        [0.1248, 0.1249, 0.1257, 0.1258, 0.1252, 0.1255, 0.1233, 0.1247]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.12540459632873535s
iter time cost:17.955784797668457s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 29-------------

Evaluate global model
Averaged Train Loss: 2.6724
Averaged Test Accurancy: 0.2610
Std Test Accurancy: 0.0542
evaluate time cost:7.897860050201416s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.684002876281738s
attn_optimize time cost:0.08425259590148926s
weights:tensor([[0.2513, 0.2483, 0.2502, 0.2502],
        [0.2514, 0.2482, 0.2502, 0.2502],
        [0.2513, 0.2482, 0.2502, 0.2502],
        [0.2514, 0.2482, 0.2503, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1248, 0.1261, 0.1241, 0.1247, 0.1251, 0.1257, 0.1249, 0.1247],
        [0.1248, 0.1261, 0.1241, 0.1246, 0.1251, 0.1257, 0.1249, 0.1247],
        [0.1248, 0.1261, 0.1241, 0.1246, 0.1251, 0.1257, 0.1249, 0.1247],
        [0.1248, 0.1261, 0.1241, 0.1246, 0.1251, 0.1257, 0.1249, 0.1247],
        [0.1248, 0.1261, 0.1241, 0.1246, 0.1251, 0.1256, 0.1250, 0.1247],
        [0.1247, 0.1261, 0.1241, 0.1246, 0.1251, 0.1257, 0.1249, 0.1247],
        [0.1248, 0.1260, 0.1241, 0.1246, 0.1251, 0.1257, 0.1249, 0.1247],
        [0.1248, 0.1260, 0.1241, 0.1246, 0.1251, 0.1257, 0.1250, 0.1247]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03450942039489746s
iter time cost:17.789928913116455s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 30-------------

Evaluate global model
Averaged Train Loss: 2.6036
Averaged Test Accurancy: 0.2639
Std Test Accurancy: 0.0540
evaluate time cost:8.226935625076294s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.219215869903564s
attn_optimize time cost:0.08595108985900879s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.015493, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 332.83it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 665.66it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.1665, 0.1671, 0.1676, 0.1668, 0.1656, 0.1665],
        [0.1664, 0.1671, 0.1676, 0.1667, 0.1656, 0.1665],
        [0.1664, 0.1671, 0.1677, 0.1668, 0.1655, 0.1665],
        [0.1664, 0.1672, 0.1677, 0.1667, 0.1655, 0.1665],
        [0.1664, 0.1671, 0.1677, 0.1667, 0.1656, 0.1665],
        [0.1664, 0.1672, 0.1676, 0.1668, 0.1656, 0.1665]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3346, 0.3321, 0.3333],
        [0.3343, 0.3323, 0.3334],
        [0.3344, 0.3322, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5010, 0.4990],
        [0.5011, 0.4989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3326, 0.3343, 0.3331],
        [0.3326, 0.3343, 0.3331],
        [0.3326, 0.3343, 0.3331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.014546871185302734s
iter time cost:18.74106526374817s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 31-------------

Evaluate global model
Averaged Train Loss: 2.9326
Averaged Test Accurancy: 0.2525
Std Test Accurancy: 0.0558
evaluate time cost:7.907134771347046s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.867012023925781s
attn_optimize time cost:0.0931553840637207s
weights:tensor([[0.1653, 0.1671, 0.1672, 0.1667, 0.1668, 0.1669],
        [0.1652, 0.1672, 0.1672, 0.1667, 0.1668, 0.1669],
        [0.1652, 0.1672, 0.1672, 0.1666, 0.1668, 0.1669],
        [0.1653, 0.1672, 0.1671, 0.1667, 0.1668, 0.1669],
        [0.1653, 0.1672, 0.1671, 0.1667, 0.1669, 0.1669],
        [0.1652, 0.1672, 0.1671, 0.1667, 0.1668, 0.1669]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3367, 0.3326, 0.3308],
        [0.3368, 0.3324, 0.3308],
        [0.3367, 0.3325, 0.3308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4980, 0.5020],
        [0.4980, 0.5020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3341, 0.3325, 0.3334],
        [0.3342, 0.3324, 0.3334],
        [0.3342, 0.3325, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.022999048233032227s
iter time cost:17.97321081161499s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 32-------------

Evaluate global model
Averaged Train Loss: 3.0174
Averaged Test Accurancy: 0.2528
Std Test Accurancy: 0.0505
evaluate time cost:7.74576210975647s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.56491208076477s
attn_optimize time cost:0.0895543098449707s
weights:tensor([[0.1675, 0.1659, 0.1664, 0.1674, 0.1665, 0.1663],
        [0.1675, 0.1659, 0.1664, 0.1674, 0.1665, 0.1663],
        [0.1675, 0.1660, 0.1665, 0.1674, 0.1665, 0.1663],
        [0.1675, 0.1659, 0.1663, 0.1674, 0.1665, 0.1664],
        [0.1674, 0.1659, 0.1664, 0.1674, 0.1665, 0.1663],
        [0.1675, 0.1659, 0.1664, 0.1674, 0.1665, 0.1663]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3347, 0.3326, 0.3327],
        [0.3346, 0.3325, 0.3329],
        [0.3346, 0.3327, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5002, 0.4998],
        [0.5003, 0.4997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3327, 0.3340, 0.3333],
        [0.3327, 0.3340, 0.3333],
        [0.3326, 0.3340, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.06605839729309082s
iter time cost:17.567128896713257s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 33-------------

Evaluate global model
Averaged Train Loss: 2.8841
Averaged Test Accurancy: 0.2546
Std Test Accurancy: 0.0555
evaluate time cost:7.824614763259888s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.863349676132202s
attn_optimize time cost:0.0865631103515625s
weights:tensor([[0.1671, 0.1660, 0.1663, 0.1670, 0.1663, 0.1674],
        [0.1670, 0.1660, 0.1664, 0.1670, 0.1662, 0.1674],
        [0.1671, 0.1660, 0.1664, 0.1669, 0.1662, 0.1674],
        [0.1670, 0.1660, 0.1664, 0.1670, 0.1662, 0.1673],
        [0.1670, 0.1660, 0.1664, 0.1670, 0.1663, 0.1673],
        [0.1671, 0.1660, 0.1664, 0.1670, 0.1663, 0.1674]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3376, 0.3322, 0.3302],
        [0.3378, 0.3321, 0.3301],
        [0.3376, 0.3323, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4992, 0.5008],
        [0.4993, 0.5007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3340, 0.3314, 0.3346],
        [0.3339, 0.3315, 0.3345],
        [0.3340, 0.3314, 0.3346]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.06531214714050293s
iter time cost:17.94547986984253s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 34-------------

Evaluate global model
Averaged Train Loss: 2.5490
Averaged Test Accurancy: 0.2755
Std Test Accurancy: 0.0497
evaluate time cost:7.679808855056763s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.222288131713867s
attn_optimize time cost:0.09097075462341309s
weights:tensor([[0.1661, 0.1665, 0.1661, 0.1671, 0.1678, 0.1664],
        [0.1661, 0.1666, 0.1661, 0.1671, 0.1678, 0.1664],
        [0.1660, 0.1666, 0.1662, 0.1671, 0.1678, 0.1664],
        [0.1660, 0.1666, 0.1661, 0.1671, 0.1678, 0.1664],
        [0.1660, 0.1666, 0.1662, 0.1671, 0.1677, 0.1664],
        [0.1660, 0.1666, 0.1662, 0.1672, 0.1677, 0.1664]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3355, 0.3333, 0.3312],
        [0.3354, 0.3333, 0.3313],
        [0.3355, 0.3333, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5026, 0.4974],
        [0.5026, 0.4974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3328, 0.3348, 0.3324],
        [0.3328, 0.3348, 0.3324],
        [0.3327, 0.3348, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.07849454879760742s
iter time cost:18.17209482192993s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 35-------------

Evaluate global model
Averaged Train Loss: 2.5239
Averaged Test Accurancy: 0.2810
Std Test Accurancy: 0.0478
evaluate time cost:7.912005186080933s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.066180229187012s
attn_optimize time cost:0.08817243576049805s
weights:tensor([[0.1669, 0.1668, 0.1661, 0.1669, 0.1660, 0.1672],
        [0.1669, 0.1668, 0.1661, 0.1670, 0.1660, 0.1672],
        [0.1668, 0.1669, 0.1661, 0.1669, 0.1660, 0.1672],
        [0.1670, 0.1668, 0.1661, 0.1669, 0.1660, 0.1672],
        [0.1669, 0.1669, 0.1661, 0.1669, 0.1660, 0.1672],
        [0.1669, 0.1668, 0.1661, 0.1670, 0.1660, 0.1672]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3336, 0.3332, 0.3332],
        [0.3336, 0.3331, 0.3333],
        [0.3337, 0.3330, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4972, 0.5028],
        [0.4973, 0.5027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3332, 0.3324, 0.3344],
        [0.3331, 0.3325, 0.3344],
        [0.3332, 0.3325, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.030198097229003906s
iter time cost:18.196360111236572s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 36-------------

Evaluate global model
Averaged Train Loss: 2.8544
Averaged Test Accurancy: 0.2498
Std Test Accurancy: 0.0608
evaluate time cost:7.993981838226318s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.898058414459229s
attn_optimize time cost:0.08419251441955566s
weights:tensor([[0.1673, 0.1674, 0.1664, 0.1668, 0.1663, 0.1658],
        [0.1673, 0.1674, 0.1665, 0.1668, 0.1662, 0.1657],
        [0.1673, 0.1675, 0.1664, 0.1668, 0.1662, 0.1658],
        [0.1674, 0.1675, 0.1665, 0.1668, 0.1662, 0.1657],
        [0.1673, 0.1675, 0.1665, 0.1668, 0.1662, 0.1657],
        [0.1673, 0.1674, 0.1665, 0.1668, 0.1662, 0.1658]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3332, 0.3335, 0.3333],
        [0.3333, 0.3335, 0.3332],
        [0.3333, 0.3334, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5001, 0.4999],
        [0.5000, 0.5000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3333, 0.3327, 0.3340],
        [0.3334, 0.3327, 0.3339],
        [0.3333, 0.3327, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.05429267883300781s
iter time cost:18.14255952835083s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 37-------------

Evaluate global model
Averaged Train Loss: 2.5313
Averaged Test Accurancy: 0.2786
Std Test Accurancy: 0.0486
evaluate time cost:8.114727258682251s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.225659370422363s
attn_optimize time cost:0.10771346092224121s
weights:tensor([[0.1667, 0.1653, 0.1673, 0.1668, 0.1669, 0.1670],
        [0.1667, 0.1653, 0.1673, 0.1667, 0.1669, 0.1671],
        [0.1667, 0.1653, 0.1673, 0.1667, 0.1669, 0.1670],
        [0.1667, 0.1653, 0.1673, 0.1668, 0.1669, 0.1671],
        [0.1667, 0.1653, 0.1673, 0.1668, 0.1669, 0.1670],
        [0.1668, 0.1652, 0.1673, 0.1667, 0.1669, 0.1671]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3348, 0.3332, 0.3320],
        [0.3349, 0.3332, 0.3320],
        [0.3348, 0.3332, 0.3320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4980, 0.5020],
        [0.4979, 0.5021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3332, 0.3342, 0.3327],
        [0.3331, 0.3341, 0.3328],
        [0.3331, 0.3341, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.030048847198486328s
iter time cost:18.575992822647095s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 38-------------

Evaluate global model
Averaged Train Loss: 2.9523
Averaged Test Accurancy: 0.2559
Std Test Accurancy: 0.0461
evaluate time cost:8.19941234588623s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.214850664138794s
attn_optimize time cost:0.10259413719177246s
weights:tensor([[0.1669, 0.1673, 0.1650, 0.1676, 0.1670, 0.1663],
        [0.1668, 0.1673, 0.1650, 0.1676, 0.1670, 0.1663],
        [0.1669, 0.1673, 0.1650, 0.1675, 0.1670, 0.1663],
        [0.1668, 0.1673, 0.1650, 0.1676, 0.1671, 0.1663],
        [0.1669, 0.1673, 0.1650, 0.1675, 0.1670, 0.1663],
        [0.1669, 0.1673, 0.1650, 0.1675, 0.1670, 0.1663]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3350, 0.3340, 0.3310],
        [0.3348, 0.3342, 0.3309],
        [0.3349, 0.3341, 0.3310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4988, 0.5012],
        [0.4989, 0.5011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3326, 0.3342, 0.3333],
        [0.3325, 0.3343, 0.3333],
        [0.3326, 0.3342, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.025002241134643555s
iter time cost:17.59519076347351s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 39-------------

Evaluate global model
Averaged Train Loss: 2.6096
Averaged Test Accurancy: 0.2791
Std Test Accurancy: 0.0495
evaluate time cost:8.077909469604492s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.876927852630615s
attn_optimize time cost:0.11233353614807129s
weights:tensor([[0.1670, 0.1672, 0.1665, 0.1669, 0.1652, 0.1673],
        [0.1670, 0.1672, 0.1665, 0.1669, 0.1652, 0.1673],
        [0.1670, 0.1672, 0.1665, 0.1669, 0.1651, 0.1672],
        [0.1670, 0.1672, 0.1664, 0.1669, 0.1651, 0.1673],
        [0.1670, 0.1672, 0.1664, 0.1670, 0.1652, 0.1673],
        [0.1670, 0.1672, 0.1665, 0.1669, 0.1652, 0.1673]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3360, 0.3317, 0.3323],
        [0.3360, 0.3317, 0.3323],
        [0.3359, 0.3317, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5018, 0.4982],
        [0.5019, 0.4981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3334, 0.3334, 0.3332],
        [0.3334, 0.3334, 0.3332],
        [0.3334, 0.3334, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.029458284378051758s
iter time cost:17.142683267593384s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 40-------------

Evaluate global model
Averaged Train Loss: 2.6212
Averaged Test Accurancy: 0.2671
Std Test Accurancy: 0.0395
evaluate time cost:8.517788410186768s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.852519750595093s
attn_optimize time cost:0.11243915557861328s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.010305, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 283.42it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 566.84it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.1430, 0.1427, 0.1431, 0.1425, 0.1430, 0.1432, 0.1425],
        [0.1429, 0.1427, 0.1431, 0.1425, 0.1431, 0.1432, 0.1425],
        [0.1430, 0.1428, 0.1431, 0.1425, 0.1430, 0.1431, 0.1425],
        [0.1429, 0.1427, 0.1432, 0.1425, 0.1430, 0.1432, 0.1425],
        [0.1429, 0.1427, 0.1431, 0.1425, 0.1431, 0.1432, 0.1425],
        [0.1429, 0.1427, 0.1431, 0.1425, 0.1430, 0.1431, 0.1425],
        [0.1429, 0.1427, 0.1431, 0.1425, 0.1431, 0.1431, 0.1425]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2510, 0.2480, 0.2496, 0.2514],
        [0.2510, 0.2480, 0.2495, 0.2515],
        [0.2510, 0.2480, 0.2496, 0.2513],
        [0.2510, 0.2480, 0.2496, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4991, 0.5009],
        [0.4991, 0.5009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.011999130249023438s
iter time cost:17.588887929916382s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 41-------------

Evaluate global model
Averaged Train Loss: 2.5819
Averaged Test Accurancy: 0.2696
Std Test Accurancy: 0.0443
evaluate time cost:8.061092138290405s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.961592674255371s
attn_optimize time cost:0.12568950653076172s
weights:tensor([[0.1432, 0.1417, 0.1452, 0.1419, 0.1423, 0.1422, 0.1436],
        [0.1432, 0.1417, 0.1451, 0.1418, 0.1423, 0.1422, 0.1436],
        [0.1432, 0.1417, 0.1451, 0.1418, 0.1422, 0.1423, 0.1437],
        [0.1432, 0.1417, 0.1451, 0.1419, 0.1423, 0.1422, 0.1437],
        [0.1432, 0.1417, 0.1452, 0.1419, 0.1423, 0.1422, 0.1436],
        [0.1432, 0.1417, 0.1452, 0.1419, 0.1422, 0.1422, 0.1436],
        [0.1432, 0.1417, 0.1451, 0.1419, 0.1423, 0.1423, 0.1436]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2490, 0.2497, 0.2508, 0.2504],
        [0.2491, 0.2498, 0.2509, 0.2503],
        [0.2490, 0.2497, 0.2509, 0.2504],
        [0.2490, 0.2498, 0.2508, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5003, 0.4997],
        [0.5002, 0.4998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.0790719985961914s
iter time cost:18.32853627204895s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 42-------------

Evaluate global model
Averaged Train Loss: 2.5619
Averaged Test Accurancy: 0.2827
Std Test Accurancy: 0.0455
evaluate time cost:8.031741619110107s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.901217699050903s
attn_optimize time cost:0.12325072288513184s
weights:tensor([[0.1438, 0.1431, 0.1430, 0.1431, 0.1418, 0.1416, 0.1436],
        [0.1438, 0.1431, 0.1430, 0.1430, 0.1418, 0.1417, 0.1436],
        [0.1438, 0.1431, 0.1431, 0.1430, 0.1418, 0.1417, 0.1436],
        [0.1438, 0.1431, 0.1431, 0.1430, 0.1418, 0.1417, 0.1435],
        [0.1438, 0.1431, 0.1431, 0.1430, 0.1418, 0.1417, 0.1435],
        [0.1438, 0.1431, 0.1430, 0.1430, 0.1418, 0.1417, 0.1435],
        [0.1438, 0.1431, 0.1430, 0.1430, 0.1418, 0.1417, 0.1435]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2482, 0.2497, 0.2513, 0.2507],
        [0.2483, 0.2498, 0.2513, 0.2507],
        [0.2482, 0.2497, 0.2513, 0.2508],
        [0.2483, 0.2497, 0.2513, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4990, 0.5010],
        [0.4989, 0.5011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.023594379425048828s
iter time cost:18.163493156433105s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 43-------------

Evaluate global model
Averaged Train Loss: 2.3306
Averaged Test Accurancy: 0.2982
Std Test Accurancy: 0.0612
evaluate time cost:8.400791883468628s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.874482870101929s
attn_optimize time cost:0.12871003150939941s
weights:tensor([[0.1434, 0.1422, 0.1435, 0.1422, 0.1429, 0.1430, 0.1428],
        [0.1435, 0.1423, 0.1435, 0.1422, 0.1429, 0.1430, 0.1427],
        [0.1435, 0.1422, 0.1435, 0.1421, 0.1429, 0.1431, 0.1428],
        [0.1435, 0.1422, 0.1435, 0.1421, 0.1429, 0.1430, 0.1428],
        [0.1435, 0.1422, 0.1435, 0.1422, 0.1429, 0.1430, 0.1427],
        [0.1435, 0.1422, 0.1435, 0.1421, 0.1429, 0.1431, 0.1428],
        [0.1435, 0.1422, 0.1435, 0.1421, 0.1429, 0.1430, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2518, 0.2486, 0.2492, 0.2504],
        [0.2516, 0.2487, 0.2492, 0.2505],
        [0.2518, 0.2485, 0.2492, 0.2505],
        [0.2517, 0.2487, 0.2492, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4976, 0.5024],
        [0.4975, 0.5025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03187417984008789s
iter time cost:18.538084506988525s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 44-------------

Evaluate global model
Averaged Train Loss: 2.8388
Averaged Test Accurancy: 0.2557
Std Test Accurancy: 0.0493
evaluate time cost:8.211910724639893s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.82678747177124s
attn_optimize time cost:0.11330723762512207s
weights:tensor([[0.1434, 0.1425, 0.1422, 0.1427, 0.1428, 0.1436, 0.1427],
        [0.1434, 0.1425, 0.1422, 0.1427, 0.1428, 0.1436, 0.1427],
        [0.1434, 0.1425, 0.1423, 0.1428, 0.1428, 0.1436, 0.1427],
        [0.1434, 0.1425, 0.1422, 0.1428, 0.1429, 0.1436, 0.1426],
        [0.1434, 0.1425, 0.1422, 0.1428, 0.1429, 0.1435, 0.1427],
        [0.1434, 0.1425, 0.1422, 0.1428, 0.1429, 0.1436, 0.1427],
        [0.1434, 0.1425, 0.1423, 0.1428, 0.1429, 0.1435, 0.1426]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2492, 0.2507, 0.2504, 0.2497],
        [0.2491, 0.2508, 0.2505, 0.2497],
        [0.2491, 0.2507, 0.2504, 0.2498],
        [0.2491, 0.2508, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4996, 0.5004],
        [0.4995, 0.5005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.027993202209472656s
iter time cost:18.193100214004517s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 45-------------

Evaluate global model
Averaged Train Loss: 2.4879
Averaged Test Accurancy: 0.2812
Std Test Accurancy: 0.0538
evaluate time cost:8.750155925750732s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.799493312835693s
attn_optimize time cost:0.12375545501708984s
weights:tensor([[0.1429, 0.1436, 0.1432, 0.1423, 0.1431, 0.1421, 0.1429],
        [0.1428, 0.1436, 0.1432, 0.1423, 0.1431, 0.1420, 0.1429],
        [0.1429, 0.1436, 0.1432, 0.1424, 0.1431, 0.1420, 0.1429],
        [0.1428, 0.1435, 0.1432, 0.1423, 0.1431, 0.1421, 0.1430],
        [0.1429, 0.1435, 0.1432, 0.1423, 0.1431, 0.1420, 0.1429],
        [0.1428, 0.1436, 0.1432, 0.1424, 0.1431, 0.1420, 0.1429],
        [0.1429, 0.1435, 0.1432, 0.1423, 0.1431, 0.1421, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2504, 0.2496, 0.2505, 0.2496],
        [0.2504, 0.2496, 0.2504, 0.2496],
        [0.2503, 0.2496, 0.2505, 0.2496],
        [0.2503, 0.2496, 0.2505, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5002, 0.4998],
        [0.5003, 0.4997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.02799844741821289s
iter time cost:18.757843255996704s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 46-------------

Evaluate global model
Averaged Train Loss: 2.3591
Averaged Test Accurancy: 0.2968
Std Test Accurancy: 0.0523
evaluate time cost:8.345974206924438s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.496368408203125s
attn_optimize time cost:0.12693023681640625s
weights:tensor([[0.1434, 0.1426, 0.1427, 0.1422, 0.1424, 0.1431, 0.1435],
        [0.1435, 0.1426, 0.1427, 0.1422, 0.1424, 0.1431, 0.1434],
        [0.1434, 0.1426, 0.1427, 0.1422, 0.1424, 0.1432, 0.1434],
        [0.1434, 0.1427, 0.1426, 0.1422, 0.1424, 0.1432, 0.1435],
        [0.1435, 0.1426, 0.1427, 0.1422, 0.1424, 0.1432, 0.1434],
        [0.1434, 0.1426, 0.1427, 0.1422, 0.1424, 0.1432, 0.1434],
        [0.1434, 0.1427, 0.1427, 0.1422, 0.1424, 0.1432, 0.1435]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2493, 0.2493, 0.2503, 0.2511],
        [0.2493, 0.2494, 0.2503, 0.2510],
        [0.2494, 0.2493, 0.2503, 0.2511],
        [0.2494, 0.2492, 0.2503, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4999, 0.5001],
        [0.4999, 0.5001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.07908439636230469s
iter time cost:18.146361589431763s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 47-------------

Evaluate global model
Averaged Train Loss: 2.6174
Averaged Test Accurancy: 0.2774
Std Test Accurancy: 0.0495
evaluate time cost:8.100345611572266s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.619889736175537s
attn_optimize time cost:0.11538934707641602s
weights:tensor([[0.1435, 0.1434, 0.1426, 0.1424, 0.1419, 0.1430, 0.1431],
        [0.1436, 0.1434, 0.1425, 0.1424, 0.1419, 0.1430, 0.1431],
        [0.1436, 0.1434, 0.1425, 0.1424, 0.1419, 0.1430, 0.1432],
        [0.1435, 0.1434, 0.1425, 0.1424, 0.1419, 0.1430, 0.1432],
        [0.1435, 0.1434, 0.1426, 0.1423, 0.1420, 0.1430, 0.1431],
        [0.1436, 0.1435, 0.1425, 0.1424, 0.1419, 0.1430, 0.1431],
        [0.1435, 0.1434, 0.1425, 0.1424, 0.1420, 0.1430, 0.1431]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2500, 0.2522, 0.2485, 0.2493],
        [0.2501, 0.2520, 0.2486, 0.2493],
        [0.2501, 0.2521, 0.2486, 0.2492],
        [0.2500, 0.2522, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4992, 0.5008],
        [0.4991, 0.5009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.01900649070739746s
iter time cost:17.969409704208374s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 48-------------

Evaluate global model
Averaged Train Loss: 2.5169
Averaged Test Accurancy: 0.2688
Std Test Accurancy: 0.0424
evaluate time cost:8.275908946990967s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.696104288101196s
attn_optimize time cost:0.11728143692016602s
weights:tensor([[0.1429, 0.1431, 0.1448, 0.1424, 0.1423, 0.1422, 0.1422],
        [0.1429, 0.1431, 0.1447, 0.1424, 0.1423, 0.1423, 0.1422],
        [0.1429, 0.1431, 0.1447, 0.1424, 0.1423, 0.1423, 0.1422],
        [0.1429, 0.1431, 0.1448, 0.1424, 0.1423, 0.1422, 0.1422],
        [0.1429, 0.1431, 0.1448, 0.1424, 0.1423, 0.1423, 0.1422],
        [0.1429, 0.1431, 0.1448, 0.1424, 0.1423, 0.1423, 0.1423],
        [0.1429, 0.1430, 0.1448, 0.1424, 0.1423, 0.1423, 0.1422]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2488, 0.2490, 0.2500, 0.2522],
        [0.2489, 0.2491, 0.2500, 0.2521],
        [0.2488, 0.2490, 0.2501, 0.2522],
        [0.2488, 0.2489, 0.2501, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4984, 0.5016],
        [0.4983, 0.5017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.02039361000061035s
iter time cost:18.179436683654785s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 49-------------

Evaluate global model
Averaged Train Loss: 2.6020
Averaged Test Accurancy: 0.2719
Std Test Accurancy: 0.0604
evaluate time cost:8.210245370864868s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.266250371932983s
attn_optimize time cost:0.11246085166931152s
weights:tensor([[0.1432, 0.1421, 0.1444, 0.1431, 0.1417, 0.1428, 0.1427],
        [0.1432, 0.1421, 0.1443, 0.1431, 0.1417, 0.1428, 0.1427],
        [0.1432, 0.1422, 0.1444, 0.1431, 0.1417, 0.1428, 0.1427],
        [0.1432, 0.1421, 0.1444, 0.1431, 0.1416, 0.1428, 0.1427],
        [0.1432, 0.1421, 0.1444, 0.1432, 0.1417, 0.1428, 0.1427],
        [0.1432, 0.1421, 0.1444, 0.1431, 0.1417, 0.1428, 0.1427],
        [0.1432, 0.1421, 0.1444, 0.1432, 0.1417, 0.1428, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2507, 0.2479, 0.2517, 0.2498],
        [0.2507, 0.2479, 0.2516, 0.2497],
        [0.2507, 0.2479, 0.2516, 0.2498],
        [0.2507, 0.2478, 0.2517, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4971, 0.5029],
        [0.4970, 0.5030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03499889373779297s
iter time cost:18.73612403869629s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 50-------------

Evaluate global model
Averaged Train Loss: 2.6441
Averaged Test Accurancy: 0.2744
Std Test Accurancy: 0.0452
evaluate time cost:8.37913703918457s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.9126296043396s
attn_optimize time cost:0.12027192115783691s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.008351, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 334.90it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 530.35it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.4991, 0.5009],
        [0.4991, 0.5009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1430, 0.1430, 0.1419, 0.1431, 0.1436, 0.1432, 0.1422],
        [0.1430, 0.1430, 0.1419, 0.1431, 0.1436, 0.1432, 0.1422],
        [0.1430, 0.1430, 0.1419, 0.1430, 0.1436, 0.1433, 0.1421],
        [0.1430, 0.1430, 0.1419, 0.1431, 0.1436, 0.1433, 0.1422],
        [0.1430, 0.1430, 0.1419, 0.1431, 0.1436, 0.1433, 0.1421],
        [0.1429, 0.1430, 0.1419, 0.1430, 0.1436, 0.1433, 0.1423],
        [0.1429, 0.1430, 0.1420, 0.1431, 0.1436, 0.1433, 0.1422]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2503, 0.2488, 0.2502, 0.2507],
        [0.2503, 0.2488, 0.2501, 0.2508],
        [0.2503, 0.2488, 0.2501, 0.2507],
        [0.2503, 0.2488, 0.2501, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.011001825332641602s
iter time cost:19.563570737838745s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 51-------------

Evaluate global model
Averaged Train Loss: 2.4290
Averaged Test Accurancy: 0.2786
Std Test Accurancy: 0.0361
evaluate time cost:8.054972171783447s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.799236536026001s
attn_optimize time cost:0.08446168899536133s
weights:tensor([[0.5012, 0.4988],
        [0.5011, 0.4989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1437, 0.1423, 0.1438, 0.1427, 0.1430, 0.1424, 0.1422],
        [0.1437, 0.1423, 0.1437, 0.1427, 0.1431, 0.1423, 0.1422],
        [0.1438, 0.1422, 0.1437, 0.1427, 0.1430, 0.1424, 0.1422],
        [0.1437, 0.1422, 0.1438, 0.1427, 0.1430, 0.1423, 0.1422],
        [0.1438, 0.1423, 0.1437, 0.1427, 0.1431, 0.1423, 0.1422],
        [0.1438, 0.1423, 0.1437, 0.1426, 0.1430, 0.1424, 0.1423],
        [0.1437, 0.1423, 0.1438, 0.1427, 0.1430, 0.1422, 0.1422]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2515, 0.2499, 0.2503, 0.2482],
        [0.2515, 0.2499, 0.2503, 0.2482],
        [0.2515, 0.2500, 0.2503, 0.2482],
        [0.2516, 0.2500, 0.2503, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.05982255935668945s
iter time cost:18.141507148742676s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 52-------------

Evaluate global model
Averaged Train Loss: 2.5666
Averaged Test Accurancy: 0.2814
Std Test Accurancy: 0.0494
evaluate time cost:8.082839727401733s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.95067286491394s
attn_optimize time cost:0.09459900856018066s
weights:tensor([[0.4991, 0.5009],
        [0.4992, 0.5008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1426, 0.1441, 0.1420, 0.1442, 0.1430, 0.1419, 0.1423],
        [0.1426, 0.1441, 0.1420, 0.1441, 0.1430, 0.1418, 0.1423],
        [0.1426, 0.1441, 0.1419, 0.1442, 0.1430, 0.1419, 0.1422],
        [0.1427, 0.1441, 0.1419, 0.1442, 0.1430, 0.1419, 0.1422],
        [0.1427, 0.1441, 0.1419, 0.1442, 0.1430, 0.1419, 0.1422],
        [0.1426, 0.1441, 0.1419, 0.1442, 0.1430, 0.1419, 0.1422],
        [0.1426, 0.1441, 0.1421, 0.1442, 0.1430, 0.1418, 0.1422]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2497, 0.2494, 0.2498, 0.2511],
        [0.2497, 0.2494, 0.2497, 0.2511],
        [0.2497, 0.2494, 0.2497, 0.2512],
        [0.2497, 0.2494, 0.2498, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.11178708076477051s
iter time cost:18.369374752044678s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 53-------------

Evaluate global model
Averaged Train Loss: 2.3112
Averaged Test Accurancy: 0.3048
Std Test Accurancy: 0.0476
evaluate time cost:8.20519757270813s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.912732124328613s
attn_optimize time cost:0.01780104637145996s
weights:tensor([[0.4990, 0.5010],
        [0.4990, 0.5010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1421, 0.1425, 0.1433, 0.1436, 0.1436, 0.1433, 0.1417],
        [0.1421, 0.1426, 0.1433, 0.1436, 0.1436, 0.1432, 0.1417],
        [0.1421, 0.1426, 0.1433, 0.1436, 0.1436, 0.1432, 0.1416],
        [0.1421, 0.1426, 0.1433, 0.1436, 0.1436, 0.1433, 0.1416],
        [0.1421, 0.1426, 0.1433, 0.1436, 0.1436, 0.1433, 0.1416],
        [0.1420, 0.1426, 0.1433, 0.1436, 0.1436, 0.1432, 0.1417],
        [0.1420, 0.1426, 0.1433, 0.1436, 0.1436, 0.1432, 0.1416]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2504, 0.2493, 0.2508, 0.2495],
        [0.2503, 0.2494, 0.2507, 0.2495],
        [0.2504, 0.2494, 0.2507, 0.2495],
        [0.2504, 0.2494, 0.2508, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03649115562438965s
iter time cost:18.18517017364502s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 54-------------

Evaluate global model
Averaged Train Loss: 2.3493
Averaged Test Accurancy: 0.2869
Std Test Accurancy: 0.0539
evaluate time cost:7.885320425033569s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.753482103347778s
attn_optimize time cost:0.06370806694030762s
weights:tensor([[0.5000, 0.5000],
        [0.4999, 0.5001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1433, 0.1433, 0.1436, 0.1426, 0.1428, 0.1409, 0.1435],
        [0.1433, 0.1433, 0.1436, 0.1426, 0.1429, 0.1409, 0.1434],
        [0.1433, 0.1433, 0.1435, 0.1426, 0.1428, 0.1410, 0.1434],
        [0.1432, 0.1432, 0.1436, 0.1426, 0.1428, 0.1410, 0.1434],
        [0.1433, 0.1433, 0.1436, 0.1426, 0.1429, 0.1409, 0.1435],
        [0.1432, 0.1432, 0.1436, 0.1426, 0.1428, 0.1410, 0.1435],
        [0.1433, 0.1433, 0.1436, 0.1427, 0.1428, 0.1409, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2504, 0.2485, 0.2517, 0.2494],
        [0.2505, 0.2484, 0.2516, 0.2494],
        [0.2505, 0.2485, 0.2516, 0.2494],
        [0.2505, 0.2486, 0.2516, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.032999277114868164s
iter time cost:16.781242847442627s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 55-------------

Evaluate global model
Averaged Train Loss: 2.5434
Averaged Test Accurancy: 0.2813
Std Test Accurancy: 0.0541
evaluate time cost:7.510077238082886s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.504777431488037s
attn_optimize time cost:0.016997814178466797s
weights:tensor([[0.4985, 0.5015],
        [0.4985, 0.5015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1430, 0.1421, 0.1422, 0.1430, 0.1426, 0.1436, 0.1435],
        [0.1429, 0.1421, 0.1423, 0.1431, 0.1427, 0.1435, 0.1435],
        [0.1430, 0.1421, 0.1422, 0.1430, 0.1426, 0.1435, 0.1435],
        [0.1429, 0.1421, 0.1423, 0.1430, 0.1426, 0.1436, 0.1435],
        [0.1429, 0.1421, 0.1422, 0.1430, 0.1426, 0.1435, 0.1435],
        [0.1429, 0.1422, 0.1422, 0.1430, 0.1426, 0.1435, 0.1436],
        [0.1430, 0.1421, 0.1424, 0.1430, 0.1426, 0.1434, 0.1435]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2503, 0.2493, 0.2503, 0.2500],
        [0.2503, 0.2493, 0.2504, 0.2501],
        [0.2502, 0.2493, 0.2505, 0.2501],
        [0.2503, 0.2493, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03743720054626465s
iter time cost:16.07993984222412s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 56-------------

Evaluate global model
Averaged Train Loss: 2.3198
Averaged Test Accurancy: 0.2984
Std Test Accurancy: 0.0657
evaluate time cost:8.124275922775269s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.120935916900635s
attn_optimize time cost:0.019999980926513672s
weights:tensor([[0.5014, 0.4986],
        [0.5014, 0.4986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1429, 0.1434, 0.1428, 0.1434, 0.1427, 0.1417, 0.1432],
        [0.1429, 0.1434, 0.1429, 0.1434, 0.1427, 0.1417, 0.1432],
        [0.1429, 0.1434, 0.1429, 0.1434, 0.1427, 0.1417, 0.1431],
        [0.1429, 0.1433, 0.1428, 0.1434, 0.1427, 0.1418, 0.1431],
        [0.1429, 0.1434, 0.1429, 0.1434, 0.1427, 0.1417, 0.1431],
        [0.1429, 0.1433, 0.1429, 0.1433, 0.1426, 0.1418, 0.1431],
        [0.1428, 0.1434, 0.1429, 0.1434, 0.1427, 0.1417, 0.1432]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2511, 0.2493, 0.2499, 0.2497],
        [0.2512, 0.2493, 0.2498, 0.2497],
        [0.2511, 0.2493, 0.2499, 0.2497],
        [0.2512, 0.2492, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.034940242767333984s
iter time cost:17.310142040252686s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 57-------------

Evaluate global model
Averaged Train Loss: 2.6890
Averaged Test Accurancy: 0.2793
Std Test Accurancy: 0.0447
evaluate time cost:7.978682279586792s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.38297414779663s
attn_optimize time cost:0.06309175491333008s
weights:tensor([[0.4987, 0.5013],
        [0.4986, 0.5014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1433, 0.1432, 0.1407, 0.1431, 0.1431, 0.1428, 0.1438],
        [0.1433, 0.1432, 0.1406, 0.1431, 0.1431, 0.1428, 0.1438],
        [0.1433, 0.1432, 0.1407, 0.1431, 0.1431, 0.1428, 0.1438],
        [0.1433, 0.1432, 0.1406, 0.1431, 0.1431, 0.1428, 0.1439],
        [0.1433, 0.1432, 0.1406, 0.1431, 0.1432, 0.1428, 0.1439],
        [0.1433, 0.1432, 0.1407, 0.1431, 0.1431, 0.1428, 0.1438],
        [0.1433, 0.1432, 0.1407, 0.1431, 0.1432, 0.1428, 0.1438]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2496, 0.2505, 0.2488, 0.2511],
        [0.2496, 0.2504, 0.2488, 0.2511],
        [0.2495, 0.2505, 0.2489, 0.2512],
        [0.2496, 0.2505, 0.2488, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.10226774215698242s
iter time cost:17.657350063323975s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 58-------------

Evaluate global model
Averaged Train Loss: 2.5593
Averaged Test Accurancy: 0.2675
Std Test Accurancy: 0.0336
evaluate time cost:7.613094806671143s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.22787356376648s
attn_optimize time cost:0.07226371765136719s
weights:tensor([[0.5007, 0.4993],
        [0.5007, 0.4993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1431, 0.1433, 0.1425, 0.1426, 0.1436, 0.1426, 0.1423],
        [0.1432, 0.1433, 0.1425, 0.1425, 0.1436, 0.1426, 0.1423],
        [0.1431, 0.1433, 0.1425, 0.1426, 0.1436, 0.1426, 0.1422],
        [0.1431, 0.1433, 0.1425, 0.1425, 0.1436, 0.1426, 0.1423],
        [0.1431, 0.1433, 0.1425, 0.1425, 0.1436, 0.1427, 0.1422],
        [0.1431, 0.1433, 0.1426, 0.1425, 0.1436, 0.1427, 0.1423],
        [0.1431, 0.1434, 0.1425, 0.1425, 0.1436, 0.1428, 0.1423]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2500, 0.2505, 0.2503, 0.2492],
        [0.2500, 0.2505, 0.2504, 0.2491],
        [0.2500, 0.2505, 0.2503, 0.2492],
        [0.2501, 0.2506, 0.2503, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.14051127433776855s
iter time cost:17.178206205368042s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 59-------------

Evaluate global model
Averaged Train Loss: 2.4140
Averaged Test Accurancy: 0.2864
Std Test Accurancy: 0.0426
evaluate time cost:7.58780837059021s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.894657611846924s
attn_optimize time cost:0.0743551254272461s
weights:tensor([[0.4973, 0.5027],
        [0.4972, 0.5028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1429, 0.1433, 0.1426, 0.1434, 0.1428, 0.1422, 0.1428],
        [0.1429, 0.1434, 0.1426, 0.1434, 0.1428, 0.1422, 0.1428],
        [0.1429, 0.1434, 0.1426, 0.1434, 0.1428, 0.1422, 0.1428],
        [0.1429, 0.1433, 0.1426, 0.1435, 0.1427, 0.1421, 0.1429],
        [0.1429, 0.1433, 0.1425, 0.1434, 0.1428, 0.1422, 0.1428],
        [0.1429, 0.1433, 0.1426, 0.1434, 0.1428, 0.1423, 0.1428],
        [0.1429, 0.1433, 0.1426, 0.1434, 0.1428, 0.1422, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2511, 0.2490, 0.2508, 0.2491],
        [0.2511, 0.2490, 0.2508, 0.2492],
        [0.2511, 0.2489, 0.2508, 0.2492],
        [0.2511, 0.2489, 0.2508, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.07835125923156738s
iter time cost:16.760095357894897s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 60-------------

Evaluate global model
Averaged Train Loss: 2.2688
Averaged Test Accurancy: 0.2972
Std Test Accurancy: 0.0533
evaluate time cost:7.592485666275024s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.808260679244995s
attn_optimize time cost:0.06918740272521973s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.005714, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 492.69it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 651.29it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.1120, 0.1111, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105],
        [0.1120, 0.1111, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105],
        [0.1119, 0.1111, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105],
        [0.1120, 0.1110, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105],
        [0.1119, 0.1111, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105],
        [0.1119, 0.1110, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105],
        [0.1119, 0.1111, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105],
        [0.1119, 0.1111, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105],
        [0.1120, 0.1111, 0.1112, 0.1113, 0.1108, 0.1113, 0.1114, 0.1105, 0.1105]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3325, 0.3341, 0.3335],
        [0.3326, 0.3340, 0.3335],
        [0.3325, 0.3341, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.012215375900268555s
iter time cost:16.577911138534546s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 61-------------

Evaluate global model
Averaged Train Loss: 2.2686
Averaged Test Accurancy: 0.2908
Std Test Accurancy: 0.0344
evaluate time cost:7.696513414382935s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.528170347213745s
attn_optimize time cost:0.08672642707824707s
weights:tensor([[0.1101, 0.1105, 0.1109, 0.1118, 0.1118, 0.1112, 0.1109, 0.1112, 0.1115],
        [0.1101, 0.1106, 0.1109, 0.1118, 0.1118, 0.1112, 0.1110, 0.1112, 0.1115],
        [0.1101, 0.1105, 0.1109, 0.1118, 0.1118, 0.1112, 0.1109, 0.1112, 0.1115],
        [0.1101, 0.1106, 0.1109, 0.1118, 0.1118, 0.1112, 0.1109, 0.1112, 0.1116],
        [0.1101, 0.1106, 0.1109, 0.1118, 0.1118, 0.1112, 0.1109, 0.1112, 0.1115],
        [0.1101, 0.1106, 0.1109, 0.1118, 0.1118, 0.1112, 0.1109, 0.1112, 0.1115],
        [0.1101, 0.1106, 0.1109, 0.1118, 0.1118, 0.1111, 0.1110, 0.1112, 0.1115],
        [0.1101, 0.1106, 0.1109, 0.1118, 0.1118, 0.1112, 0.1109, 0.1112, 0.1115],
        [0.1101, 0.1105, 0.1109, 0.1118, 0.1118, 0.1112, 0.1110, 0.1112, 0.1115]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3315, 0.3358, 0.3327],
        [0.3316, 0.3357, 0.3327],
        [0.3315, 0.3357, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03470921516418457s
iter time cost:16.360117435455322s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 62-------------

Evaluate global model
Averaged Train Loss: 2.5015
Averaged Test Accurancy: 0.2848
Std Test Accurancy: 0.0606
evaluate time cost:7.7451698780059814s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.832595348358154s
attn_optimize time cost:0.13264060020446777s
weights:tensor([[0.1109, 0.1114, 0.1101, 0.1114, 0.1110, 0.1118, 0.1110, 0.1114, 0.1109],
        [0.1109, 0.1114, 0.1101, 0.1114, 0.1110, 0.1118, 0.1110, 0.1115, 0.1109],
        [0.1109, 0.1114, 0.1101, 0.1114, 0.1110, 0.1118, 0.1110, 0.1115, 0.1109],
        [0.1109, 0.1114, 0.1101, 0.1113, 0.1110, 0.1118, 0.1110, 0.1115, 0.1109],
        [0.1109, 0.1114, 0.1101, 0.1114, 0.1110, 0.1118, 0.1111, 0.1114, 0.1109],
        [0.1109, 0.1114, 0.1101, 0.1113, 0.1111, 0.1118, 0.1111, 0.1114, 0.1109],
        [0.1109, 0.1114, 0.1101, 0.1113, 0.1110, 0.1118, 0.1111, 0.1114, 0.1109],
        [0.1109, 0.1114, 0.1101, 0.1114, 0.1110, 0.1118, 0.1110, 0.1115, 0.1109],
        [0.1109, 0.1114, 0.1101, 0.1114, 0.1110, 0.1118, 0.1110, 0.1115, 0.1109]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3321, 0.3357, 0.3322],
        [0.3320, 0.3358, 0.3322],
        [0.3321, 0.3357, 0.3321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.022976160049438477s
iter time cost:16.770828247070312s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 63-------------

Evaluate global model
Averaged Train Loss: 2.4217
Averaged Test Accurancy: 0.2770
Std Test Accurancy: 0.0579
evaluate time cost:8.035264730453491s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.01564359664917s
attn_optimize time cost:0.12994384765625s
weights:tensor([[0.1106, 0.1106, 0.1115, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111],
        [0.1107, 0.1106, 0.1115, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111],
        [0.1107, 0.1106, 0.1115, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111],
        [0.1106, 0.1106, 0.1115, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111],
        [0.1107, 0.1106, 0.1115, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111],
        [0.1107, 0.1106, 0.1115, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111],
        [0.1107, 0.1106, 0.1114, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111],
        [0.1107, 0.1106, 0.1115, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111],
        [0.1106, 0.1106, 0.1115, 0.1113, 0.1119, 0.1109, 0.1109, 0.1112, 0.1111]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3316, 0.3339, 0.3344],
        [0.3317, 0.3339, 0.3344],
        [0.3316, 0.3339, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.07267999649047852s
iter time cost:17.38348364830017s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 64-------------

Evaluate global model
Averaged Train Loss: 2.8324
Averaged Test Accurancy: 0.2639
Std Test Accurancy: 0.0506
evaluate time cost:7.889327526092529s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.855688571929932s
attn_optimize time cost:0.1264803409576416s
weights:tensor([[0.1118, 0.1116, 0.1101, 0.1109, 0.1105, 0.1113, 0.1109, 0.1115, 0.1114],
        [0.1118, 0.1116, 0.1101, 0.1109, 0.1105, 0.1113, 0.1109, 0.1115, 0.1114],
        [0.1118, 0.1116, 0.1101, 0.1109, 0.1105, 0.1113, 0.1109, 0.1115, 0.1114],
        [0.1118, 0.1116, 0.1101, 0.1108, 0.1105, 0.1113, 0.1109, 0.1114, 0.1114],
        [0.1117, 0.1117, 0.1101, 0.1109, 0.1105, 0.1113, 0.1109, 0.1115, 0.1114],
        [0.1118, 0.1116, 0.1101, 0.1109, 0.1105, 0.1113, 0.1108, 0.1114, 0.1114],
        [0.1118, 0.1117, 0.1101, 0.1109, 0.1106, 0.1114, 0.1108, 0.1115, 0.1114],
        [0.1118, 0.1116, 0.1101, 0.1109, 0.1105, 0.1113, 0.1109, 0.1115, 0.1114],
        [0.1118, 0.1116, 0.1101, 0.1109, 0.1105, 0.1113, 0.1109, 0.1114, 0.1114]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3342, 0.3320, 0.3338],
        [0.3342, 0.3320, 0.3338],
        [0.3342, 0.3319, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.0240018367767334s
iter time cost:17.95132827758789s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 65-------------

Evaluate global model
Averaged Train Loss: 2.2603
Averaged Test Accurancy: 0.2917
Std Test Accurancy: 0.0296
evaluate time cost:7.873702764511108s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.543147087097168s
attn_optimize time cost:0.13744640350341797s
weights:tensor([[0.1109, 0.1106, 0.1117, 0.1111, 0.1114, 0.1106, 0.1106, 0.1115, 0.1115],
        [0.1109, 0.1106, 0.1117, 0.1111, 0.1114, 0.1106, 0.1106, 0.1115, 0.1115],
        [0.1109, 0.1105, 0.1118, 0.1112, 0.1114, 0.1106, 0.1106, 0.1115, 0.1116],
        [0.1109, 0.1106, 0.1118, 0.1112, 0.1114, 0.1106, 0.1106, 0.1115, 0.1115],
        [0.1109, 0.1106, 0.1117, 0.1111, 0.1114, 0.1106, 0.1106, 0.1115, 0.1115],
        [0.1109, 0.1105, 0.1117, 0.1111, 0.1115, 0.1106, 0.1106, 0.1115, 0.1116],
        [0.1109, 0.1105, 0.1117, 0.1111, 0.1114, 0.1106, 0.1106, 0.1115, 0.1116],
        [0.1109, 0.1106, 0.1117, 0.1111, 0.1114, 0.1106, 0.1106, 0.1115, 0.1115],
        [0.1109, 0.1106, 0.1118, 0.1111, 0.1114, 0.1106, 0.1106, 0.1115, 0.1115]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3333, 0.3343, 0.3324],
        [0.3331, 0.3344, 0.3324],
        [0.3332, 0.3342, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.05093240737915039s
iter time cost:17.71925115585327s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 66-------------

Evaluate global model
Averaged Train Loss: 2.3549
Averaged Test Accurancy: 0.2914
Std Test Accurancy: 0.0479
evaluate time cost:7.639847993850708s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.766801595687866s
attn_optimize time cost:0.13046598434448242s
weights:tensor([[0.1117, 0.1101, 0.1107, 0.1114, 0.1113, 0.1110, 0.1109, 0.1119, 0.1110],
        [0.1118, 0.1101, 0.1107, 0.1113, 0.1113, 0.1110, 0.1109, 0.1118, 0.1110],
        [0.1118, 0.1101, 0.1107, 0.1114, 0.1113, 0.1110, 0.1109, 0.1118, 0.1110],
        [0.1117, 0.1101, 0.1107, 0.1114, 0.1113, 0.1110, 0.1109, 0.1119, 0.1109],
        [0.1118, 0.1101, 0.1108, 0.1114, 0.1113, 0.1110, 0.1109, 0.1119, 0.1110],
        [0.1117, 0.1101, 0.1107, 0.1114, 0.1113, 0.1110, 0.1109, 0.1119, 0.1110],
        [0.1118, 0.1101, 0.1107, 0.1113, 0.1113, 0.1110, 0.1109, 0.1119, 0.1110],
        [0.1118, 0.1101, 0.1107, 0.1113, 0.1113, 0.1110, 0.1109, 0.1119, 0.1110],
        [0.1118, 0.1101, 0.1108, 0.1114, 0.1113, 0.1110, 0.1109, 0.1118, 0.1110]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3320, 0.3356, 0.3324],
        [0.3320, 0.3356, 0.3324],
        [0.3319, 0.3358, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03161764144897461s
iter time cost:16.605839729309082s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 67-------------

Evaluate global model
Averaged Train Loss: 2.3911
Averaged Test Accurancy: 0.2870
Std Test Accurancy: 0.0454
evaluate time cost:7.830222129821777s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.91865849494934s
attn_optimize time cost:0.13397693634033203s
weights:tensor([[0.1120, 0.1111, 0.1112, 0.1124, 0.1105, 0.1115, 0.1101, 0.1108, 0.1104],
        [0.1120, 0.1111, 0.1112, 0.1124, 0.1104, 0.1116, 0.1101, 0.1108, 0.1104],
        [0.1120, 0.1111, 0.1112, 0.1124, 0.1104, 0.1116, 0.1101, 0.1108, 0.1104],
        [0.1120, 0.1111, 0.1112, 0.1124, 0.1105, 0.1115, 0.1101, 0.1108, 0.1104],
        [0.1120, 0.1111, 0.1112, 0.1124, 0.1105, 0.1115, 0.1101, 0.1108, 0.1104],
        [0.1120, 0.1111, 0.1112, 0.1124, 0.1105, 0.1115, 0.1101, 0.1108, 0.1104],
        [0.1120, 0.1111, 0.1112, 0.1123, 0.1105, 0.1115, 0.1102, 0.1108, 0.1104],
        [0.1120, 0.1111, 0.1112, 0.1124, 0.1104, 0.1116, 0.1101, 0.1108, 0.1104],
        [0.1120, 0.1111, 0.1112, 0.1124, 0.1104, 0.1115, 0.1101, 0.1108, 0.1104]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3342, 0.3318, 0.3340],
        [0.3342, 0.3317, 0.3341],
        [0.3342, 0.3318, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.028912067413330078s
iter time cost:16.95668387413025s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 68-------------

Evaluate global model
Averaged Train Loss: 2.0467
Averaged Test Accurancy: 0.3084
Std Test Accurancy: 0.0511
evaluate time cost:8.202612161636353s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.93629240989685s
attn_optimize time cost:0.13379907608032227s
weights:tensor([[0.1109, 0.1110, 0.1111, 0.1111, 0.1109, 0.1115, 0.1113, 0.1111, 0.1111],
        [0.1109, 0.1110, 0.1111, 0.1111, 0.1109, 0.1114, 0.1113, 0.1112, 0.1111],
        [0.1109, 0.1110, 0.1111, 0.1111, 0.1108, 0.1114, 0.1112, 0.1112, 0.1111],
        [0.1109, 0.1110, 0.1111, 0.1111, 0.1109, 0.1114, 0.1113, 0.1111, 0.1111],
        [0.1110, 0.1110, 0.1111, 0.1111, 0.1108, 0.1114, 0.1112, 0.1112, 0.1111],
        [0.1110, 0.1110, 0.1111, 0.1111, 0.1108, 0.1114, 0.1113, 0.1111, 0.1111],
        [0.1109, 0.1111, 0.1111, 0.1111, 0.1109, 0.1114, 0.1112, 0.1111, 0.1111],
        [0.1110, 0.1111, 0.1111, 0.1111, 0.1108, 0.1114, 0.1113, 0.1112, 0.1111],
        [0.1109, 0.1111, 0.1111, 0.1111, 0.1108, 0.1114, 0.1113, 0.1112, 0.1111]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3328, 0.3333, 0.3339],
        [0.3327, 0.3335, 0.3338],
        [0.3327, 0.3334, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.021590471267700195s
iter time cost:17.345918893814087s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 69-------------

Evaluate global model
Averaged Train Loss: 2.2746
Averaged Test Accurancy: 0.2993
Std Test Accurancy: 0.0460
evaluate time cost:7.956724166870117s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.82750940322876s
attn_optimize time cost:0.12579703330993652s
weights:tensor([[0.1117, 0.1106, 0.1105, 0.1103, 0.1110, 0.1117, 0.1112, 0.1111, 0.1117],
        [0.1118, 0.1107, 0.1105, 0.1103, 0.1110, 0.1117, 0.1112, 0.1111, 0.1117],
        [0.1118, 0.1107, 0.1105, 0.1103, 0.1109, 0.1117, 0.1112, 0.1111, 0.1117],
        [0.1118, 0.1106, 0.1105, 0.1104, 0.1110, 0.1117, 0.1112, 0.1111, 0.1117],
        [0.1118, 0.1106, 0.1105, 0.1104, 0.1110, 0.1117, 0.1112, 0.1112, 0.1117],
        [0.1118, 0.1107, 0.1105, 0.1104, 0.1110, 0.1117, 0.1112, 0.1111, 0.1117],
        [0.1118, 0.1107, 0.1105, 0.1104, 0.1109, 0.1117, 0.1112, 0.1111, 0.1117],
        [0.1118, 0.1107, 0.1105, 0.1103, 0.1110, 0.1117, 0.1112, 0.1111, 0.1117],
        [0.1118, 0.1107, 0.1105, 0.1104, 0.1110, 0.1117, 0.1112, 0.1112, 0.1117]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3320, 0.3336, 0.3344],
        [0.3319, 0.3336, 0.3345],
        [0.3320, 0.3335, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.10735797882080078s
iter time cost:18.145322561264038s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 70-------------

Evaluate global model
Averaged Train Loss: 2.3377
Averaged Test Accurancy: 0.2892
Std Test Accurancy: 0.0377
evaluate time cost:8.134324312210083s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.052441835403442s
attn_optimize time cost:0.13557767868041992s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.012284, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 332.75it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 665.50it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.5003, 0.4997],
        [0.5004, 0.4996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1435, 0.1434, 0.1414, 0.1432, 0.1429, 0.1424, 0.1432],
        [0.1435, 0.1434, 0.1414, 0.1432, 0.1429, 0.1424, 0.1431],
        [0.1436, 0.1434, 0.1414, 0.1432, 0.1429, 0.1424, 0.1431],
        [0.1436, 0.1434, 0.1414, 0.1432, 0.1429, 0.1424, 0.1431],
        [0.1436, 0.1434, 0.1414, 0.1432, 0.1429, 0.1424, 0.1431],
        [0.1435, 0.1434, 0.1414, 0.1432, 0.1429, 0.1424, 0.1432],
        [0.1435, 0.1434, 0.1414, 0.1433, 0.1429, 0.1424, 0.1432]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2503, 0.2486, 0.2488, 0.2523],
        [0.2503, 0.2486, 0.2489, 0.2522],
        [0.2503, 0.2485, 0.2488, 0.2524],
        [0.2503, 0.2485, 0.2489, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.013999700546264648s
iter time cost:18.55429720878601s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 71-------------

Evaluate global model
Averaged Train Loss: 2.2717
Averaged Test Accurancy: 0.2894
Std Test Accurancy: 0.0562
evaluate time cost:8.137701034545898s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.77064847946167s
attn_optimize time cost:0.07025694847106934s
weights:tensor([[0.5002, 0.4998],
        [0.5000, 0.5000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1435, 0.1419, 0.1435, 0.1423, 0.1426, 0.1431, 0.1431],
        [0.1435, 0.1418, 0.1435, 0.1423, 0.1425, 0.1431, 0.1432],
        [0.1434, 0.1418, 0.1435, 0.1423, 0.1426, 0.1432, 0.1431],
        [0.1435, 0.1418, 0.1435, 0.1423, 0.1426, 0.1431, 0.1432],
        [0.1434, 0.1419, 0.1435, 0.1423, 0.1426, 0.1431, 0.1431],
        [0.1435, 0.1419, 0.1435, 0.1423, 0.1426, 0.1431, 0.1431],
        [0.1434, 0.1418, 0.1435, 0.1423, 0.1426, 0.1432, 0.1432]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2493, 0.2510, 0.2494, 0.2503],
        [0.2493, 0.2509, 0.2494, 0.2503],
        [0.2492, 0.2509, 0.2494, 0.2505],
        [0.2492, 0.2509, 0.2494, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.07561326026916504s
iter time cost:18.179868459701538s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 72-------------

Evaluate global model
Averaged Train Loss: 2.2915
Averaged Test Accurancy: 0.2936
Std Test Accurancy: 0.0513
evaluate time cost:7.94047737121582s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.679689168930054s
attn_optimize time cost:0.07020926475524902s
weights:tensor([[0.5000, 0.5000],
        [0.5000, 0.5000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1435, 0.1420, 0.1428, 0.1426, 0.1429, 0.1436, 0.1426],
        [0.1435, 0.1420, 0.1428, 0.1426, 0.1429, 0.1436, 0.1427],
        [0.1435, 0.1420, 0.1428, 0.1426, 0.1429, 0.1436, 0.1426],
        [0.1435, 0.1420, 0.1428, 0.1426, 0.1429, 0.1436, 0.1426],
        [0.1435, 0.1420, 0.1427, 0.1427, 0.1429, 0.1436, 0.1427],
        [0.1435, 0.1420, 0.1428, 0.1426, 0.1429, 0.1436, 0.1427],
        [0.1434, 0.1419, 0.1428, 0.1427, 0.1429, 0.1435, 0.1426]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2508, 0.2506, 0.2505, 0.2480],
        [0.2508, 0.2507, 0.2504, 0.2481],
        [0.2508, 0.2507, 0.2505, 0.2479],
        [0.2508, 0.2508, 0.2503, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.02507781982421875s
iter time cost:17.765825033187866s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 73-------------

Evaluate global model
Averaged Train Loss: 2.2888
Averaged Test Accurancy: 0.2901
Std Test Accurancy: 0.0486
evaluate time cost:7.71633243560791s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.759215116500854s
attn_optimize time cost:0.0627291202545166s
weights:tensor([[0.5025, 0.4975],
        [0.5026, 0.4974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1428, 0.1423, 0.1427, 0.1429, 0.1428, 0.1434, 0.1431],
        [0.1428, 0.1423, 0.1427, 0.1429, 0.1428, 0.1434, 0.1431],
        [0.1428, 0.1424, 0.1426, 0.1429, 0.1428, 0.1435, 0.1431],
        [0.1428, 0.1423, 0.1426, 0.1429, 0.1428, 0.1434, 0.1431],
        [0.1428, 0.1423, 0.1427, 0.1429, 0.1428, 0.1435, 0.1431],
        [0.1428, 0.1423, 0.1427, 0.1429, 0.1428, 0.1434, 0.1431],
        [0.1429, 0.1424, 0.1426, 0.1428, 0.1427, 0.1435, 0.1431]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2501, 0.2479, 0.2487, 0.2533],
        [0.2502, 0.2479, 0.2487, 0.2533],
        [0.2502, 0.2479, 0.2487, 0.2533],
        [0.2502, 0.2478, 0.2487, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.07417678833007812s
iter time cost:17.740780353546143s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 74-------------

Evaluate global model
Averaged Train Loss: 2.2979
Averaged Test Accurancy: 0.2853
Std Test Accurancy: 0.0580
evaluate time cost:7.640641212463379s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.977303504943848s
attn_optimize time cost:0.0711052417755127s
weights:tensor([[0.4979, 0.5021],
        [0.4980, 0.5020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1430, 0.1427, 0.1423, 0.1430, 0.1429, 0.1429, 0.1432],
        [0.1430, 0.1427, 0.1423, 0.1431, 0.1430, 0.1428, 0.1431],
        [0.1430, 0.1427, 0.1423, 0.1431, 0.1429, 0.1429, 0.1432],
        [0.1430, 0.1427, 0.1423, 0.1430, 0.1429, 0.1429, 0.1432],
        [0.1430, 0.1427, 0.1423, 0.1430, 0.1429, 0.1428, 0.1432],
        [0.1430, 0.1427, 0.1423, 0.1431, 0.1429, 0.1429, 0.1432],
        [0.1429, 0.1426, 0.1423, 0.1431, 0.1429, 0.1429, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2499, 0.2481, 0.2500, 0.2521],
        [0.2498, 0.2481, 0.2500, 0.2521],
        [0.2499, 0.2481, 0.2500, 0.2519],
        [0.2498, 0.2481, 0.2500, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.1538081169128418s
iter time cost:17.970659732818604s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 75-------------

Evaluate global model
Averaged Train Loss: 2.2618
Averaged Test Accurancy: 0.2919
Std Test Accurancy: 0.0341
evaluate time cost:7.61406946182251s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.988558530807495s
attn_optimize time cost:0.06863713264465332s
weights:tensor([[0.4979, 0.5021],
        [0.4981, 0.5019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1439, 0.1425, 0.1422, 0.1417, 0.1438, 0.1432, 0.1427],
        [0.1439, 0.1425, 0.1423, 0.1416, 0.1438, 0.1432, 0.1427],
        [0.1439, 0.1425, 0.1423, 0.1416, 0.1438, 0.1432, 0.1427],
        [0.1439, 0.1425, 0.1423, 0.1416, 0.1438, 0.1432, 0.1427],
        [0.1439, 0.1424, 0.1423, 0.1416, 0.1438, 0.1432, 0.1428],
        [0.1438, 0.1425, 0.1422, 0.1417, 0.1438, 0.1432, 0.1427],
        [0.1438, 0.1424, 0.1422, 0.1417, 0.1438, 0.1432, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2504, 0.2523, 0.2489, 0.2484],
        [0.2505, 0.2524, 0.2489, 0.2483],
        [0.2503, 0.2524, 0.2489, 0.2483],
        [0.2505, 0.2525, 0.2489, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.018563270568847656s
iter time cost:17.786628246307373s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 76-------------

Evaluate global model
Averaged Train Loss: 2.4675
Averaged Test Accurancy: 0.2816
Std Test Accurancy: 0.0454
evaluate time cost:7.84752893447876s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.853961706161499s
attn_optimize time cost:0.0618138313293457s
weights:tensor([[0.5012, 0.4988],
        [0.5014, 0.4986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1425, 0.1426, 0.1421, 0.1438, 0.1435, 0.1433, 0.1423],
        [0.1425, 0.1425, 0.1421, 0.1438, 0.1435, 0.1433, 0.1423],
        [0.1425, 0.1425, 0.1421, 0.1438, 0.1435, 0.1432, 0.1423],
        [0.1426, 0.1425, 0.1421, 0.1438, 0.1435, 0.1433, 0.1423],
        [0.1426, 0.1425, 0.1421, 0.1438, 0.1435, 0.1432, 0.1423],
        [0.1425, 0.1425, 0.1421, 0.1438, 0.1435, 0.1433, 0.1423],
        [0.1424, 0.1426, 0.1421, 0.1439, 0.1436, 0.1433, 0.1422]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2489, 0.2503, 0.2507, 0.2500],
        [0.2489, 0.2504, 0.2506, 0.2502],
        [0.2488, 0.2503, 0.2508, 0.2501],
        [0.2488, 0.2503, 0.2507, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.08451294898986816s
iter time cost:17.976444959640503s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 77-------------

Evaluate global model
Averaged Train Loss: 2.1527
Averaged Test Accurancy: 0.2996
Std Test Accurancy: 0.0574
evaluate time cost:7.990899085998535s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.718543767929077s
attn_optimize time cost:0.06506729125976562s
weights:tensor([[0.5010, 0.4990],
        [0.5007, 0.4993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1426, 0.1423, 0.1430, 0.1430, 0.1431, 0.1438, 0.1422],
        [0.1427, 0.1423, 0.1430, 0.1430, 0.1431, 0.1438, 0.1422],
        [0.1427, 0.1423, 0.1430, 0.1430, 0.1431, 0.1437, 0.1422],
        [0.1426, 0.1423, 0.1430, 0.1430, 0.1431, 0.1438, 0.1422],
        [0.1427, 0.1422, 0.1430, 0.1430, 0.1431, 0.1438, 0.1422],
        [0.1427, 0.1423, 0.1430, 0.1430, 0.1431, 0.1438, 0.1422],
        [0.1426, 0.1422, 0.1430, 0.1430, 0.1431, 0.1438, 0.1422]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2480, 0.2488, 0.2498, 0.2535],
        [0.2479, 0.2488, 0.2498, 0.2534],
        [0.2479, 0.2487, 0.2498, 0.2536],
        [0.2480, 0.2488, 0.2498, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.05713462829589844s
iter time cost:17.958722829818726s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 78-------------

Evaluate global model
Averaged Train Loss: 2.3809
Averaged Test Accurancy: 0.2873
Std Test Accurancy: 0.0490
evaluate time cost:7.733275890350342s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.746112585067749s
attn_optimize time cost:0.07016944885253906s
weights:tensor([[0.5005, 0.4995],
        [0.5005, 0.4995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1435, 0.1426, 0.1432, 0.1427, 0.1427, 0.1419, 0.1434],
        [0.1435, 0.1426, 0.1432, 0.1427, 0.1427, 0.1419, 0.1434],
        [0.1435, 0.1426, 0.1432, 0.1427, 0.1427, 0.1419, 0.1434],
        [0.1435, 0.1426, 0.1432, 0.1428, 0.1428, 0.1419, 0.1433],
        [0.1435, 0.1426, 0.1432, 0.1427, 0.1427, 0.1419, 0.1433],
        [0.1435, 0.1426, 0.1432, 0.1427, 0.1428, 0.1419, 0.1433],
        [0.1435, 0.1426, 0.1432, 0.1427, 0.1428, 0.1419, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2501, 0.2500, 0.2487, 0.2513],
        [0.2501, 0.2501, 0.2486, 0.2512],
        [0.2501, 0.2499, 0.2487, 0.2513],
        [0.2500, 0.2501, 0.2487, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.06804299354553223s
iter time cost:17.74254083633423s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 79-------------

Evaluate global model
Averaged Train Loss: 2.4232
Averaged Test Accurancy: 0.2828
Std Test Accurancy: 0.0568
evaluate time cost:7.757381200790405s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.767099142074585s
attn_optimize time cost:0.06434011459350586s
weights:tensor([[0.4980, 0.5020],
        [0.4981, 0.5019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1430, 0.1436, 0.1409, 0.1433, 0.1426, 0.1436, 0.1429],
        [0.1430, 0.1436, 0.1409, 0.1433, 0.1426, 0.1436, 0.1429],
        [0.1430, 0.1436, 0.1409, 0.1432, 0.1426, 0.1436, 0.1430],
        [0.1430, 0.1436, 0.1409, 0.1433, 0.1427, 0.1436, 0.1430],
        [0.1430, 0.1436, 0.1409, 0.1433, 0.1426, 0.1436, 0.1429],
        [0.1430, 0.1437, 0.1409, 0.1433, 0.1426, 0.1436, 0.1429],
        [0.1430, 0.1436, 0.1408, 0.1433, 0.1427, 0.1437, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.2490, 0.2502, 0.2495, 0.2514],
        [0.2490, 0.2503, 0.2495, 0.2513],
        [0.2490, 0.2502, 0.2494, 0.2514],
        [0.2489, 0.2501, 0.2495, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.06858158111572266s
iter time cost:17.787673711776733s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 80-------------

Evaluate global model
Averaged Train Loss: 2.4489
Averaged Test Accurancy: 0.2819
Std Test Accurancy: 0.0467
evaluate time cost:7.6175031661987305s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.688756942749023s
attn_optimize time cost:0.0702817440032959s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.043403, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 333.38it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 666.77it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.2009, 0.1994, 0.2004, 0.1994, 0.1998],
        [0.2009, 0.1995, 0.2004, 0.1994, 0.1998],
        [0.2009, 0.1995, 0.2004, 0.1994, 0.1998],
        [0.2009, 0.1995, 0.2003, 0.1994, 0.1998],
        [0.2009, 0.1995, 0.2004, 0.1994, 0.1998]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5003, 0.4997],
        [0.5005, 0.4995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4995, 0.5005],
        [0.4994, 0.5006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5025, 0.4975],
        [0.5024, 0.4976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3339, 0.3325, 0.3336],
        [0.3338, 0.3325, 0.3337],
        [0.3339, 0.3326, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4994, 0.5006],
        [0.4994, 0.5006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.012000322341918945s
iter time cost:17.549250602722168s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 81-------------

Evaluate global model
Averaged Train Loss: 2.4967
Averaged Test Accurancy: 0.2867
Std Test Accurancy: 0.0507
evaluate time cost:8.056779861450195s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.099307775497437s
attn_optimize time cost:0.10943818092346191s
weights:tensor([[0.1984, 0.2000, 0.2012, 0.1990, 0.2015],
        [0.1984, 0.1999, 0.2012, 0.1989, 0.2015],
        [0.1984, 0.1999, 0.2013, 0.1989, 0.2016],
        [0.1984, 0.1999, 0.2013, 0.1989, 0.2015],
        [0.1985, 0.1999, 0.2012, 0.1989, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4999, 0.5001],
        [0.4999, 0.5001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5003, 0.4997],
        [0.5001, 0.4999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4980, 0.5020],
        [0.4982, 0.5018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3324, 0.3341, 0.3336],
        [0.3324, 0.3341, 0.3335],
        [0.3325, 0.3340, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5001, 0.4999],
        [0.5001, 0.4999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.029938220977783203s
iter time cost:18.35297679901123s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 82-------------

Evaluate global model
Averaged Train Loss: 2.0068
Averaged Test Accurancy: 0.3034
Std Test Accurancy: 0.0432
evaluate time cost:7.800685405731201s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.056072235107422s
attn_optimize time cost:0.09707522392272949s
weights:tensor([[0.2003, 0.1993, 0.1991, 0.2000, 0.2013],
        [0.2003, 0.1993, 0.1991, 0.2000, 0.2013],
        [0.2003, 0.1993, 0.1991, 0.2000, 0.2013],
        [0.2003, 0.1993, 0.1991, 0.2000, 0.2013],
        [0.2003, 0.1992, 0.1990, 0.2001, 0.2013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5007, 0.4993],
        [0.5007, 0.4993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5007, 0.4993],
        [0.5009, 0.4991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4960, 0.5040],
        [0.4960, 0.5040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3341, 0.3338, 0.3321],
        [0.3341, 0.3338, 0.3321],
        [0.3342, 0.3336, 0.3322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4991, 0.5009],
        [0.4991, 0.5009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.1328134536743164s
iter time cost:18.16628646850586s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 83-------------

Evaluate global model
Averaged Train Loss: 2.3702
Averaged Test Accurancy: 0.2896
Std Test Accurancy: 0.0479
evaluate time cost:8.141208410263062s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.047866582870483s
attn_optimize time cost:0.09886288642883301s
weights:tensor([[0.1984, 0.2002, 0.1998, 0.1989, 0.2027],
        [0.1984, 0.2002, 0.1999, 0.1989, 0.2026],
        [0.1985, 0.2002, 0.2000, 0.1989, 0.2024],
        [0.1984, 0.2002, 0.1999, 0.1989, 0.2026],
        [0.1985, 0.2002, 0.2000, 0.1990, 0.2023]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4969, 0.5031],
        [0.4970, 0.5030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4999, 0.5001],
        [0.4999, 0.5001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4969, 0.5031],
        [0.4969, 0.5031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3316, 0.3335, 0.3348],
        [0.3314, 0.3337, 0.3349],
        [0.3315, 0.3337, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4985, 0.5015],
        [0.4983, 0.5017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.020434856414794922s
iter time cost:18.375975370407104s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 84-------------

Evaluate global model
Averaged Train Loss: 2.1625
Averaged Test Accurancy: 0.2980
Std Test Accurancy: 0.0411
evaluate time cost:8.28388524055481s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.250105619430542s
attn_optimize time cost:0.018002748489379883s
weights:tensor([[0.1991, 0.2005, 0.2009, 0.2004, 0.1991],
        [0.1991, 0.2005, 0.2009, 0.2003, 0.1991],
        [0.1991, 0.2005, 0.2009, 0.2003, 0.1992],
        [0.1991, 0.2005, 0.2009, 0.2004, 0.1991],
        [0.1992, 0.2004, 0.2009, 0.2004, 0.1992]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4985, 0.5015],
        [0.4986, 0.5014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4975, 0.5025],
        [0.4975, 0.5025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4975, 0.5025],
        [0.4975, 0.5025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3341, 0.3332, 0.3327],
        [0.3341, 0.3332, 0.3327],
        [0.3340, 0.3333, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4965, 0.5035],
        [0.4964, 0.5036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03127622604370117s
iter time cost:17.593255043029785s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 85-------------

Evaluate global model
Averaged Train Loss: 2.1617
Averaged Test Accurancy: 0.2835
Std Test Accurancy: 0.0411
evaluate time cost:7.9736433029174805s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.333486557006836s
attn_optimize time cost:0.10731935501098633s
weights:tensor([[0.2009, 0.1995, 0.2002, 0.2009, 0.1984],
        [0.2009, 0.1995, 0.2002, 0.2009, 0.1984],
        [0.2010, 0.1995, 0.2002, 0.2009, 0.1984],
        [0.2010, 0.1995, 0.2002, 0.2009, 0.1984],
        [0.2010, 0.1996, 0.2003, 0.2009, 0.1983]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5002, 0.4998],
        [0.5002, 0.4998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5015, 0.4985],
        [0.5017, 0.4983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4982, 0.5018],
        [0.4982, 0.5018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3343, 0.3316, 0.3341],
        [0.3345, 0.3312, 0.3343],
        [0.3343, 0.3316, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5007, 0.4993],
        [0.5008, 0.4992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.026751995086669922s
iter time cost:17.50899577140808s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 86-------------

Evaluate global model
Averaged Train Loss: 2.2607
Averaged Test Accurancy: 0.2888
Std Test Accurancy: 0.0439
evaluate time cost:7.641142129898071s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.032060384750366s
attn_optimize time cost:0.10808444023132324s
weights:tensor([[0.2008, 0.2007, 0.1994, 0.2001, 0.1989],
        [0.2008, 0.2007, 0.1994, 0.2002, 0.1989],
        [0.2008, 0.2006, 0.1995, 0.2002, 0.1989],
        [0.2008, 0.2007, 0.1994, 0.2002, 0.1989],
        [0.2008, 0.2007, 0.1995, 0.2001, 0.1988]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5004, 0.4996],
        [0.5004, 0.4996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4995, 0.5005],
        [0.4993, 0.5007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4996, 0.5004],
        [0.4997, 0.5003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3346, 0.3299, 0.3355],
        [0.3346, 0.3298, 0.3355],
        [0.3347, 0.3297, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5014, 0.4986],
        [0.5015, 0.4985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.10825848579406738s
iter time cost:16.958519458770752s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 87-------------

Evaluate global model
Averaged Train Loss: 2.2327
Averaged Test Accurancy: 0.2880
Std Test Accurancy: 0.0289
evaluate time cost:7.51933479309082s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.405430793762207s
attn_optimize time cost:0.01699995994567871s
weights:tensor([[0.1984, 0.2004, 0.2004, 0.2002, 0.2007],
        [0.1984, 0.2003, 0.2003, 0.2002, 0.2007],
        [0.1984, 0.2004, 0.2003, 0.2002, 0.2008],
        [0.1984, 0.2004, 0.2003, 0.2002, 0.2007],
        [0.1984, 0.2003, 0.2003, 0.2003, 0.2007]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5007, 0.4993],
        [0.5007, 0.4993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5026, 0.4974],
        [0.5025, 0.4975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4999, 0.5001],
        [0.4997, 0.5003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3325, 0.3340, 0.3334],
        [0.3323, 0.3342, 0.3335],
        [0.3324, 0.3341, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4996, 0.5004],
        [0.4995, 0.5005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.030328750610351562s
iter time cost:16.9853298664093s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 88-------------

Evaluate global model
Averaged Train Loss: 2.2858
Averaged Test Accurancy: 0.2937
Std Test Accurancy: 0.0553
evaluate time cost:8.312358379364014s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:10.020383596420288s
attn_optimize time cost:0.09574484825134277s
weights:tensor([[0.2009, 0.2000, 0.1993, 0.1999, 0.1999],
        [0.2008, 0.2000, 0.1993, 0.1999, 0.1999],
        [0.2009, 0.2000, 0.1993, 0.1999, 0.1999],
        [0.2008, 0.2000, 0.1993, 0.1999, 0.2000],
        [0.2008, 0.2001, 0.1992, 0.1999, 0.2000]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5013, 0.4987],
        [0.5013, 0.4987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4985, 0.5015],
        [0.4984, 0.5016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4978, 0.5022],
        [0.4977, 0.5023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3336, 0.3309, 0.3356],
        [0.3336, 0.3307, 0.3357],
        [0.3336, 0.3307, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4995, 0.5005],
        [0.4995, 0.5005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.02759861946105957s
iter time cost:18.538390398025513s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 89-------------

Evaluate global model
Averaged Train Loss: 2.1882
Averaged Test Accurancy: 0.2894
Std Test Accurancy: 0.0485
evaluate time cost:7.611740589141846s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.860531568527222s
attn_optimize time cost:0.10297751426696777s
weights:tensor([[0.2002, 0.2006, 0.2002, 0.1995, 0.1995],
        [0.2002, 0.2006, 0.2001, 0.1995, 0.1996],
        [0.2002, 0.2006, 0.2001, 0.1995, 0.1996],
        [0.2001, 0.2006, 0.2001, 0.1995, 0.1996],
        [0.2003, 0.2007, 0.2002, 0.1994, 0.1995]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5011, 0.4989],
        [0.5011, 0.4989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4998, 0.5002],
        [0.4997, 0.5003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4997, 0.5003],
        [0.4996, 0.5004]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.3341, 0.3321, 0.3338],
        [0.3341, 0.3320, 0.3339],
        [0.3341, 0.3321, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4993, 0.5007],
        [0.4992, 0.5008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.01955890655517578s
iter time cost:16.603818655014038s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 90-------------

Evaluate global model
Averaged Train Loss: 2.5064
Averaged Test Accurancy: 0.2743
Std Test Accurancy: 0.0488
evaluate time cost:8.458420753479004s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.518192291259766s
attn_optimize time cost:0.09886789321899414s
running k-means on cuda..
[running kmeans]: 0it [00:00, ?it/s]device is :cuda
[running kmeans]: 0it [00:00, ?it/s, center_shift=0.021397, iteration=1, tol=0.000100]device is :cuda
[running kmeans]: 1it [00:00, 249.97it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 499.95it/s, center_shift=0.000000, iteration=2, tol=0.000100]
weights:tensor([[0.4992, 0.5008],
        [0.4992, 0.5008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5001, 0.4999],
        [0.5001, 0.4999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4987, 0.5013],
        [0.4987, 0.5013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1247, 0.1253, 0.1242, 0.1253, 0.1252, 0.1249, 0.1250, 0.1255],
        [0.1246, 0.1252, 0.1243, 0.1253, 0.1252, 0.1250, 0.1250, 0.1255],
        [0.1247, 0.1253, 0.1242, 0.1253, 0.1252, 0.1249, 0.1249, 0.1255],
        [0.1247, 0.1254, 0.1242, 0.1253, 0.1252, 0.1249, 0.1249, 0.1255],
        [0.1247, 0.1253, 0.1243, 0.1253, 0.1252, 0.1249, 0.1249, 0.1254],
        [0.1247, 0.1253, 0.1243, 0.1253, 0.1252, 0.1249, 0.1249, 0.1255],
        [0.1247, 0.1253, 0.1242, 0.1253, 0.1252, 0.1249, 0.1250, 0.1255],
        [0.1247, 0.1253, 0.1242, 0.1253, 0.1252, 0.1249, 0.1249, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.012173652648925781s
iter time cost:17.160925149917603s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 91-------------

Evaluate global model
Averaged Train Loss: 2.0565
Averaged Test Accurancy: 0.2994
Std Test Accurancy: 0.0419
evaluate time cost:7.601529598236084s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.932195663452148s
attn_optimize time cost:0.07331562042236328s
weights:tensor([[0.5041, 0.4959],
        [0.5040, 0.4960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4986, 0.5014],
        [0.4987, 0.5013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5030, 0.4970],
        [0.5032, 0.4968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1248, 0.1250, 0.1254, 0.1241, 0.1251, 0.1256, 0.1252, 0.1249],
        [0.1247, 0.1250, 0.1254, 0.1241, 0.1251, 0.1256, 0.1252, 0.1249],
        [0.1248, 0.1250, 0.1254, 0.1241, 0.1251, 0.1256, 0.1251, 0.1248],
        [0.1247, 0.1250, 0.1254, 0.1241, 0.1251, 0.1257, 0.1251, 0.1248],
        [0.1247, 0.1250, 0.1254, 0.1242, 0.1251, 0.1256, 0.1251, 0.1249],
        [0.1248, 0.1251, 0.1254, 0.1241, 0.1251, 0.1256, 0.1251, 0.1249],
        [0.1248, 0.1251, 0.1253, 0.1242, 0.1251, 0.1256, 0.1251, 0.1249],
        [0.1248, 0.1250, 0.1253, 0.1241, 0.1251, 0.1255, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03151082992553711s
iter time cost:16.737261295318604s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 92-------------

Evaluate global model
Averaged Train Loss: 2.3744
Averaged Test Accurancy: 0.2896
Std Test Accurancy: 0.0385
evaluate time cost:7.484367609024048s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.1159086227417s
attn_optimize time cost:0.07785820960998535s
weights:tensor([[0.5003, 0.4997],
        [0.5004, 0.4996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4991, 0.5009],
        [0.4991, 0.5009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4980, 0.5020],
        [0.4980, 0.5020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1247, 0.1238, 0.1254, 0.1269, 0.1245, 0.1247, 0.1249, 0.1251],
        [0.1247, 0.1238, 0.1254, 0.1269, 0.1245, 0.1247, 0.1249, 0.1251],
        [0.1247, 0.1238, 0.1254, 0.1269, 0.1245, 0.1246, 0.1250, 0.1250],
        [0.1247, 0.1239, 0.1254, 0.1269, 0.1245, 0.1246, 0.1250, 0.1250],
        [0.1247, 0.1238, 0.1254, 0.1269, 0.1246, 0.1247, 0.1250, 0.1250],
        [0.1247, 0.1238, 0.1254, 0.1270, 0.1245, 0.1247, 0.1250, 0.1250],
        [0.1247, 0.1238, 0.1254, 0.1269, 0.1245, 0.1246, 0.1249, 0.1251],
        [0.1247, 0.1238, 0.1254, 0.1269, 0.1246, 0.1246, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03349161148071289s
iter time cost:16.79380989074707s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 93-------------

Evaluate global model
Averaged Train Loss: 2.5363
Averaged Test Accurancy: 0.2864
Std Test Accurancy: 0.0553
evaluate time cost:7.4826343059539795s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.588690042495728s
attn_optimize time cost:0.07723045349121094s
weights:tensor([[0.5011, 0.4989],
        [0.5011, 0.4989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4981, 0.5019],
        [0.4980, 0.5020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4987, 0.5013],
        [0.4985, 0.5015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1247, 0.1251, 0.1248, 0.1239, 0.1250, 0.1258, 0.1253, 0.1255],
        [0.1247, 0.1250, 0.1249, 0.1239, 0.1250, 0.1259, 0.1252, 0.1255],
        [0.1247, 0.1251, 0.1248, 0.1239, 0.1250, 0.1258, 0.1253, 0.1254],
        [0.1247, 0.1251, 0.1248, 0.1238, 0.1250, 0.1258, 0.1253, 0.1255],
        [0.1247, 0.1250, 0.1248, 0.1239, 0.1250, 0.1258, 0.1253, 0.1255],
        [0.1247, 0.1250, 0.1248, 0.1239, 0.1250, 0.1259, 0.1252, 0.1255],
        [0.1247, 0.1251, 0.1248, 0.1239, 0.1250, 0.1258, 0.1253, 0.1254],
        [0.1247, 0.1251, 0.1248, 0.1239, 0.1250, 0.1259, 0.1252, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.09515571594238281s
iter time cost:16.3565776348114s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 94-------------

Evaluate global model
Averaged Train Loss: 2.3878
Averaged Test Accurancy: 0.2780
Std Test Accurancy: 0.0527
evaluate time cost:7.4618823528289795s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.94707441329956s
attn_optimize time cost:0.07841324806213379s
weights:tensor([[0.4991, 0.5009],
        [0.4991, 0.5009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4985, 0.5015],
        [0.4984, 0.5016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5033, 0.4967],
        [0.5033, 0.4967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1255, 0.1247, 0.1252, 0.1248, 0.1249, 0.1249, 0.1251, 0.1249],
        [0.1255, 0.1247, 0.1251, 0.1248, 0.1248, 0.1250, 0.1251, 0.1250],
        [0.1255, 0.1247, 0.1251, 0.1248, 0.1249, 0.1250, 0.1251, 0.1249],
        [0.1255, 0.1247, 0.1252, 0.1249, 0.1248, 0.1250, 0.1250, 0.1249],
        [0.1255, 0.1248, 0.1251, 0.1249, 0.1249, 0.1250, 0.1251, 0.1249],
        [0.1255, 0.1247, 0.1251, 0.1248, 0.1249, 0.1250, 0.1251, 0.1249],
        [0.1255, 0.1247, 0.1252, 0.1248, 0.1248, 0.1250, 0.1251, 0.1249],
        [0.1255, 0.1248, 0.1251, 0.1248, 0.1249, 0.1249, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.03399825096130371s
iter time cost:16.58807897567749s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 95-------------

Evaluate global model
Averaged Train Loss: 2.1791
Averaged Test Accurancy: 0.2872
Std Test Accurancy: 0.0421
evaluate time cost:7.497774600982666s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:21866.922818422318s
attn_optimize time cost:0.02500295639038086s
weights:tensor([[0.5008, 0.4992],
        [0.5008, 0.4992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5007, 0.4993],
        [0.5007, 0.4993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4968, 0.5032],
        [0.4970, 0.5030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1260, 0.1250, 0.1241, 0.1237, 0.1243, 0.1260, 0.1258, 0.1252],
        [0.1260, 0.1249, 0.1241, 0.1238, 0.1243, 0.1261, 0.1257, 0.1251],
        [0.1259, 0.1250, 0.1241, 0.1238, 0.1243, 0.1261, 0.1258, 0.1251],
        [0.1260, 0.1251, 0.1241, 0.1237, 0.1243, 0.1260, 0.1257, 0.1251],
        [0.1259, 0.1250, 0.1241, 0.1238, 0.1243, 0.1260, 0.1258, 0.1251],
        [0.1259, 0.1250, 0.1241, 0.1238, 0.1243, 0.1260, 0.1258, 0.1251],
        [0.1259, 0.1250, 0.1241, 0.1237, 0.1243, 0.1260, 0.1258, 0.1251],
        [0.1259, 0.1249, 0.1241, 0.1238, 0.1243, 0.1260, 0.1258, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.030394792556762695s
iter time cost:21874.48973464966s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 96-------------

Evaluate global model
Averaged Train Loss: 2.1933
Averaged Test Accurancy: 0.2991
Std Test Accurancy: 0.0435
evaluate time cost:10.817408561706543s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:9.409329175949097s
attn_optimize time cost:0.018013954162597656s
weights:tensor([[0.4995, 0.5005],
        [0.4995, 0.5005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4995, 0.5005],
        [0.4994, 0.5006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5009, 0.4991],
        [0.5007, 0.4993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1248, 0.1254, 0.1248, 0.1260, 0.1253, 0.1247, 0.1250, 0.1241],
        [0.1248, 0.1253, 0.1248, 0.1259, 0.1254, 0.1248, 0.1249, 0.1241],
        [0.1248, 0.1253, 0.1248, 0.1259, 0.1253, 0.1248, 0.1249, 0.1241],
        [0.1248, 0.1253, 0.1248, 0.1259, 0.1253, 0.1248, 0.1249, 0.1241],
        [0.1248, 0.1253, 0.1248, 0.1259, 0.1253, 0.1248, 0.1249, 0.1241],
        [0.1248, 0.1253, 0.1248, 0.1260, 0.1253, 0.1247, 0.1249, 0.1241],
        [0.1248, 0.1254, 0.1248, 0.1259, 0.1254, 0.1248, 0.1249, 0.1241],
        [0.1248, 0.1253, 0.1248, 0.1260, 0.1254, 0.1247, 0.1249, 0.1241]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.047998666763305664s
iter time cost:20.30475878715515s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 97-------------

Evaluate global model
Averaged Train Loss: 2.2456
Averaged Test Accurancy: 0.2820
Std Test Accurancy: 0.0531
evaluate time cost:7.803743124008179s
local training....
saving psub_res([psub1, psub2,...,]) to psub_res.pth
saved psub_res([psub1, psub2,...,]) to psub_res.pth
clients training time cost:8.610708475112915s
attn_optimize time cost:0.019013166427612305s
weights:tensor([[0.4989, 0.5011],
        [0.4989, 0.5011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.5005, 0.4995],
        [0.5005, 0.4995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.4985, 0.5015],
        [0.4984, 0.5016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
weights:tensor([[0.1253, 0.1238, 0.1249, 0.1249, 0.1252, 0.1252, 0.1241, 0.1266],
        [0.1253, 0.1238, 0.1249, 0.1249, 0.1252, 0.1253, 0.1240, 0.1266],
        [0.1254, 0.1237, 0.1249, 0.1249, 0.1252, 0.1252, 0.1241, 0.1266],
        [0.1254, 0.1237, 0.1249, 0.1248, 0.1253, 0.1253, 0.1241, 0.1266],
        [0.1254, 0.1237, 0.1249, 0.1249, 0.1252, 0.1253, 0.1240, 0.1266],
        [0.1254, 0.1237, 0.1249, 0.1248, 0.1253, 0.1252, 0.1241, 0.1266],
        [0.1254, 0.1237, 0.1249, 0.1249, 0.1252, 0.1253, 0.1240, 0.1266],
        [0.1254, 0.1237, 0.1249, 0.1249, 0.1252, 0.1252, 0.1240, 0.1266]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
intra_cluster_agg time cost:0.0406951904296875s
iter time cost:16.485108137130737s
{'emb_layer': Linear(in_features=10100, out_features=512, bias=True), 'intra_attn_model': Attn_Model(
  (query): Linear(in_features=512, out_features=128, bias=True)
  (key): Linear(in_features=512, out_features=128, bias=True)
)}
saved attn_model

-------------Round number: 98-------------

Evaluate global model
Traceback (most recent call last):
  File "main.py", line 423, in <module>
    run(args, wandb_run)
  File "main.py", line 254, in run
    server.train()
  File "D:\repos\PFedTrans\system\flcore\servers\serverfedtrans.py", line 88, in train
    self.evaluate()
  File "D:\repos\PFedTrans\system\flcore\servers\serverbase.py", line 208, in evaluate
    stats_train = self.train_metrics()
  File "D:\repos\PFedTrans\system\flcore\servers\serverbase.py", line 197, in train_metrics
    cl, ns = c.train_metrics()
  File "D:\repos\PFedTrans\system\flcore\clients\clientbase.py", line 122, in train_metrics
    for x, y in trainloader:
  File "D:\tools\miniconda3\envs\fl_core\lib\site-packages\torch\utils\data\dataloader.py", line 681, in __next__
    data = self._next_data()
  File "D:\tools\miniconda3\envs\fl_core\lib\site-packages\torch\utils\data\dataloader.py", line 721, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "D:\tools\miniconda3\envs\fl_core\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    return self.collate_fn(data)
  File "D:\tools\miniconda3\envs\fl_core\lib\site-packages\torch\utils\data\_utils\collate.py", line 166, in default_collate
    elif isinstance(elem, collections.abc.Sequence):
KeyboardInterrupt
