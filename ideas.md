## related works
### efficient transformers
高效的transformers，注意力机制的稀疏化、聚类、LSH



## insights

### 大纲
attention的背景，non-iid FL的背景，为什么attention能够应用到non-iid FL？有什么独特优势？
直接应用attention的挑战，这篇文章怎么解决的?
实验是否验证了这些insights，并且给出合理的分析，如果有证明的话可以添加进去。

### attention背景
参考最新的attention、transformer文章，包括GPT系列文章
attention已经不局限于NLP领域，CV领域也已经....VIT，CLIP
这些文章的共同思想就是，把研究的问题建模成一个序列：
当NLP研究的时候，词语token存在天然的序列关系，transformer通过位置编码技术表征一个token在序列中的位置。
而在CV领域，研究图像的时候，将图像切割成一个个patch，（vit 16*16）然后将这些patch视为一个序列。PS：这时候的positional encoding似乎采用什么方法都不重要，实验结果没差别。

为什么attention机制本身能够取得这样的成功?
attention减少了 inductive bias, 今年CVPR有篇文章研究了transformer在视觉领域的可视化问题，


### 

FL中利用attention机制加强weighted aggregation的一些insights
- CFL的一个基本insight是，每个客户端的数据分布都是一堆分布的混合，利用
- attention处理序列数据的时候具有独特的优势，不定长，scalable，能够对而联邦学习中的客户端也可以视为序列中的tokens，可以利用attention机制对客户端的相似度进行建模，对更加相似的客户端提供更高的权重，利用相似客户端之间差异的部分增加personalized Model的泛化性，利用相同的部分增加模型对local data distribution的预测性能。
- non inductive bias，更加适合海量数据的联邦学习，而且客户端数量越多，表征精确度越高。
- 联邦学习的特征，可能随时会有客户端加入，因此需要学到一个通用的客户端表征机制，而不是针对客户端本身
  - 在实验中，我们模拟了新的客户端加入时候的模型性能，可以看到，客户端可以根据共享的base layer，在经过本地的微调之后，得到很好的训练效果。
- 可能会有恶意客户端，attention可能就降低恶意客户端的权值
    恶意客户端的模型参数和梯度经过注意力机制后，与其他正常客户端的训练行为不一致，这个不一致性会被注意力机制捕捉，并分配一个比较小的注意力权重，在本地更新的过程中，
    我们在实验中模拟了一个随机的客户端模型，然后观察他的行为，实验证明了使用注意力机制可以提高联邦学习面临恶意投毒攻击时的鲁棒性。
- 其他insights?对于异构数据而言，对客户端模型参数和梯度进行嵌入输入注意力机制，可以建模数据异构性质。所谓异构性质其实就是可以用客户端模型参数的更新过程来表征，目标函数异构可以通过客户端模型和参数变化来表征。
  - 后续可能跟模型融合方法结合起来，做到层级粒度的agg更新。
  - 层级注意力，利用注意力去进行分组cluster，具体的论文看cvpr23的文章
  - 注意力的可解释性实验，
- 注意力

如何加强attention机制在weighting agg
- 对客户端参数和梯度进行联合嵌入，直觉上来看，参数表示客户端模型在解空间所处的位置，而梯度则是移动的方向，结合解空间hessian性质可以得知客户端对应的更新是否是有意义的。
- 采用随机投影矩阵+cluster，加强attention效率 > from reforemer
- 结合model fusion的技术，
- 结合meta-learning思想，对模型更新inner-loop 和 outer-loop进行改进
    meta-learning的行为跟本方法非常相似。都是对local training(inner-loop)的meta-learning，因此我们可以利用meta-learning的思想，对文章方法进行改进，
- FL过程：稳定收敛快，效率高
- 客户端表征：得到一个好的embedding

实验代码怎么改
1.目前跟什么方法对比，对比的效果是什么
- 我目前的更新方式其实是跟local training效果比较相近，调整异构化参数，当异构性特别大的时候就适用于。
- 跟cfl的更新方式对比一下，fedsoft方法根据模型准确率估计聚类，然后聚类中心怎么算，下一轮客户端的起始参数怎么算


### GMM
高斯混合模型（Gaussian Mixture Model）通常简称GMM，是一种业界广泛使用的聚类算法，该方法使用了高斯分布作为参数模型，并使用了期望最大（Expectation Maximization，简称EM）算法进行训练。

本文对该方法的原理进行了通俗易懂的讲解，期望读者能够更直观地理解方法原理。文本的最后还分析了高斯混合模型了另一种常见聚类算法K-means的关系，实际上在特定约束条件下，K-means算法可以被看作是高斯混合模型（GMM）的一种特殊形式
GMM是多个高斯分布组合刻画数据分布，

买点东西：


## 近期事项
事情很多，所有时间都用于准备这些事情上面，然后论文方面赶紧赶出来。
但是所有的事情也都是围绕论文内容展开，所以基本上可以复用

背景；动机；方法；结果；
组会汇报涉及比较详细的论文内容，所以精读可以放到后面准备ppt的时候，现在写的内容是当前方法的ideas和效果。


### 中期考核ppt 
等通知，具体时间还没出来，但是应该是在4月7号前

### 和ccc讨论论文进展
大概周三
资料：可能涉及到一些 attention机制、model fusion技术、最新的non-iid FL方法、
这两天大概要快速浏览相关论文，储备知识量，
写的思路：

### 组会汇报 
主题：Non-iid的联邦学习
### 论文实验
有几个Intro里面提到的ideas，把实验设计出来


时间安排：
3.27 完成讨论初稿
3.28 写一下实验代码+讨论初稿修改
3.29 


